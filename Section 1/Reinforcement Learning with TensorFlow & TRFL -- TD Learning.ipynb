{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reinforcement Learning with TensorFlow & TRFL -- TD Learning.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"npzJ574a6A94","colab_type":"text"},"cell_type":"markdown","source":["**Reinforcement Learning with TensorFlow & TRFL: TD Learning**\n","* This notebook shows how to use TD learning with TRFL.\n","* This simple example is to get you familiar with TRFL. For one step TD learning updates in the tabular case, the classic method (shown in the code for reference) is faster. The TRFL method is superior when using the TD learning loss on batches of tensors. Section 2, Deep Q Networks, will hightlight this.\n","\n","Outline:\n","1. Install TRFL\n","2. Define the GridWorld environment\n","3. Introduce Gym Environments\n","4. Find the value of each state value in the environment using TD learning and a random policy\n","\n","\n","\n","\n"]},{"metadata":{"id":"qzteyVI7mNmV","colab_type":"code","outputId":"cd36815e-ee16-4ea5-97ab-a1d423838c32","colab":{"base_uri":"https://localhost:8080/","height":328}},"cell_type":"code","source":["#TRFL works with TensorFlow 1.12\n","#installs TensorFlow version 1.12 then restarts the runtime\n","!pip install tensorflow==1.12\n","\n","import os\n","os.kill(os.getpid(), 9)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==1.12 in /usr/local/lib/python3.6/dist-packages (1.12.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.7)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.2.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (3.7.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.11.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.15.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.16.2)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.12.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.33.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.9)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (40.9.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.1)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (0.15.2)\n"],"name":"stdout"}]},{"metadata":{"id":"iZh_4T4U7Osx","colab_type":"code","outputId":"ed688957-9170-40ae-f04b-4fae19147498","executionInfo":{"status":"ok","timestamp":1555640220962,"user_tz":240,"elapsed":5600,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":174}},"cell_type":"code","source":["#install TRFL\n","!pip install trfl==1.0\n","\n","#install Tensorflow Probability\n","!pip install tensorflow-probability==0.5.0"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: trfl==1.0 in /usr/local/lib/python3.6/dist-packages (1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.16.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.11.0)\n","Requirement already satisfied: dm-sonnet in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.23)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (0.7.1)\n","Requirement already satisfied: tensorflow-probability==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.16.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.11.0)\n"],"name":"stdout"}]},{"metadata":{"id":"B-RZQ2KOmG9D","colab_type":"text"},"cell_type":"markdown","source":["**Gridworld**\n","\n","The Gridworld environment is a four by four grid. The agent randomly starts on the grid and can move either up, left, right, or down. If the agent reaches the upper left or lower right the episode is over. Every action the agent takes gets a reward of -1 until you reach the upper left or over right. Gridworld code from Denny Britz's Reinforcement Learning repo: https://github.com/dennybritz/reinforcement-learning\n"]},{"metadata":{"id":"0v8rA8v67PKc","colab_type":"code","colab":{}},"cell_type":"code","source":["#Environment from: https://github.com/dennybritz/reinforcement-learning/blob/cee9e78652f8ce98d6079282daf20680e5e17c6a/lib/envs/gridworld.py\n","\n","#define the environment\n","\n","import io\n","import numpy as np\n","import sys\n","from gym.envs.toy_text import discrete\n","import pprint\n","\n","UP = 0\n","RIGHT = 1\n","DOWN = 2\n","LEFT = 3\n","\n","class GridworldEnv(discrete.DiscreteEnv):\n","    \"\"\"\n","    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n","    You are an agent on an MxN grid and your goal is to reach the terminal\n","    state at the top left or the bottom right corner.\n","    For example, a 4x4 grid looks as follows:\n","    T  o  o  o\n","    o  x  o  o\n","    o  o  o  o\n","    o  o  o  T\n","    x is your position and T are the two terminal states.\n","    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n","    Actions going off the edge leave you in your current state.\n","    You receive a reward of -1 at each step until you reach a terminal state.\n","    \"\"\"\n","\n","    metadata = {'render.modes': ['human', 'ansi']}\n","\n","    def __init__(self, shape=[4,4]):\n","        if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n","            raise ValueError('shape argument must be a list/tuple of length 2')\n","\n","        self.shape = shape\n","\n","        nS = np.prod(shape)\n","        nA = 4\n","\n","        MAX_Y = shape[0]\n","        MAX_X = shape[1]\n","\n","        P = {}\n","        grid = np.arange(nS).reshape(shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            # P[s][a] = (prob, next_state, reward, is_done)\n","            P[s] = {a : [] for a in range(nA)}\n","\n","            is_done = lambda s: s == 0 or s == (nS - 1)\n","            reward = 0.0 if is_done(s) else -1.0\n","            #reward = 1.0 if is_done(s) else 0.0\n","\n","            # We're stuck in a terminal state\n","            if is_done(s):\n","                P[s][UP] = [(1.0, s, reward, True)]\n","                P[s][RIGHT] = [(1.0, s, reward, True)]\n","                P[s][DOWN] = [(1.0, s, reward, True)]\n","                P[s][LEFT] = [(1.0, s, reward, True)]\n","            # Not a terminal state\n","            else:\n","                ns_up = s if y == 0 else s - MAX_X\n","                ns_right = s if x == (MAX_X - 1) else s + 1\n","                ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n","                ns_left = s if x == 0 else s - 1\n","                P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n","                P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n","                P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n","                P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n","\n","            it.iternext()\n","\n","        # Initial state distribution is uniform\n","        isd = np.ones(nS) / nS\n","\n","        # We expose the model of the environment for educational purposes\n","        # This should not be used in any model-free learning algorithm\n","        self.P = P\n","\n","        super(GridworldEnv, self).__init__(nS, nA, P, isd)\n","\n","    def _render(self, mode='human', close=False):\n","        \"\"\" Renders the current gridworld layout\n","         For example, a 4x4 grid with the mode=\"human\" looks like:\n","            T  o  o  o\n","            o  x  o  o\n","            o  o  o  o\n","            o  o  o  T\n","        where x is your position and T are the two terminal states.\n","        \"\"\"\n","        if close:\n","            return\n","\n","        outfile = io.StringIO() if mode == 'ansi' else sys.stdout\n","\n","        grid = np.arange(self.nS).reshape(self.shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            if self.s == s:\n","                output = \" x \"\n","            elif s == 0 or s == self.nS - 1:\n","                output = \" T \"\n","            else:\n","                output = \" o \"\n","\n","            if x == 0:\n","                output = output.lstrip()\n","            if x == self.shape[1] - 1:\n","                output = output.rstrip()\n","\n","            outfile.write(output)\n","\n","            if x == self.shape[1] - 1:\n","                outfile.write(\"\\n\")\n","\n","            it.iternext()\n","            \n","pp = pprint.PrettyPrinter(indent=2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QKeUO0LKnCgp","colab_type":"text"},"cell_type":"markdown","source":["**An Introduction to Gym Environments**\n","\n","Gym is a popular RL library created by OpenAI. Gym has a variety of environments (https://gym.openai.com/). With gym environments you take actions and receive rewards and test algorithms and policies (among other things). At the core of interacting with a gym environment (env) are a few key methods:\n","\n","* *env.reset()*: Resets the environment and returns an observation of the current state.\n","* *env.step(action)*: Input an action and the env outputs a observation, reward, done indication, and info. Agents typically interact and receive feedback from the env using this method.\n","\n","I'll show you a simple example of the GridWorld env in action. The 'x' is the agent, 'T' are the terminal states that end the episode. Watch how the agent moves in the grid by taking actions. In this notebook we are using a random policy i.e. the agent takes a random action at each step."]},{"metadata":{"id":"YoXR4S1JpOuN","colab_type":"code","outputId":"665bf77a-e180-4088-8774-c2a589282e8a","executionInfo":{"status":"ok","timestamp":1555640221309,"user_tz":240,"elapsed":1567,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":1406}},"cell_type":"code","source":["#declare the environment\n","env = GridworldEnv()\n","#reset the environment and get the agent's current position (observation)\n","observation = env.reset()\n","env._render()\n","print(\"\")\n","action_dict = {0:\"UP\",1:\"RIGHT\", 2:\"DOWN\",3:\"LEFT\"}\n","\n","for i in range(10):\n","  #get a random action\n","  random_action = env.action_space.sample()\n","  observation,reward,done,info = env.step(random_action)\n","  print(\"Agent took action {} and is now in state {} \".format(action_dict[random_action], observation))\n","  env._render()\n","  print(\"\")\n","  if done:\n","    print(\"Agent reached end of episode, resetting the env\")\n","    print(env.reset())\n","    print(\"\")\n","    env._render()\n","    print(\"\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["T  o  o  o\n","o  o  o  o\n","x  o  o  o\n","o  o  o  T\n","\n","Agent took action UP and is now in state 4 \n","T  o  o  o\n","x  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took action LEFT and is now in state 4 \n","T  o  o  o\n","x  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took action RIGHT and is now in state 5 \n","T  o  o  o\n","o  x  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took action UP and is now in state 1 \n","T  x  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took action LEFT and is now in state 0 \n","x  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent reached end of episode, resetting the env\n","15\n","\n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  x\n","\n","Agent took action LEFT and is now in state 15 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  x\n","\n","Agent reached end of episode, resetting the env\n","13\n","\n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  x  o  T\n","\n","Agent took action LEFT and is now in state 12 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","x  o  o  T\n","\n","Agent took action LEFT and is now in state 12 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","x  o  o  T\n","\n","Agent took action RIGHT and is now in state 13 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  x  o  T\n","\n","Agent took action LEFT and is now in state 12 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","x  o  o  T\n","\n"],"name":"stdout"}]},{"metadata":{"id":"PLyi98bjnq9B","colab_type":"text"},"cell_type":"markdown","source":["** TRFL Usage **\n","\n","The main steps for using trfl:\n","1. In the TensorFlow graph, define the necessary TensorFlow tensors\n","2. In the graph, feed the tensors into the trfl method\n","3. In the TensorFlow session, run the graph operation\n","\n","Steps 1. and 2. are in the next cell. We define the tensors needed (step 1) and then pass them to trfl.td_learning (step 2). Step 3 is defined in the td_learning_value_estimate() function in the line sess.run([td_learning_t],..)"]},{"metadata":{"id":"FvOrudwY9Voy","colab_type":"code","colab":{}},"cell_type":"code","source":["#set up TRFL graph\n","import tensorflow as tf\n","import trfl\n","\n","#https://github.com/deepmind/trfl/blob/master/docs/trfl.md#td_learningv_tm1-r_t-pcont_t-v_t-nametdlearning\n","# Args:\n","# v_tm1: Tensor holding values at previous timestep, shape [B].\n","# r_t: Tensor holding rewards, shape [B].\n","# pcont_t: Tensor holding pcontinue values, shape [B].\n","# v_t: Tensor holding values at current timestep, shape [B].\n","# name: name to prefix ops created by this function.\n","\n","state_value_t = tf.placeholder(dtype=tf.float32,name=\"state_value\")\n","reward_t = tf.placeholder(dtype=tf.float32,name='reward')\n","gamma_t = tf.placeholder(dtype=tf.float32,name='discount_factor')\n","next_state_value_t = tf.placeholder(dtype=tf.float32,name='next_state_value')\n","  \n","td_learning_t = trfl.td_learning(state_value_t,reward_t,gamma_t,next_state_value_t,name=\"td_learning\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GpiR5UEXpSnN","colab_type":"text"},"cell_type":"markdown","source":["** The RL Training Loop **\n","\n","In the next cell we are going to define the training loop and then run it in the following cell. The goal is to estimate the value of each state (each cell in the gridworld) under a random policy using TD learning. state_value_array holds the estimated values and after each step the agent takes in the env, we update the state_value_array with the TD learning formula.\n","\n","** TRFL Usage **\n","\n","The TRFL usage here is to run the trfl operation td_learning_t in sess.run(). We then take the output (td_sess_output) and extract the td_error part of that tensor. Using the td_error we update the state_value_array. For reference, the code below shows the full output of trfl.td_learning and the classic RL method of performing tabular TD learning updates."]},{"metadata":{"id":"rHliEwLn8sf-","colab_type":"code","colab":{}},"cell_type":"code","source":["def td_learning_value_estimate(env,episodes=1000,alpha=0.05,discount_factor=1.0):\n","  \"\"\"\n","     Args:\n","        env: OpenAI env. env.P represents the transition probabilities of the environment.\n","            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n","            env.nS is a number of states in the environment. \n","            env.nA is a number of actions in the environment.\n","        episodes: number of episodes to run\n","        alpha: learning rate for state value updates\n","        discount_factor: Gamma discount factor. pcont_t TRFL argument\n","        \n","     Returns:\n","      Value of each state using a random policy\n","  \"\"\"\n","  \n","  with tf.Session() as sess:\n","    #initialize the estimated state values to zero\n","    state_value_array = np.zeros(env.nS)\n","    #reset the env\n","    current_state = env.reset()\n","    #env._render()\n","\n","    #run through each episode taking a random action each time\n","    #upgrade estimated state value after each action\n","    current_episode = 0\n","    while current_episode < episodes:\n","      #take a random action\n","      random_action = env.action_space.sample()\n","      next_state, rew, done, info = env.step(random_action)\n","      \n","      #run TRFL operation in the session\n","      td_sess_output = sess.run([td_learning_t],feed_dict={state_value_t:state_value_array[current_state],reward_t:rew,\n","                                                                gamma_t:discount_factor,next_state_value_t:state_value_array[next_state]})\n","\n","      #trfl.td_learning() returns\n","        #loss_output(loss=0.0, extra=td_extra(target=2.0, td_error=0.0))\n","          #loss can be used with a gradient descent optimizer (we will see this in the Deep Q Network section)\n","          #td_extra contains:\n","            #target: a batch of next_state_value\n","            #td_error: this is what we use to update our state values in td learning\n","\n","      #use the TD learning TD error to update estimated state values\n","      state_value_array[current_state] = state_value_array[current_state] + alpha*td_sess_output[0].extra.td_error\n","      #For reference, here is the tabular TD learning method\n","      #state_value_array[current_state] = state_value_array[current_state] + alpha * (rew + discount_factor*state_value_array[next_state] -state_value_array[current_state])\n","        \n","      #if the epsiode is done, reset the env, if not the next state becomes the current state and the loop repeats\n","      if done:\n","        current_state = env.reset()\n","        current_episode += 1\n","      else:\n","        current_state = next_state\n","\n","\n","    return state_value_array\n","  \n","\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"eDcyg7ITu7sx","colab_type":"code","outputId":"484337d6-4d22-4545-df3b-58b72fca35fc","executionInfo":{"status":"ok","timestamp":1555640245857,"user_tz":240,"elapsed":23757,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":188}},"cell_type":"code","source":["#run episodes with TD learning and get the state value estimates\n","state_values = td_learning_value_estimate(env,episodes=2000,alpha=0.03)\n","\n","print(\"State Value Estimates:\")\n","print(np.round(state_values,2))\n","print(\"\")\n","\n","print(\"Reshaped State Value Estimates:\")\n","print(np.round(state_values.reshape(env.shape),2))\n","print(\"\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["State Value Estimates:\n","[  0.   -13.06 -19.26 -20.96 -12.74 -17.34 -19.32 -19.88 -19.   -18.89\n"," -16.99 -16.74 -20.88 -18.12 -11.52   0.  ]\n","\n","Reshaped State Value Estimates:\n","[[  0.   -13.06 -19.26 -20.96]\n"," [-12.74 -17.34 -19.32 -19.88]\n"," [-19.   -18.89 -16.99 -16.74]\n"," [-20.88 -18.12 -11.52   0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"gg90mb4sqTQy","colab_type":"text"},"cell_type":"markdown","source":["The 'Reshaped State Value Estimates' show the TD learning estimate for the state values. The closer the agent is to a terminal state, the higher the estimate (since the agent is more likely to randomly choose an action and end up ending the episode)."]},{"metadata":{"id":"3PoyqsvCFDkQ","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}