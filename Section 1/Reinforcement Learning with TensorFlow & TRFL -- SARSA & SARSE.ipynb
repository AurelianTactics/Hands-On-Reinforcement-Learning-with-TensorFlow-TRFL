{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reinforcement Learning with TensorFlow & TRFL -- SARSA & SARSE.ipynb","version":"0.3.2","provenance":[{"file_id":"1J4zEuSgvtZvG6hQJCylcH3XLODZceU2f","timestamp":1549649307856},{"file_id":"1oW2DyHFO8smkVrlT6zt8dgEmdnDflnAe","timestamp":1549649256657},{"file_id":"1ru_y_J0AWJjsVR4tL1NpcJYBqo04AsQr","timestamp":1549644496478}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"npzJ574a6A94","colab_type":"text"},"cell_type":"markdown","source":["**Reinforcement Learning with TensorFlow & TRFL: SARSE & SARSE**\n","* This notebook shows how to apply the classic Reinforcement Learning (RL) concepts of SARSA and SARSE with TRFL.\n","* In SARSA, we estimate action values: Q(s,a) like we did in Q learning. However in SARSA we do on-policy updates while in Q learning we do off-policy updates\n","* We can create a policy from the action values. Two types of policy categorizations are on-policy and off-policy methods. \n","* In off-policy methods we use one policy for exploration (behavior policy) while we learn a seperate policy (target policy). In on-policy methods, the exploration and learned policy are the same. In SARSA we explore with the policy we are learning.\n","* SARSE is a slight variation of SARSA. In SARSA the next state is found by sampling an action from the policy, in SARSE the next state is the expected value of all states weighted by the policy. In SARS**A** we take an **A**ction while in SARS**E** we use **E**xpected value.\n","\n","Outline:\n","1. Install TRFL\n","2. Define the GridWorld environment\n","3. Discuss On-policy and Off-policy methods\n","4. Find the value of each state-action value in the environment using SARSA\n","5. Find the value of each state-action value in the environment using SARSE\n","\n","\n","\n"]},{"metadata":{"id":"Js54fOL2vFfx","colab_type":"code","outputId":"8fd64103-cbb1-4660-c53f-0ed428715da8","colab":{"base_uri":"https://localhost:8080/","height":328}},"cell_type":"code","source":["#TRFL has issues on Colab with TensorFlow version tensorflow-1.13.0rc1\n","#install TensorFlow 1.12 and restart run time\n","!pip install tensorflow==1.12\n","\n","import os\n","os.kill(os.getpid(), 9)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==1.12 in /usr/local/lib/python3.6/dist-packages (1.12.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (3.7.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.15.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.7)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.9)\n","Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.12.2)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.33.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.11.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.16.2)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.2.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (40.9.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.8.0)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (0.15.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.1)\n"],"name":"stdout"}]},{"metadata":{"id":"iZh_4T4U7Osx","colab_type":"code","outputId":"8bfedd8a-c01b-43a2-f886-2a957b1ee170","executionInfo":{"status":"ok","timestamp":1555640129128,"user_tz":240,"elapsed":6313,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":174}},"cell_type":"code","source":["#install TRFL\n","!pip install trfl==1.0\n","\n","#install Tensorflow Probability\n","!pip install tensorflow-probability==0.5.0"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: trfl==1.0 in /usr/local/lib/python3.6/dist-packages (1.0)\n","Requirement already satisfied: dm-sonnet in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.23)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (0.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.16.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.11.0)\n","Requirement already satisfied: tensorflow-probability==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.16.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.11.0)\n"],"name":"stdout"}]},{"metadata":{"id":"B-RZQ2KOmG9D","colab_type":"text"},"cell_type":"markdown","source":["**GridWorld**\n","\n","The GridWorld environment is a four by four grid. The agent randomly starts on the grid and can move either up, left, right, or down. If the agent reaches the upper left or lower right the episode is over. Every action the agent takes gets a reward of -1 until you reach the upper left or over right."]},{"metadata":{"id":"0v8rA8v67PKc","colab_type":"code","colab":{}},"cell_type":"code","source":["#Environment from: https://github.com/dennybritz/reinforcement-learning/blob/cee9e78652f8ce98d6079282daf20680e5e17c6a/lib/envs/gridworld.py\n","#https://github.com/dennybritz/reinforcement-learning/blob/cee9e78652f8ce98d6079282daf20680e5e17c6a/DP/Value%20Iteration%20Solution.ipynb\n","\n","#define the environment\n","\n","import io\n","import numpy as np\n","import sys\n","from gym.envs.toy_text import discrete\n","import pprint\n","\n","UP = 0\n","RIGHT = 1\n","DOWN = 2\n","LEFT = 3\n","\n","class GridworldEnv(discrete.DiscreteEnv):\n","    \"\"\"\n","    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n","    You are an agent on an MxN grid and your goal is to reach the terminal\n","    state at the top left or the bottom right corner.\n","    For example, a 4x4 grid looks as follows:\n","    T  o  o  o\n","    o  x  o  o\n","    o  o  o  o\n","    o  o  o  T\n","    x is your position and T are the two terminal states.\n","    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n","    Actions going off the edge leave you in your current state.\n","    You receive a reward of -1 at each step until you reach a terminal state.\n","    \"\"\"\n","\n","    metadata = {'render.modes': ['human', 'ansi']}\n","\n","    def __init__(self, shape=[4,4]):\n","        if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n","            raise ValueError('shape argument must be a list/tuple of length 2')\n","\n","        self.shape = shape\n","\n","        nS = np.prod(shape)\n","        nA = 4\n","\n","        MAX_Y = shape[0]\n","        MAX_X = shape[1]\n","\n","        P = {}\n","        grid = np.arange(nS).reshape(shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            # P[s][a] = (prob, next_state, reward, is_done)\n","            P[s] = {a : [] for a in range(nA)}\n","\n","            is_done = lambda s: s == 0 or s == (nS - 1)\n","            reward = 0.0 if is_done(s) else -1.0\n","            #reward = 1.0 if is_done(s) else 0.0\n","\n","            # We're stuck in a terminal state\n","            if is_done(s):\n","                P[s][UP] = [(1.0, s, reward, True)]\n","                P[s][RIGHT] = [(1.0, s, reward, True)]\n","                P[s][DOWN] = [(1.0, s, reward, True)]\n","                P[s][LEFT] = [(1.0, s, reward, True)]\n","            # Not a terminal state\n","            else:\n","                ns_up = s if y == 0 else s - MAX_X\n","                ns_right = s if x == (MAX_X - 1) else s + 1\n","                ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n","                ns_left = s if x == 0 else s - 1\n","                P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n","                P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n","                P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n","                P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n","\n","            it.iternext()\n","\n","        # Initial state distribution is uniform\n","        isd = np.ones(nS) / nS\n","\n","        # We expose the model of the environment for educational purposes\n","        # This should not be used in any model-free learning algorithm\n","        self.P = P\n","\n","        super(GridworldEnv, self).__init__(nS, nA, P, isd)\n","\n","    def _render(self, mode='human', close=False):\n","        \"\"\" Renders the current gridworld layout\n","         For example, a 4x4 grid with the mode=\"human\" looks like:\n","            T  o  o  o\n","            o  x  o  o\n","            o  o  o  o\n","            o  o  o  T\n","        where x is your position and T are the two terminal states.\n","        \"\"\"\n","        if close:\n","            return\n","\n","        outfile = io.StringIO() if mode == 'ansi' else sys.stdout\n","\n","        grid = np.arange(self.nS).reshape(self.shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            if self.s == s:\n","                output = \" x \"\n","            elif s == 0 or s == self.nS - 1:\n","                output = \" T \"\n","            else:\n","                output = \" o \"\n","\n","            if x == 0:\n","                output = output.lstrip()\n","            if x == self.shape[1] - 1:\n","                output = output.rstrip()\n","\n","            outfile.write(output)\n","\n","            if x == self.shape[1] - 1:\n","                outfile.write(\"\\n\")\n","\n","            it.iternext()\n","            \n","pp = pprint.PrettyPrinter(indent=2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QKeUO0LKnCgp","colab_type":"text"},"cell_type":"markdown","source":["**Policies: On-Policy vs. Off-Policy**\n","\n","A policy is the agent's action selection method for each state (a probability distribution over actions). This can be a deterministic choice like a greedy policy where the highest valued action is always chosen or a stochastic choice like in the TD learning notebook were we used a random policy at each state. Two categorizations of policies are on-policy and off-policy methods. SARSA and Q learning are very similar. The difference is in how the  action value estimate is updated. In Q learning the update is off-policy, in SARSA the update is on-policy.\n","\n","In off-policy methods we use one policy for exploration (behavior policy) while we learn a separate policy (target policy). In on-policy methods, the exploration and learned policy are the same. In SARSA we explore and learn with one policy. The difference is in how we use the TD error. In Q learning the TD error is:\n","\n","reward + gamma*max(Q(s',a)) - current_state_estimate. \n","\n","The max value isn't based on the current policy that the agent is actually following, it's based on a greedy policy that is always selecting the highest action value estimate. Contrast this to SARSA where the TD error is:\n","\n","reward + gamma*Q(s',sampled_action) - current_state_estimate\n","\n","In SARSA we sample the next action selected from the policy and use that for our next action value estimate. The code cell below has the updates side by side. SARSA is making updates using the policy that SARSA is exploring the env with.\n"]},{"metadata":{"id":"YoXR4S1JpOuN","colab_type":"code","outputId":"9cd4f5ba-532c-44b1-d433-8438447d0736","executionInfo":{"status":"ok","timestamp":1555640129502,"user_tz":240,"elapsed":2469,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":1611}},"cell_type":"code","source":["#declare the environment\n","env = GridworldEnv()\n","#reset the environment and get the agent's current position (observation)\n","current_state = env.reset()\n","env._render()\n","print(\"\")\n","action_dict = {0:\"UP\",1:\"RIGHT\", 2:\"DOWN\",3:\"LEFT\"}\n","q_table = np.array([[ 0.,   0.,   0.,   0. ],\n"," [-1.7, -2.4, -2.2, -1. ],\n"," [-2.3, -2.8, -2.6, -2. ],\n"," [-3.2, -3.3, -3.,  -3. ],\n"," [-1.,  -2.4, -2.6, -1.8],\n"," [-2.,  -2.8, -2.5, -2. ],\n"," [-3.,  -3.,  -3.,  -3. ],\n"," [-2.7, -2.5, -2.,  -2.5],\n"," [-2.,  -2.4, -2.6, -2.4],\n"," [-3.,  -3.,  -3.,  -3. ],\n"," [-2.5, -2.,  -2.,  -2.9],\n"," [-1.9, -1.5, -1.,  -2.3],\n"," [-3.,  -3.,  -3.5, -3.1],\n"," [-2.9, -2.,  -2.6, -2.9],\n"," [-2.5, -1.,  -1.6, -2.3],\n"," [ 0.,   0.,   0.,   0. ]])\n","alpha = 0.1\n","gamma = 1.\n","\n","epsilon = 0.1\n","\n","def get_action(s):\n","  #choose random action epsilon amount of the time\n","  if np.random.rand() < epsilon:\n","    action = env.action_space.sample()\n","    action_type = \"random\"\n","  else:\n","    #Choose a greedy action.\n","    action = np.argmax(q_table[s])\n","    action_type = \"greedy\"\n","  return action, action_type\n","   \n","action,action_type = get_action(current_state)\n","\n","for i in range(10):\n","  next_state,reward,done,info = env.step(action)\n","  print(\"Agent took {} action {} and is now in state {} \".format(action_type, action_dict[action], current_state))\n","  #in SARSA we find our next action based on the current policy (on-policy). In Q learning we don't need the next action, we take the max of the next state\n","  next_action, action_type = get_action(next_state) \n","  \n","  #update q table on-policy (SARSA)\n","  q_table[current_state,action] = q_table[current_state,action] + alpha*(gamma*q_table[next_state,next_action] - q_table[current_state,action])\n","  \n","  #For reference update q table off-policy (Q learning)\n","  #q_table[current_state,action] = q_table[current_state,action] + alpha*(gamma*np.max(q_table[next_state]) - q_table[current_state,action])\n","  \n","  env._render()\n","  print(\"\")\n","  if done:\n","    print(\"Agent reached end of episode, resetting the env\")\n","    current_state = env.reset()\n","    print(\"\")\n","    env._render()\n","    print(\"\")\n","  else:\n","    current_state = next_state\n","    action = next_action"],"execution_count":4,"outputs":[{"output_type":"stream","text":["T  o  o  o\n","o  o  o  x\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took random action UP and is now in state 7 \n","T  o  o  x\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action DOWN and is now in state 3 \n","T  o  o  o\n","o  o  o  x\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action DOWN and is now in state 7 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  x\n","o  o  o  T\n","\n","Agent took greedy action DOWN and is now in state 11 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  x\n","\n","Agent reached end of episode, resetting the env\n","\n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  x  T\n","\n","Agent took greedy action DOWN and is now in state 14 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  x  T\n","\n","Agent took greedy action RIGHT and is now in state 14 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  x\n","\n","Agent reached end of episode, resetting the env\n","\n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  x  o  T\n","\n","Agent took greedy action RIGHT and is now in state 13 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  x  T\n","\n","Agent took greedy action RIGHT and is now in state 14 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  x\n","\n","Agent reached end of episode, resetting the env\n","\n","T  o  o  o\n","o  o  o  o\n","o  o  x  o\n","o  o  o  T\n","\n","Agent took greedy action RIGHT and is now in state 10 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  x\n","o  o  o  T\n","\n","Agent took greedy action DOWN and is now in state 11 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  x\n","\n","Agent reached end of episode, resetting the env\n","\n","T  o  o  o\n","o  o  o  x\n","o  o  o  o\n","o  o  o  T\n","\n"],"name":"stdout"}]},{"metadata":{"id":"o7erpNzhwCir","colab_type":"text"},"cell_type":"markdown","source":["** TRFL Usage **\n","\n","Once again, the three main TRFL steps are:\n","1. In the TensorFlow graph, define the necessary TensorFlow tensors\n","2. In the graph, feed the tensors into the trfl method\n","3. In the TensorFlow session, run the graph operation\n","\n","The difference between this trfl.sarsa and trfl.qlearning is that in trfl.sarsa an additional argument is needed: the next_action_t. SARSA updates estimated values using this next_action_t while in Q learning, the update is done with the max value of q_next_t."]},{"metadata":{"id":"FvOrudwY9Voy","colab_type":"code","colab":{}},"cell_type":"code","source":["#set up TRFL graph\n","import tensorflow as tf\n","import trfl\n","\n","num_actions = env.action_space.n\n","batch_size = 1\n","\n","#https://github.com/deepmind/trfl/blob/master/docs/trfl.md#sarsaq_tm1-a_tm1-r_t-pcont_t-q_t-a_t-namesarsa\n","# Args:\n","# q_tm1: Tensor holding Q-values for first timestep in a batch of transitions, shape [B x num_actions].\n","# a_tm1: Tensor holding action indices, shape [B].\n","# r_t: Tensor holding rewards, shape [B].\n","# pcont_t: Tensor holding pcontinue values, shape [B].\n","# q_t: Tensor holding Q-values for second timestep in a batch of transitions, shape [B x num_actions].\n","# a_t: Tensor holding action indices for second timestep, shape [B].\n","# name: name to prefix ops created within this op.\n","\n","q_t = tf.placeholder(dtype=tf.float32,shape=[batch_size,num_actions],name=\"action_value\")\n","action_t = tf.placeholder(dtype=tf.int32,shape=[batch_size],name=\"action\")\n","reward_t = tf.placeholder(dtype=tf.float32,shape=[batch_size],name='reward')\n","gamma_t = tf.placeholder(dtype=tf.float32,shape=[batch_size],name='discount_factor')\n","q_next_t = tf.placeholder(dtype=tf.float32,shape=[batch_size,num_actions],name=\"next_action_value\")\n","next_action_t = tf.placeholder(dtype=tf.int32,shape=[batch_size],name=\"next_action_action\")\n","\n","_, sarsa_t = trfl.sarsa(q_t, action_t, reward_t, gamma_t, q_next_t, next_action_t, name='Sarsa')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g4Ge-npOyKt0","colab_type":"text"},"cell_type":"markdown","source":["** The RL Training Loop **\n","\n","In the next cell we are going to define the training loop and then run it in the following cell. The goal is to estimate the action value of each state (the value of each state-action combination) using SARSA. action_value_array holds the estimated values. After each step the agent takes in the env, we update the action_value_array with the SARSA formula. The SARSA loop differs in that prior to updating the estimate, we select the next action. We use the next action in the update and then in the agent's next step we use that next action as the action to take.\n","\n","** TRFL Usage **\n","\n","The TRFL usage here is to run the trfl operation sarsa_t in sess.run(). We then take the output (sarsa_output) and extract the td_error part of that tensor. Using the td_error we update the action_value_array. For reference, the code below shows the full output of trfl.sarsa and the classic RL method of performing tabular SARSA learning updates."]},{"metadata":{"id":"rHliEwLn8sf-","colab_type":"code","colab":{}},"cell_type":"code","source":["def choose_action(q_table, state, epsilon=0.1):\n","  #choose action based on epsilon-greedy policy\n","  if np.random.rand() < epsilon:\n","    eg_action = env.action_space.sample()\n","  else:\n","    #Choose a greedy action. We will learn greedy actions with Q learning in the following cells.\n","    eg_action = np.argmax(q_table[state])\n","  return eg_action\n","\n","def sarsa_action_value_estimate(env,episodes=1000,alpha=0.05,discount_factor=1.0,epsilon=0.1):\n","  \"\"\"\n","     Args:\n","        env: OpenAI env. env.P represents the transition probabilities of the environment.\n","            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n","            env.nS is a number of states in the environment. \n","            env.nA is a number of actions in the environment.\n","        episodes: number of episodes to run\n","        alpha: learning rate for state value updates\n","        discount_factor: Gamma discount factor. pcont_t TRFL argument\n","        \n","     Returns:\n","      Value of each state with random policy\n","  \"\"\"\n","  \n","  with tf.Session() as sess:\n","    #initialize the estimated state values to zero\n","    action_value_array = np.zeros((env.nS,env.nA))\n","    #reset the env\n","    current_state = env.reset()\n","    eg_action = choose_action(action_value_array, current_state, epsilon)\n","    \n","    #run through each episode taking a random action each time\n","    #upgrade estimated state value after each action\n","    current_episode = 0\n","    while current_episode < episodes:\n","      \n","      #take a step using epsilon-greedy action\n","      next_state, rew, done, info = env.step(eg_action)\n","      next_action = choose_action(action_value_array, next_state, epsilon)\n","      \n","      #run TRFL operation in the session\n","      sarsa_output = sess.run([sarsa_t],feed_dict={q_t:np.expand_dims(action_value_array[current_state],axis=0),\n","                                                             action_t:np.expand_dims(eg_action,axis=0),\n","                                                             reward_t:np.expand_dims(rew,axis=0),\n","                                                             gamma_t:np.expand_dims(discount_factor,axis=0),\n","                                                             q_next_t:np.expand_dims(action_value_array[next_state],axis=0),\n","                                                             next_action_t:np.expand_dims(next_action,axis=0)})\n","      \n","#      trfl.sarsa() returns:\n","#       A namedtuple with fields:\n","#         * `loss`: a tensor containing the batch of losses, shape `[B]`.\n","#         * `extra`: a namedtuple with fields:\n","#             * `target`: batch of target values for `q_tm1[a_tm1]`, shape `[B]`.\n","#             * `td_error`: batch of temporal difference errors, shape `[B]`.\n","      \n","      #Use the SARSA TD error to update estimated state-action values\n","      action_value_array[current_state,eg_action] = action_value_array[current_state,eg_action] + alpha * sarsa_output[0].td_error\n","      \n","      #For reference, here is the tabular SARSA update method\n","#       action_value_array[current_state,eg_action] = action_value_array[current_state,eg_action] + \\\n","#          alpha * (rew + discount_factor*action_value_array[next_state,next_action] - action_value_array[current_state,eg_action])\n","      \n","      #if the epsiode is done, reset the env, if not the next state becomes the current state and the loop repeats\n","      if done:\n","        current_state = env.reset()\n","        eg_action = choose_action(action_value_array, current_state, epsilon)\n","        current_episode += 1\n","      else:\n","        current_state = next_state\n","        eg_action = next_action\n","\n","\n","    return action_value_array\n","  \n","\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"eDcyg7ITu7sx","colab_type":"code","outputId":"d2f84be3-f794-4fb0-e7d5-5b06f3ae1f4a","executionInfo":{"status":"ok","timestamp":1555640139206,"user_tz":240,"elapsed":7674,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"cell_type":"code","source":["#run episodes with SARSA and get the state value estimates\n","action_values = sarsa_action_value_estimate(env,episodes=1000,alpha=0.1)\n","\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_values.reshape((16,4)),2))\n","print(\"each row is a state, each column is an action\")\n","print(\"\")\n","\n","optimal_action_estimates = np.max(action_values,axis=1)\n","print(\"Current Policy State Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(env.shape),2))\n","print(\"estimate of the current state value at each state\")\n","print(\"\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["All Action Value Estimates:\n","[[ 0.    0.    0.    0.  ]\n"," [-1.54 -2.14 -1.76 -1.  ]\n"," [-2.43 -2.52 -2.21 -2.07]\n"," [-3.16 -3.2  -3.01 -3.01]\n"," [-1.   -1.74 -1.85 -1.57]\n"," [-2.01 -2.52 -2.11 -2.08]\n"," [-2.89 -2.89 -2.88 -2.88]\n"," [-2.58 -2.35 -2.08 -2.37]\n"," [-2.04 -2.19 -2.56 -2.52]\n"," [-2.76 -2.77 -2.74 -2.77]\n"," [-2.69 -2.05 -2.02 -2.17]\n"," [-1.75 -1.3  -1.   -2.12]\n"," [-3.   -3.   -3.06 -3.01]\n"," [-2.3  -2.04 -2.23 -2.28]\n"," [-1.91 -1.   -1.55 -1.64]\n"," [ 0.    0.    0.    0.  ]]\n","each row is a state, each column is an action\n","\n","Current Policy State Value Estimates:\n","[[ 0.   -1.   -2.07 -3.01]\n"," [-1.   -2.01 -2.88 -2.08]\n"," [-2.04 -2.74 -2.02 -1.  ]\n"," [-3.   -2.04 -1.    0.  ]]\n","estimate of the current state value at each state\n","\n"],"name":"stdout"}]},{"metadata":{"id":"gy6vmJ8nnamW","colab_type":"text"},"cell_type":"markdown","source":["**SARSE vs. SARSA**\n","\n","SARSE slightly modifies SARSA. While in SARSA we sample to get the next action, in SARSE we use the policy probabilities to create an expected value of the next state estimate. For example, with SARSA we used epsilon-greedy exploration to get the next action. 92.5% of the time SARSA chose the greedy action (90% greedy + 2.5% random) and 2.5% of the time each of the other non-greedy actions were chosen. SARSE uses these probabilities (0.925, 0.025, 0.025, 0.025) and the state-action value estimates to create an expectation. The TD error update becomes:\n","\n","reward + gamma*next_state_estimate - current_state_estimate\n","\n","\n","where next_state_estimate is:\n","\n","next_state_estimate = 0.925 x q_table[next_state_0,next_action_0] + 0.025 x q_table[next_state_1,next_action_1]  + 0.025 x q_table[next_state_2, next_action_2] + 0.025 x q_table[next_state_3,next_action_3]\n","\n","\n","\n","SARSE is on-policy.\n","\n","**TRFL Usage**\n","\n","In SARSE we use the sarse_action_probs_t instead of next_action_t. Ie we are using the expected distribution of actions rather than the action that was actually selected by the policy.\n"]},{"metadata":{"id":"g-JnmPgYp1n2","colab_type":"code","colab":{}},"cell_type":"code","source":["#set up TRFL graph\n","import tensorflow as tf\n","import trfl\n","\n","num_actions = env.action_space.n\n","batch_size = 1\n","\n","#SARSE replaces the next_action tensor with a tensor holding a probability of next_actions\n","\n","#https://github.com/deepmind/trfl/blob/master/docs/trfl.md#sarseq_tm1-a_tm1-r_t-pcont_t-q_t-probs_a_t-debugfalse-namesarse\n","# Args:\n","# q_tm1: Tensor holding Q-values for first timestep in a batch of transitions, shape [B x num_actions].\n","# a_tm1: Tensor holding action indices, shape [B].\n","# r_t: Tensor holding rewards, shape [B].\n","# pcont_t: Tensor holding pcontinue values, shape [B].\n","# q_t: Tensor holding Q-values for second timestep in a batch of transitions, shape [B x num_actions].\n","# probs_a_t: Tensor holding action probabilities for second timestep, shape [B x num_actions].\n","# debug: Boolean flag, when set to True adds ops to check whether probs_a_t is a batch of (approximately) valid probability distributions.\n","# name: name to prefix ops created by this function.\n","\n","sarse_q_t = tf.placeholder(dtype=tf.float32,shape=[batch_size,num_actions],name=\"action_value\")\n","sarse_action_t = tf.placeholder(dtype=tf.int32,shape=[batch_size],name=\"action\")\n","sarse_reward_t = tf.placeholder(dtype=tf.float32,shape=[batch_size],name='reward')\n","sarse_gamma_t = tf.placeholder(dtype=tf.float32,shape=[batch_size],name='discount_factor')\n","sarse_q_next_t = tf.placeholder(dtype=tf.float32,shape=[batch_size,num_actions],name=\"next_action_value\")\n","sarse_action_probs_t = tf.placeholder(dtype=tf.float32,shape=[batch_size,num_actions],name='action_probs')\n","\n","_, sarse_t = trfl.sarse(sarse_q_t, sarse_action_t, sarse_reward_t, sarse_gamma_t, sarse_q_next_t, sarse_action_probs_t, name='Sarse')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KyNqdl1rqoP7","colab_type":"code","colab":{}},"cell_type":"code","source":["def sarse_action_value_estimate(env,episodes=1000,alpha=0.05,discount_factor=1.0,epsilon=0.1):\n","  \"\"\"\n","     Args:\n","        env: OpenAI env. env.P represents the transition probabilities of the environment.\n","            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n","            env.nS is a number of states in the environment. \n","            env.nA is a number of actions in the environment.\n","        episodes: number of episodes to run\n","        alpha: learning rate for state value updates\n","        discount_factor: Gamma discount factor. pcont_t TRFL argument\n","        \n","     Returns:\n","      Value of each state with random policy\n","  \"\"\"\n","  \n","  with tf.Session() as sess:\n","    #initialize the estimated state values to zero\n","    action_value_array = np.zeros((env.nS,env.nA))\n","    #reset the env\n","    current_state = env.reset()\n","    \n","    #chance of choosing random action based on epsilon. use this with SARSE's action probabilities\n","    random_prob = epsilon/env.nA\n","    greedy_prob = 1.-epsilon\n","    \n","    #run through each episode taking a random action each time\n","    #upgrade estimated state value after each action\n","    current_episode = 0\n","    while current_episode < episodes:\n","      #choose action based on epsilon-greedy policy\n","      if np.random.rand() < epsilon:\n","        eg_action = env.action_space.sample()\n","      else:\n","        #Choose a greedy action. We will learn greedy actions with Q learning in the following cells.\n","        eg_action = np.argmax(action_value_array[current_state])\n","      \n","      #take a step using epsilon-greedy action\n","      next_state, rew, done, info = env.step(eg_action)\n","      \n","      #generate action probabilities\n","      #randomly choose each action with probability epislon/4 \n","      action_probs = np.array([random_prob]*env.nA) \n","      #choose greedy action with probability 1-epsilon\n","      action_probs[np.argmax(action_value_array[next_state])] += greedy_prob \n","      \n","      #run TRFL operation in the session\n","      sarse_output = sess.run([sarse_t],feed_dict={sarse_q_t:np.expand_dims(action_value_array[current_state],axis=0),\n","                                                             sarse_action_t:np.expand_dims(eg_action,axis=0),\n","                                                             sarse_reward_t:np.expand_dims(rew,axis=0),\n","                                                             sarse_gamma_t:np.expand_dims(discount_factor,axis=0),\n","                                                             sarse_q_next_t:np.expand_dims(action_value_array[next_state],axis=0),\n","                                                             sarse_action_probs_t:np.expand_dims(action_probs,axis=0)})\n","      \n","#      trfl.sarse() returns:\n","#       A namedtuple with fields:\n","#         * `loss`: a tensor containing the batch of losses, shape `[B]`.\n","#         * `extra`: a namedtuple with fields:\n","#             * `target`: batch of target values for `q_tm1[a_tm1]`, shape `[B]`.\n","#             * `td_error`: batch of temporal difference errors, shape `[B]`.\n","      \n","      #Use the SARSE TD error to update estimated state-action values\n","      action_value_array[current_state,eg_action] = action_value_array[current_state,eg_action] + alpha * sarse_output[0].td_error\n","      \n","      #For reference, here is the tabular SARSE update method\n","#       next_action_value_estimate = 0.\n","#       for i in range(env.nA):\n","#         next_action_value_estimate += action_probs[i] * action_value_array[next_state,i]\n","#       action_value_array[current_state,eg_action] = action_value_array[current_state,eg_action] + \\\n","#          alpha * (rew + discount_factor*next_action_value_estimate - action_value_array[current_state,eg_action])\n","      \n","      #if the epsiode is done, reset the env, if not the next state becomes the current state and the loop repeats\n","      if done:\n","        current_state = env.reset()\n","        current_episode += 1\n","      else:\n","        current_state = next_state\n","\n","    return action_value_array"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P2qwyyRAqrJ2","colab_type":"code","outputId":"35cad065-6c57-4241-c6f3-afb15a4f723a","executionInfo":{"status":"ok","timestamp":1555640141562,"user_tz":240,"elapsed":4804,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"cell_type":"code","source":["#run episodes with SARSE and get the state value estimates\n","action_values = sarse_action_value_estimate(env,episodes=1000,alpha=0.1)\n","\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_values.reshape((16,4)),2))\n","print(\"each row is a state, each column is an action\")\n","print(\"\")\n","\n","optimal_action_estimates = np.max(action_values,axis=1)\n","print(\"Current Policy State Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(env.shape),2))\n","print(\"estimate of the current state value at each state\")\n","print(\"\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["All Action Value Estimates:\n","[[ 0.    0.    0.    0.  ]\n"," [-1.49 -1.71 -1.77 -1.  ]\n"," [-2.16 -2.35 -2.38 -2.05]\n"," [-3.   -3.16 -3.   -3.  ]\n"," [-1.   -1.86 -1.25 -1.42]\n"," [-2.03 -2.25 -2.33 -2.03]\n"," [-2.83 -2.83 -2.81 -2.82]\n"," [-2.26 -2.25 -2.05 -2.33]\n"," [-2.04 -2.17 -2.31 -2.29]\n"," [-2.88 -2.86 -2.87 -2.87]\n"," [-2.35 -2.04 -2.04 -2.84]\n"," [-1.75 -1.53 -1.   -1.93]\n"," [-3.   -3.   -3.01 -3.18]\n"," [-2.75 -2.05 -2.36 -2.48]\n"," [-2.06 -1.   -1.59 -1.72]\n"," [ 0.    0.    0.    0.  ]]\n","each row is a state, each column is an action\n","\n","Current Policy State Value Estimates:\n","[[ 0.   -1.   -2.05 -3.  ]\n"," [-1.   -2.03 -2.81 -2.05]\n"," [-2.04 -2.86 -2.04 -1.  ]\n"," [-3.   -2.05 -1.    0.  ]]\n","estimate of the current state value at each state\n","\n"],"name":"stdout"}]},{"metadata":{"id":"VO3mJzJ0r6ki","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}