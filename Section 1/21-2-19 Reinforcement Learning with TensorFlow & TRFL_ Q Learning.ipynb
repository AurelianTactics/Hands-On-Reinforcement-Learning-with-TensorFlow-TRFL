{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reinforcement Learning with TensorFlow & TRFL: Q Learning.ipynb","version":"0.3.2","provenance":[{"file_id":"1ru_y_J0AWJjsVR4tL1NpcJYBqo04AsQr","timestamp":1549644496478}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"npzJ574a6A94","colab_type":"text"},"cell_type":"markdown","source":["**Reinforcement Learning with TensorFlow & TRFL: Q Learning**\n","* This notebook shows how to apply the classic Reinforcement Learning (RL) idea of Q learning with TRFL.\n","* In TD learning we estimated state values: V(s). In Q learning we estimate action values: Q(s,a). Here we'll go over Q learning in the simple tabular case. Next section we will use this same Q learning function in powerful Deep Learning algorithms like Deep Q Network.\n","* A key concept in RL is exploration. We'll introduce and use epsilon greedy exploration, which is often used with Q learning.\n","\n","Outline:\n","1. Install TRFL\n","2. Define the GridWorld environment\n","3. Discuss Epsilon-Greedy Exploration\n","4. Find the value of each state-action value in the environment using Q learning\n","\n","\n","\n"]},{"metadata":{"id":"RqN0Yz--M6sM","colab_type":"code","outputId":"18ab0973-7382-451f-df80-8479f222b62e","colab":{"base_uri":"https://localhost:8080/","height":302}},"cell_type":"code","source":["#TRFL has issues on Colab with TensorFlow version tensorflow-1.13.0rc1\n","#install TensorFlow 1.12 and restart run time\n","!pip install tensorflow==1.12\n","\n","import os\n","os.kill(os.getpid(), 9)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==1.12 in /usr/local/lib/python3.6/dist-packages (1.12.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.14.6)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.2.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.12.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.9)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.7)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.33.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.11.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (3.6.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.15.0)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (0.14.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.0.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (40.8.0)\n"],"name":"stdout"}]},{"metadata":{"id":"iZh_4T4U7Osx","colab_type":"code","outputId":"0d849e54-e241-44c4-adca-31392ba40cfd","executionInfo":{"status":"ok","timestamp":1550360471121,"user_tz":300,"elapsed":2954,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"cell_type":"code","source":["#install TRFL\n","!pip install trfl"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: trfl in /usr/local/lib/python3.6/dist-packages (1.0)\n","Requirement already satisfied: dm-sonnet in /usr/local/lib/python3.6/dist-packages (from trfl) (1.23)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trfl) (1.14.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trfl) (1.11.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trfl) (0.7.0)\n"],"name":"stdout"}]},{"metadata":{"id":"B-RZQ2KOmG9D","colab_type":"text"},"cell_type":"markdown","source":["**GridWorld**\n","\n","The GridWorld environment is a four by four grid. The agent randomly starts on the grid and can move either up, left, right, or down. If the agent reaches the upper left or lower right the episode is over. Every action the agent takes gets a reward of -1 until you reach the upper left or over right."]},{"metadata":{"id":"0v8rA8v67PKc","colab_type":"code","colab":{}},"cell_type":"code","source":["#Environment from: https://github.com/dennybritz/reinforcement-learning/blob/cee9e78652f8ce98d6079282daf20680e5e17c6a/lib/envs/gridworld.py\n","\n","#define the environment\n","\n","import io\n","import numpy as np\n","import sys\n","from gym.envs.toy_text import discrete\n","import pprint\n","\n","UP = 0\n","RIGHT = 1\n","DOWN = 2\n","LEFT = 3\n","\n","class GridworldEnv(discrete.DiscreteEnv):\n","    \"\"\"\n","    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n","    You are an agent on an MxN grid and your goal is to reach the terminal\n","    state at the top left or the bottom right corner.\n","    For example, a 4x4 grid looks as follows:\n","    T  o  o  o\n","    o  x  o  o\n","    o  o  o  o\n","    o  o  o  T\n","    x is your position and T are the two terminal states.\n","    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n","    Actions going off the edge leave you in your current state.\n","    You receive a reward of -1 at each step until you reach a terminal state.\n","    \"\"\"\n","\n","    metadata = {'render.modes': ['human', 'ansi']}\n","\n","    def __init__(self, shape=[4,4]):\n","        if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n","            raise ValueError('shape argument must be a list/tuple of length 2')\n","\n","        self.shape = shape\n","\n","        nS = np.prod(shape)\n","        nA = 4\n","\n","        MAX_Y = shape[0]\n","        MAX_X = shape[1]\n","\n","        P = {}\n","        grid = np.arange(nS).reshape(shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            # P[s][a] = (prob, next_state, reward, is_done)\n","            P[s] = {a : [] for a in range(nA)}\n","\n","            is_done = lambda s: s == 0 or s == (nS - 1)\n","            reward = 0.0 if is_done(s) else -1.0\n","            #reward = 1.0 if is_done(s) else 0.0\n","\n","            # We're stuck in a terminal state\n","            if is_done(s):\n","                P[s][UP] = [(1.0, s, reward, True)]\n","                P[s][RIGHT] = [(1.0, s, reward, True)]\n","                P[s][DOWN] = [(1.0, s, reward, True)]\n","                P[s][LEFT] = [(1.0, s, reward, True)]\n","            # Not a terminal state\n","            else:\n","                ns_up = s if y == 0 else s - MAX_X\n","                ns_right = s if x == (MAX_X - 1) else s + 1\n","                ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n","                ns_left = s if x == 0 else s - 1\n","                P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n","                P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n","                P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n","                P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n","\n","            it.iternext()\n","\n","        # Initial state distribution is uniform\n","        isd = np.ones(nS) / nS\n","\n","        # We expose the model of the environment for educational purposes\n","        # This should not be used in any model-free learning algorithm\n","        self.P = P\n","\n","        super(GridworldEnv, self).__init__(nS, nA, P, isd)\n","\n","    def _render(self, mode='human', close=False):\n","        \"\"\" Renders the current gridworld layout\n","         For example, a 4x4 grid with the mode=\"human\" looks like:\n","            T  o  o  o\n","            o  x  o  o\n","            o  o  o  o\n","            o  o  o  T\n","        where x is your position and T are the two terminal states.\n","        \"\"\"\n","        if close:\n","            return\n","\n","        outfile = io.StringIO() if mode == 'ansi' else sys.stdout\n","\n","        grid = np.arange(self.nS).reshape(self.shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            if self.s == s:\n","                output = \" x \"\n","            elif s == 0 or s == self.nS - 1:\n","                output = \" T \"\n","            else:\n","                output = \" o \"\n","\n","            if x == 0:\n","                output = output.lstrip()\n","            if x == self.shape[1] - 1:\n","                output = output.rstrip()\n","\n","            outfile.write(output)\n","\n","            if x == self.shape[1] - 1:\n","                outfile.write(\"\\n\")\n","\n","            it.iternext()\n","            \n","pp = pprint.PrettyPrinter(indent=2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QKeUO0LKnCgp","colab_type":"text"},"cell_type":"markdown","source":["**An Introduction to Exploration: Epsilon-Greedy Exploration**\n","\n","Exploration is a key concept in RL. In order to find the best policies, an agent needs to explore the environment. By exploring, the agent can experience new states and rewards. In the last notebook, the agent explored GridWorld by taking a random action at every step. While random action explorations can work in some environments, the downside is the agent can spend too much time exploring bad states or states that have already been explored fully and not enough time exploring promising states. A simple--yet surprisingly effective--approach to exploration is Epsilon-Greedy exploration. A epsilon percentage of the time, the agent chooses a random action. The remaining amount of the time (1-epsilon) the agent choose the best estimated action aka the* greedy action*. Epsilon can be a fixed value between 0 and 1 or can start at a high value and gradually decay over time (ie start at .99 and decay to 0.01). In this notebook we will used a fixed epsilon value of 0.1. Below is a simple example of epsilon-greedy exploration.\n"]},{"metadata":{"id":"YoXR4S1JpOuN","colab_type":"code","outputId":"4100ce17-acb9-47f2-eb1a-4047367f4251","executionInfo":{"status":"ok","timestamp":1550360506316,"user_tz":300,"elapsed":335,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":1243}},"cell_type":"code","source":["#declare the environment\n","env = GridworldEnv()\n","#reset the environment and get the agent's current position (observation)\n","current_state = env.reset()\n","env._render()\n","print(\"\")\n","action_dict = {0:\"UP\",1:\"RIGHT\", 2:\"DOWN\",3:\"LEFT\"}\n","greedy_dict = {0:3,1:3,2:3,3:3,\n","              4:0,5:0,6:0,7:0,\n","              8:2,9:2,10:2,11:2,\n","              12:1,13:1,14:1,15:1}\n","epsilon = 0.1\n","\n","for i in range(10):\n","  #choose random action epsilon amount of the time\n","  if np.random.rand() < epsilon:\n","    action = env.action_space.sample()\n","    action_type = \"random\"\n","  else:\n","    #Choose a greedy action. We will learn greedy actions with Q learning in the following cells.\n","    action = greedy_dict[current_state]\n","    action_type = \"greedy\"\n"," \n","  current_state,reward,done,info = env.step(action)\n","  print(\"Agent took {} action {} and is now in state {} \".format(action_type, action_dict[action], current_state))\n","  env._render()\n","  print(\"\")\n","  if done:\n","    print(\"Agent reached end of episode, resetting the env\")\n","    print(env.reset())\n","    print(\"\")\n","    env._render()\n","    print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  x  o  T\n","\n","Agent took greedy action RIGHT and is now in state 14 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  x  T\n","\n","Agent took greedy action RIGHT and is now in state 15 \n","T  o  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  x\n","\n","Agent reached end of episode, resetting the env\n","6\n","\n","T  o  o  o\n","o  o  x  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action RIGHT and is now in state 7 \n","T  o  o  o\n","o  o  o  x\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action UP and is now in state 3 \n","T  o  o  x\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took random action UP and is now in state 3 \n","T  o  o  x\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action LEFT and is now in state 2 \n","T  o  x  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took random action LEFT and is now in state 1 \n","T  x  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took random action RIGHT and is now in state 2 \n","T  o  x  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took random action UP and is now in state 2 \n","T  o  x  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n","Agent took greedy action LEFT and is now in state 1 \n","T  x  o  o\n","o  o  o  o\n","o  o  o  o\n","o  o  o  T\n","\n"],"name":"stdout"}]},{"metadata":{"id":"GXzMtX1rsAg-","colab_type":"text"},"cell_type":"markdown","source":["** TRFL Usage **\n","\n","Once again, the three main TRFL steps are:\n","1. In the TensorFlow graph, define the necessary TensorFlow tensors\n","2. In the graph, feed the tensors into the trfl method\n","3. In the TensorFlow session, run the graph operation\n","\n","We saw this in the last notebook. Here in Q learning there are some slight differences. We use the trfl.qlearning() method and we input the action and action values (instead of state values) into the method. Note for the action values q_t and q_next_t the shape is batch size X number of actions."]},{"metadata":{"id":"FvOrudwY9Voy","colab_type":"code","colab":{}},"cell_type":"code","source":["#set up TRFL graph\n","import tensorflow as tf\n","import trfl\n","\n","#https://github.com/deepmind/trfl/blob/master/docs/trfl.md#qlearningq_tm1-a_tm1-r_t-pcont_t-q_t-nameqlearning\n","# Args:\n","# q_tm1: Tensor holding Q-values for first timestep in a batch of transitions, shape [B x num_actions].\n","# a_tm1: Tensor holding action indices, shape [B].\n","# r_t: Tensor holding rewards, shape [B].\n","# pcont_t: Tensor holding pcontinue values, shape [B].\n","# q_t: Tensor holding Q-values for second timestep in a batch of transitions, shape [B x num_actions].\n","# name: name to prefix ops created within this op.\n","\n","\n","num_actions = env.action_space.n\n","batch_size = 1\n","\n","q_t = tf.placeholder(dtype=tf.float32,shape=[batch_size,num_actions],name=\"q_value\")\n","action_t = tf.placeholder(dtype=tf.int32,shape=[batch_size],name=\"action\")\n","reward_t = tf.placeholder(dtype=tf.float32,shape=[batch_size],name='reward')\n","gamma_t = tf.placeholder(dtype=tf.float32,shape=[batch_size],name='discount_factor')\n","q_next_t= tf.placeholder(dtype=tf.float32,shape=[batch_size,num_actions],name='q_next_value')\n","  \n","qloss_t, q_extra_t = trfl.qlearning(q_t,action_t,reward_t,gamma_t,q_next_t)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"azurqbzns_EW","colab_type":"text"},"cell_type":"markdown","source":["** The RL Training Loop **\n","\n","In the next cell we are going to define the training loop and then run it in the following cell. The goal is to estimate the action value of each state (the value of each state-action combination) using Q learning. action_value_array holds the estimated values. After each step the agent takes in the env, we update the action_value_array with the Q learning formula.\n","\n","** TRFL Usage **\n","\n","The TRFL usage here is to run the trfl operation q_learning_t in sess.run(). We then take the output (q_learning_output) and extract the td_error part of that tensor. Using the td_error we update the action_value_array. For reference, the code below shows the full output of trfl.qlearning and the classic RL method of performing tabular Q learning updates."]},{"metadata":{"id":"rHliEwLn8sf-","colab_type":"code","colab":{}},"cell_type":"code","source":["def q_learning_action_value_estimate(env,episodes=1000,alpha=0.05,discount_factor=1.0,epsilon=0.1):\n","  \"\"\"\n","     Args:\n","        env: OpenAI env. env.P represents the transition probabilities of the environment.\n","            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n","            env.nS is a number of states in the environment. \n","            env.nA is a number of actions in the environment.\n","        episodes: number of episodes to run\n","        alpha: learning rate for state value updates\n","        discount_factor: Gamma discount factor. pcont_t TRFL argument\n","        \n","     Returns:\n","      Value of each state with random policy\n","  \"\"\"\n","  \n","  with tf.Session() as sess:\n","    #initialize the estimated state values to zero\n","    action_value_array = np.zeros((env.nS,env.nA))\n","    #reset the env\n","    current_state = env.reset()\n","    #env._render()\n","\n","    #run through each episode taking a random action each time\n","    #upgrade estimated state value after each action\n","    current_episode = 0\n","    while current_episode < episodes:\n","      #choose action based on epsilon-greedy policy\n","      if np.random.rand() < epsilon:\n","        eg_action = env.action_space.sample()\n","      else:\n","        #Choose a greedy action. We will learn greedy actions with Q learning in the following cells.\n","        eg_action = np.argmax(action_value_array[current_state])\n","    \n","      #take a step using epsilon-greedy action\n","      next_state, rew, done, info = env.step(eg_action)\n","      \n","      #run TRFL operation in the session\n","      q_learning_output = sess.run([q_extra_t],feed_dict={q_t:np.expand_dims(action_value_array[current_state],axis=0),\n","                                                             action_t:np.expand_dims(eg_action,axis=0),\n","                                                             reward_t:np.expand_dims(rew,axis=0),\n","                                                             gamma_t:np.expand_dims(discount_factor,axis=0),\n","                                                             q_next_t:np.expand_dims(action_value_array[next_state],axis=0)})\n","\n","#      trfl.qlearning() returns:\n","#       A namedtuple with fields:\n","#         loss: a tensor containing the batch of losses, shape [B]. \n","#         extra: a namedtuple with fields:\n","#         target: batch of target values for q_tm1[a_tm1], shape [B].\n","#         td_error: batch of temporal difference errors, shape [B].\n","#        Here we are using the td_error to update our action values. We will use the loss with a gradient descent optimizer in Deep Q Network session.\n","\n","      #Use the Q learning TD error to update estimated state-action values\n","      action_value_array[current_state,eg_action] = action_value_array[current_state,eg_action] + alpha * q_learning_output[0].td_error\n","    \n","      #For reference, here is the tabular Q learning update method\n","#       max_q_value = np.max(action_value_array[next_state])\n","#       action_value_array[current_state,eg_action] = action_value_array[current_state,eg_action] + \\\n","#         alpha * (rew + discount_factor*max_q_value - action_value_array[current_state,eg_action])\n","\n","      #if the epsiode is done, reset the env, if not the next state becomes the current state and the loop repeats\n","      if done:\n","        current_state = env.reset()\n","        current_episode += 1\n","      else:\n","        current_state = next_state\n","\n","\n","    return action_value_array\n","  \n","\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"eDcyg7ITu7sx","colab_type":"code","outputId":"50c97b6a-ae75-41e3-9d21-811439262463","executionInfo":{"status":"ok","timestamp":1550361012743,"user_tz":300,"elapsed":3692,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":454}},"cell_type":"code","source":["#run episodes with Q learning and get the state value estimates\n","action_values = q_learning_action_value_estimate(env,episodes=2000,alpha=0.1)\n","\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_values.reshape((16,4)),1))\n","print(\"each row is a state, each column is an action\")\n","print(\"\")\n","\n","optimal_action_estimates = np.max(action_values,axis=1)\n","print(\"Optimal Action Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(env.shape),1))\n","print(\"estimate of the optimal State value at each state\")\n","print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["All State-Action Value Estimates:\n","[[ 0.   0.   0.   0. ]\n"," [-1.7 -2.1 -2.3 -1. ]\n"," [-2.5 -2.8 -2.9 -2. ]\n"," [-3.3 -3.3 -3.  -3. ]\n"," [-1.  -2.3 -2.5 -1.8]\n"," [-2.  -2.8 -2.4 -2. ]\n"," [-3.  -3.  -3.  -3. ]\n"," [-2.8 -2.3 -2.  -3. ]\n"," [-2.  -2.5 -2.8 -2.4]\n"," [-3.  -3.  -3.  -3. ]\n"," [-2.8 -2.  -2.  -2.5]\n"," [-2.5 -1.8 -1.  -2.5]\n"," [-3.  -3.  -3.2 -3.2]\n"," [-3.  -2.  -2.3 -2.8]\n"," [-2.3 -1.  -1.7 -2.6]\n"," [ 0.   0.   0.   0. ]]\n","each row is a state, each column is an action\n","\n","Optimal Action Value Estimates:\n","[[ 0. -1. -2. -3.]\n"," [-1. -2. -3. -2.]\n"," [-2. -3. -2. -1.]\n"," [-3. -2. -1.  0.]]\n","estimate of the optimal State value at each state\n","\n"],"name":"stdout"}]},{"metadata":{"id":"QldcKOOtt75a","colab_type":"text"},"cell_type":"markdown","source":["The first output shows the estimated value for each action in each state. Ie row 4 column 4 is the value if the agent was in the upper right grid cell and took that action left. In the second output, we take the best action for each of the 16 states and show the agent's estimate of the state value assuming the agent always acts greedily."]},{"metadata":{"id":"Yd_0h1EgN-W4","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}