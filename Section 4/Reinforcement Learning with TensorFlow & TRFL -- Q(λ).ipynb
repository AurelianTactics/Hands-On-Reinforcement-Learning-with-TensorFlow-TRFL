{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reinforcement Learning with TensorFlow & TRFL -- Q(λ).ipynb","version":"0.3.2","provenance":[{"file_id":"15y3NrJjPTRhzYm0OWfoc813cR9BtCTGl","timestamp":1553044789533},{"file_id":"1BmiEkGoqe_CAaZmggviivF7Ygbd1iPe4","timestamp":1552945822274},{"file_id":"110YMHk2yHqxguCj1yaJaXVmkRUadX9GV","timestamp":1552791755615},{"file_id":"1cMFan2NCLOZ8w_xKSEyXTF09paJ0F930","timestamp":1552618439794},{"file_id":"1SToTDuBpTdV2UVRN9bapskLUesVhBH1J","timestamp":1551831097681},{"file_id":"1ssliB1HogX4KFHRyNKLU2aeiAV865kLv","timestamp":1551578593280},{"file_id":"1N74NgQBdDCDRER81p_VdJA_lsZOR2Hjo","timestamp":1550336742833}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"npzJ574a6A94","colab_type":"text"},"cell_type":"markdown","source":["**Reinforcement Learning with TensorFlow & TRFL: Q(λ)**\n","\n","Outline:\n","* Q(λ) \n","* TRFL usage with trfl.qlambda()\n","\n","\n"]},{"metadata":{"id":"RyxlWytnVqJI","colab_type":"code","outputId":"c3166da7-bc8e-4083-84e1-12e32ff882a2","colab":{"base_uri":"https://localhost:8080/","height":328}},"cell_type":"code","source":["#TRFL works with TensorFlow 1.12\n","#installs TensorFlow version 1.12 then restarts the runtime\n","!pip install tensorflow==1.12\n","\n","import os\n","os.kill(os.getpid(), 9)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==1.12 in /usr/local/lib/python3.6/dist-packages (1.12.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.2.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.9)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.11.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.7)\n","Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.12.2)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.16.2)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.33.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (3.7.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.8.0)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (0.15.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (40.9.0)\n"],"name":"stdout"}]},{"metadata":{"id":"XRS56AQDVybG","colab_type":"code","outputId":"0bac6928-2a56-4f6c-f69a-baff48a497f4","executionInfo":{"status":"ok","timestamp":1555640814067,"user_tz":240,"elapsed":32872,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":174}},"cell_type":"code","source":["#install tensorflow-probability 0.5.0 that works with TensorFlow 1.12\n","!pip install tensorflow-probability==0.5.0\n","\n","#install TRFL\n","!pip install trfl==1.0\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-probability==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.16.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.11.0)\n","Requirement already satisfied: trfl==1.0 in /usr/local/lib/python3.6/dist-packages (1.0)\n","Requirement already satisfied: dm-sonnet in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.23)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (0.7.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.11.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.16.2)\n"],"name":"stdout"}]},{"metadata":{"id":"SGop2a_BZCBl","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import trfl\n","import tensorflow_probability as tfp"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KMY4yn1q3sfu","colab_type":"text"},"cell_type":"markdown","source":["** Q(λ) **\n","\n","Q(λ) has many variants. Some considerations are what values of λ to use, how to handle the next state max, and how to handle on-policy and off-policy actions. In Watkins’s Q(λ), the eligibility traces are set to 0 on the first non-greedy action and remains 0 for the rest of the trajectory. Naive Q(λ) and TB(λ) don’t set eligibility traces to 0 on non-greedy actions. Peng’s Q(λ) is a hybrid of SARSA(λ) and Watkins’s Q(λ).\n","\n","In this notebooks we'll use naive Q(λ), and ignore whether the action is on-policy or off-policy. We'll solve deterministic (ie not slippery) FrozenLake 4x4, non-deterministic FrozenLake 4x4, and FrozenLake 8x8.\n","\n","\n","** Example 1: FrozenLake 4x4 Not Slippery **\n","\n","First example we set is_slippery to False in FrozenLake. Every action the agent takes becomes deterministic, making the env much easier."]},{"metadata":{"id":"ttTuMdfpj74j","colab_type":"code","colab":{}},"cell_type":"code","source":["from gym.envs.registration import register\n","register(\n","    id='FrozenLakeNotSlippery-v0',\n","    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name' : '4x4', 'is_slippery': False}\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uu84Bgs1kISF","colab_type":"code","colab":{}},"cell_type":"code","source":["#hyperparameters\n","episodes = 10000\n","learning_rate = 0.01\n","discount = 0.99\n","lambda_val = 0.5\n","epsilon_start = 1.0\n","epsilon_min = 0.01\n","epsilon_step = (epsilon_start - epsilon_min)/(episodes*.9)\n","\n","env = gym.make('FrozenLakeNotSlippery-v0')\n","num_actions = env.action_space.n\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","#et up input tensors\n","q_value_ = tf.placeholder(dtype=tf.float32, shape=[None, 1, num_actions], name=\"q_value\")\n","action_ = tf.placeholder(dtype=tf.int32, shape=[None, 1], name=\"action\")\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","q_next_ = tf.placeholder(dtype=tf.float32, shape=[None, 1, num_actions], name=\"q_next\")\n","lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\")\n","#set up TRFL qlambda tensor\n","q_lambda_return_ = trfl.qlambda(q_value_, action_, reward_, discount_, q_next_, lambda_)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"58jYfaNgTCG_","colab_type":"text"},"cell_type":"markdown","source":["** TRFL Usage **\n","Q(λ) is similar to the λ methods we have gone over in earlier videos. Like in Section 1, we replace state values with q values and add a tensor for actions. The loss return or td error return can be used to perform updates."]},{"metadata":{"id":"A41lzKpAsxp3","colab_type":"code","outputId":"549c6c7b-339b-44b8-e060-52eda93dc866","executionInfo":{"status":"ok","timestamp":1555641348739,"user_tz":240,"elapsed":562905,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":4474}},"cell_type":"code","source":["stats_success = []\n","epsilon = epsilon_start\n","\n","with tf.Session() as sess:\n","  action_value_array = np.zeros((16,num_actions))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  q_list, action_list, reward_list, q_next_list, state_int_list = [], [], [], [], []\n","\n","  while current_episode < episodes:\n","    #take epsilon greedy action\n","    if np.random.rand() < epsilon:\n","      action = env.action_space.sample()\n","    else:\n","      #Choose a greedy action. If multiple greedy actions randomly choose between\n","      max_actions = np.argwhere(action_value_array[current_state] == np.max(action_value_array[current_state])).reshape((-1))\n","      action = np.random.choice(max_actions)\n","\n","    next_state, rew, done, info = env.step(action)\n","      \n","    q_list.append(action_value_array[current_state])\n","    reward_list.append(rew)\n","    action_list.append(action)\n","    q_next_list.append(action_value_array[next_state])\n","    state_int_list.append(current_state)\n","    \n","    current_state = next_state\n","    #run TRFL qlambda tensor to get TD error\n","    q_lambda_output = sess.run(q_lambda_return_, feed_dict={\n","        q_value_:np.array(q_list).reshape(-1,1,num_actions),\n","        action_:np.array(action_list).reshape(-1,1),\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(q_list)).reshape(-1,1),\n","        q_next_:np.array(q_next_list).reshape(-1,1,num_actions),\n","        lambda_:np.array([lambda_val]*len(q_list)).reshape(-1,1),\n","      })\n","    #use TD error output update action values\n","    action_value_array[state_int_list, action_list] += np.squeeze(learning_rate*q_lambda_output.extra.td_error)\n","\n","    if done:\n","      if next_state == 15:\n","        stats_success.append(1)\n","      else:\n","        stats_success.append(0)\n","        \n","      q_list, action_list, reward_list, q_next_list, state_int_list = [], [], [], [], []\n","      current_state = env.reset()\n","      current_episode += 1\n","      \n","      #decrease epsilon\n","      epsilon -= epsilon_step\n","      if epsilon < epsilon_min:\n","        epsilon = epsilon_min\n","        \n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","        optimal_action_estimates = np.max(action_value_array,axis=1)\n","        print(\"Optimal Action Value Estimates:\")\n","        print(np.round(optimal_action_estimates.reshape(4,4),2))\n","        print(\"estimate of the optimal state value at each state\")\n","        print(\"\")\n","        print(\"All Action Value Estimates:\")\n","        print(np.round(action_value_array.reshape((16,4)),2))\n","        print(\"\")\n","        "],"execution_count":5,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 1000, 0.89, 0.03\n","Optimal Action Value Estimates:\n","[[0.05 0.06 0.07 0.05]\n"," [0.05 0.   0.08 0.  ]\n"," [0.04 0.05 0.12 0.  ]\n"," [0.   0.05 0.22 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.05 0.04 0.05 0.05]\n"," [0.05 0.   0.06 0.05]\n"," [0.05 0.07 0.04 0.05]\n"," [0.05 0.   0.03 0.03]\n"," [0.04 0.03 0.   0.05]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.08 0.   0.04]\n"," [0.   0.   0.   0.  ]\n"," [0.02 0.   0.04 0.03]\n"," [0.01 0.02 0.05 0.  ]\n"," [0.01 0.12 0.   0.03]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.05 0.01]\n"," [0.01 0.08 0.22 0.02]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 2000, 0.78, 0.07\n","Optimal Action Value Estimates:\n","[[0.36 0.37 0.39 0.35]\n"," [0.33 0.   0.43 0.  ]\n"," [0.25 0.32 0.49 0.  ]\n"," [0.   0.33 0.6  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.33 0.31 0.36 0.33]\n"," [0.33 0.   0.37 0.34]\n"," [0.32 0.39 0.31 0.34]\n"," [0.35 0.   0.26 0.24]\n"," [0.27 0.22 0.   0.33]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.43 0.   0.33]\n"," [0.   0.   0.   0.  ]\n"," [0.19 0.   0.25 0.21]\n"," [0.16 0.18 0.32 0.  ]\n"," [0.23 0.49 0.   0.28]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.12 0.33 0.11]\n"," [0.21 0.33 0.6  0.26]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 3000, 0.67, 0.15\n","Optimal Action Value Estimates:\n","[[0.71 0.73 0.76 0.7 ]\n"," [0.68 0.   0.79 0.  ]\n"," [0.62 0.74 0.85 0.  ]\n"," [0.   0.72 0.91 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.69 0.65 0.71 0.68]\n"," [0.67 0.   0.73 0.7 ]\n"," [0.69 0.76 0.67 0.72]\n"," [0.7  0.   0.58 0.62]\n"," [0.6  0.57 0.   0.68]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.79 0.   0.68]\n"," [0.   0.   0.   0.  ]\n"," [0.49 0.   0.62 0.52]\n"," [0.46 0.52 0.74 0.  ]\n"," [0.66 0.85 0.   0.67]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.33 0.72 0.38]\n"," [0.58 0.65 0.91 0.69]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 4000, 0.56, 0.23\n","Optimal Action Value Estimates:\n","[[0.84 0.87 0.87 0.86]\n"," [0.83 0.   0.92 0.  ]\n"," [0.8  0.9  0.96 0.  ]\n"," [0.   0.91 0.99 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.83 0.8  0.84 0.83]\n"," [0.84 0.   0.87 0.84]\n"," [0.83 0.87 0.84 0.86]\n"," [0.86 0.   0.78 0.78]\n"," [0.76 0.75 0.   0.83]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.92 0.   0.86]\n"," [0.   0.   0.   0.  ]\n"," [0.57 0.   0.8  0.67]\n"," [0.61 0.69 0.9  0.  ]\n"," [0.84 0.96 0.   0.86]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.51 0.91 0.56]\n"," [0.84 0.9  0.99 0.9 ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 5000, 0.45, 0.38\n","Optimal Action Value Estimates:\n","[[0.89 0.9  0.91 0.9 ]\n"," [0.87 0.   0.94 0.  ]\n"," [0.91 0.94 0.98 0.  ]\n"," [0.   0.97 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.87 0.86 0.89 0.87]\n"," [0.87 0.   0.9  0.88]\n"," [0.88 0.91 0.87 0.9 ]\n"," [0.9  0.   0.84 0.85]\n"," [0.83 0.86 0.   0.87]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.94 0.   0.89]\n"," [0.   0.   0.   0.  ]\n"," [0.68 0.   0.91 0.74]\n"," [0.77 0.81 0.94 0.  ]\n"," [0.91 0.98 0.   0.91]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.72 0.97 0.76]\n"," [0.93 0.97 1.   0.95]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 6000, 0.34, 0.54\n","Optimal Action Value Estimates:\n","[[0.9  0.91 0.92 0.91]\n"," [0.92 0.   0.96 0.  ]\n"," [0.93 0.96 0.98 0.  ]\n"," [0.   0.98 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.89 0.9  0.9  0.89]\n"," [0.89 0.   0.91 0.89]\n"," [0.89 0.92 0.89 0.91]\n"," [0.91 0.   0.87 0.87]\n"," [0.89 0.92 0.   0.89]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.96 0.   0.91]\n"," [0.   0.   0.   0.  ]\n"," [0.85 0.   0.93 0.87]\n"," [0.88 0.93 0.96 0.  ]\n"," [0.93 0.98 0.   0.93]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.83 0.98 0.83]\n"," [0.95 0.98 1.   0.95]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 7000, 0.23, 0.68\n","Optimal Action Value Estimates:\n","[[0.91 0.93 0.94 0.93]\n"," [0.92 0.   0.97 0.  ]\n"," [0.93 0.97 0.99 0.  ]\n"," [0.   0.98 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.9  0.91 0.91 0.9 ]\n"," [0.9  0.   0.93 0.91]\n"," [0.91 0.94 0.9  0.92]\n"," [0.93 0.   0.87 0.88]\n"," [0.9  0.92 0.   0.91]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.97 0.   0.92]\n"," [0.   0.   0.   0.  ]\n"," [0.92 0.   0.93 0.91]\n"," [0.92 0.95 0.97 0.  ]\n"," [0.95 0.99 0.   0.94]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.87 0.98 0.86]\n"," [0.95 0.99 1.   0.97]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 8000, 0.12, 0.81\n","Optimal Action Value Estimates:\n","[[0.94 0.95 0.96 0.94]\n"," [0.93 0.   0.97 0.  ]\n"," [0.95 0.97 0.99 0.  ]\n"," [0.   0.99 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.92 0.91 0.94 0.92]\n"," [0.92 0.   0.95 0.92]\n"," [0.93 0.96 0.93 0.94]\n"," [0.94 0.   0.88 0.88]\n"," [0.9  0.93 0.   0.91]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.97 0.   0.94]\n"," [0.   0.   0.   0.  ]\n"," [0.92 0.   0.95 0.91]\n"," [0.92 0.95 0.97 0.  ]\n"," [0.95 0.99 0.   0.95]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.88 0.99 0.88]\n"," [0.97 0.99 1.   0.97]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 9000, 0.01, 0.92\n","Optimal Action Value Estimates:\n","[[0.95 0.96 0.97 0.94]\n"," [0.93 0.   0.98 0.  ]\n"," [0.95 0.98 0.99 0.  ]\n"," [0.   0.99 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.93 0.92 0.95 0.93]\n"," [0.92 0.   0.96 0.93]\n"," [0.93 0.97 0.93 0.95]\n"," [0.94 0.   0.88 0.88]\n"," [0.9  0.93 0.   0.91]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.98 0.   0.95]\n"," [0.   0.   0.   0.  ]\n"," [0.92 0.   0.95 0.92]\n"," [0.92 0.95 0.98 0.  ]\n"," [0.95 0.99 0.   0.96]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.88 0.99 0.88]\n"," [0.97 0.99 1.   0.97]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.98\n","Optimal Action Value Estimates:\n","[[0.95 0.96 0.97 0.95]\n"," [0.94 0.   0.98 0.  ]\n"," [0.95 0.98 0.99 0.  ]\n"," [0.   0.99 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.93 0.92 0.95 0.93]\n"," [0.93 0.   0.96 0.94]\n"," [0.94 0.97 0.93 0.95]\n"," [0.95 0.   0.88 0.88]\n"," [0.9  0.94 0.   0.91]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.98 0.   0.95]\n"," [0.   0.   0.   0.  ]\n"," [0.92 0.   0.95 0.92]\n"," [0.92 0.95 0.98 0.  ]\n"," [0.95 0.99 0.   0.96]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.88 0.99 0.88]\n"," [0.97 0.99 1.   0.97]\n"," [0.   0.   0.   0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"OKktGDk_UIvd","colab_type":"code","outputId":"17076f06-2ea4-435e-d45c-5b79afd0c576","executionInfo":{"status":"ok","timestamp":1555641348742,"user_tz":240,"elapsed":561979,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"cell_type":"code","source":["print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","optimal_action_estimates = np.max(action_value_array,axis=1)\n","print(\"Optimal Action Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(4,4),2))\n","print(\"estimate of the optimal state value at each state\")\n","print(\"\")\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_value_array.reshape((16,4)),2))\n","print(\"\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.98\n","Optimal Action Value Estimates:\n","[[0.95 0.96 0.97 0.95]\n"," [0.94 0.   0.98 0.  ]\n"," [0.95 0.98 0.99 0.  ]\n"," [0.   0.99 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.93 0.92 0.95 0.93]\n"," [0.93 0.   0.96 0.94]\n"," [0.94 0.97 0.93 0.95]\n"," [0.95 0.   0.88 0.88]\n"," [0.9  0.94 0.   0.91]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.98 0.   0.95]\n"," [0.   0.   0.   0.  ]\n"," [0.92 0.   0.95 0.92]\n"," [0.92 0.95 0.98 0.  ]\n"," [0.95 0.99 0.   0.96]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.88 0.99 0.88]\n"," [0.97 0.99 1.   0.97]\n"," [0.   0.   0.   0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"5z58TlVUXyoP","colab_type":"text"},"cell_type":"markdown","source":["** Example 2: FrozenLake 4x4 Slippery **\n","\n","Standard FrozenLake env where slippery is enabled. Notice the increased failure rate and lower Q values"]},{"metadata":{"id":"FbqZr9dAUOhm","colab_type":"code","colab":{}},"cell_type":"code","source":["#hyperparameters\n","episodes = 10000\n","learning_rate = 0.01\n","discount = 1.\n","lambda_val = 0.5\n","epsilon_start = 1.0\n","epsilon_min = 0.01\n","epsilon_step = (epsilon_start - epsilon_min)/(episodes*.9)\n","\n","seed = 31\n","env = gym.make('FrozenLake-v0')\n","env.seed(seed)\n","np.random.seed(seed)\n","num_actions = env.action_space.n\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","q_value_ = tf.placeholder(dtype=tf.float32, shape=[None, 1, num_actions], name=\"q_value\")\n","action_ = tf.placeholder(dtype=tf.int32, shape=[None, 1], name=\"action\")\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","q_next_ = tf.placeholder(dtype=tf.float32, shape=[None, 1, num_actions], name=\"q_next\")\n","lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\")\n","\n","q_lambda_return_ = trfl.qlambda(q_value_, action_, reward_, discount_, q_next_, lambda_)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"017M07L0-eNx","colab_type":"code","outputId":"4e2d9fb9-bd48-4cb7-dbcd-f299e208087d","executionInfo":{"status":"ok","timestamp":1555642038857,"user_tz":240,"elapsed":1249275,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":4474}},"cell_type":"code","source":["stats_success = []\n","epsilon = epsilon_start\n","\n","with tf.Session() as sess:\n","  #initialize the estimated state values to zero\n","  action_value_array = np.zeros((16,num_actions))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  q_list, action_list, reward_list, q_next_list, state_int_list = [], [], [], [], []\n","\n","  while current_episode < episodes:\n","    #take epsilon greedy action\n","    if np.random.rand() < epsilon:\n","      action = env.action_space.sample()\n","    else:\n","      #Choose a greedy action. If multiple greedy actions randomly choose between\n","      max_actions = np.argwhere(action_value_array[current_state] == np.max(action_value_array[current_state])).reshape((-1))\n","      action = np.random.choice(max_actions)\n","\n","    next_state, rew, done, info = env.step(action)\n","      \n","    q_list.append(action_value_array[current_state])\n","    reward_list.append(rew)\n","    action_list.append(action)\n","    q_next_list.append(action_value_array[next_state])\n","    state_int_list.append(current_state)\n","    \n","    current_state = next_state\n","    \n","    q_lambda_output = sess.run(q_lambda_return_, feed_dict={\n","        q_value_:np.array(q_list).reshape(-1,1,num_actions),\n","        action_:np.array(action_list).reshape(-1,1),\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(q_list)).reshape(-1,1),\n","        q_next_:np.array(q_next_list).reshape(-1,1,num_actions),\n","        lambda_:np.array([lambda_val]*len(q_list)).reshape(-1,1),\n","      })\n","\n","    #action_value_array[state_int_list, action_list] += np.squeeze(learning_rate*q_lambda_output.extra.td_error)\n","    for s, a, td in zip(state_int_list,action_list,q_lambda_output.extra.td_error.tolist()):\n","      action_value_array[s,a] += learning_rate*td[0]\n","\n","    if done:\n","      if next_state == 15:\n","        stats_success.append(1)\n","      else:\n","        stats_success.append(0)\n","        \n","      q_list, action_list, reward_list, q_next_list, state_int_list = [], [], [], [], []\n","      current_state = env.reset()\n","      current_episode += 1\n","      epsilon -= epsilon_step\n","      if epsilon < epsilon_min:\n","        epsilon = epsilon_min\n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","        optimal_action_estimates = np.max(action_value_array,axis=1)\n","        print(\"Optimal Action Value Estimates:\")\n","        print(np.round(optimal_action_estimates.reshape(4,4),2))\n","        print(\"estimate of the optimal state value at each state\")\n","        print(\"\")\n","        print(\"All Action Value Estimates:\")\n","        print(np.round(action_value_array.reshape((16,4)),2))\n","        print(\"\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 1000, 0.89, 0.01\n","Optimal Action Value Estimates:\n","[[0.01 0.01 0.01 0.01]\n"," [0.01 0.   0.01 0.  ]\n"," [0.02 0.02 0.02 0.  ]\n"," [0.   0.02 0.05 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.01 0.01 0.01 0.01]\n"," [0.01 0.01 0.01 0.01]\n"," [0.01 0.01 0.01 0.01]\n"," [0.   0.   0.   0.01]\n"," [0.01 0.01 0.01 0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.01 0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.01 0.01 0.02]\n"," [0.01 0.02 0.01 0.01]\n"," [0.01 0.02 0.02 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.01 0.02 0.02]\n"," [0.01 0.04 0.04 0.05]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 2000, 0.78, 0.03\n","Optimal Action Value Estimates:\n","[[0.04 0.03 0.03 0.02]\n"," [0.04 0.   0.03 0.  ]\n"," [0.04 0.06 0.06 0.  ]\n"," [0.   0.1  0.14 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.04 0.03 0.03 0.03]\n"," [0.03 0.03 0.03 0.03]\n"," [0.03 0.03 0.02 0.02]\n"," [0.02 0.02 0.02 0.02]\n"," [0.04 0.03 0.03 0.03]\n"," [0.   0.   0.   0.  ]\n"," [0.03 0.01 0.03 0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.03 0.04 0.04 0.04]\n"," [0.04 0.06 0.05 0.03]\n"," [0.04 0.06 0.06 0.02]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.03 0.04 0.1  0.06]\n"," [0.05 0.13 0.12 0.14]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 3000, 0.67, 0.04\n","Optimal Action Value Estimates:\n","[[0.1  0.09 0.08 0.07]\n"," [0.1  0.   0.09 0.  ]\n"," [0.11 0.12 0.14 0.  ]\n"," [0.   0.15 0.23 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.1  0.09 0.09 0.09]\n"," [0.08 0.08 0.08 0.09]\n"," [0.08 0.07 0.08 0.08]\n"," [0.06 0.05 0.06 0.07]\n"," [0.1  0.09 0.09 0.09]\n"," [0.   0.   0.   0.  ]\n"," [0.09 0.06 0.07 0.05]\n"," [0.   0.   0.   0.  ]\n"," [0.09 0.1  0.1  0.11]\n"," [0.1  0.12 0.11 0.09]\n"," [0.11 0.14 0.1  0.06]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.09 0.12 0.15 0.15]\n"," [0.11 0.18 0.17 0.23]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 4000, 0.56, 0.03\n","Optimal Action Value Estimates:\n","[[0.15 0.14 0.13 0.12]\n"," [0.15 0.   0.15 0.  ]\n"," [0.17 0.18 0.2  0.  ]\n"," [0.   0.2  0.24 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.15 0.15 0.14 0.15]\n"," [0.14 0.14 0.12 0.14]\n"," [0.13 0.13 0.13 0.13]\n"," [0.1  0.1  0.11 0.12]\n"," [0.15 0.15 0.14 0.14]\n"," [0.   0.   0.   0.  ]\n"," [0.15 0.1  0.11 0.07]\n"," [0.   0.   0.   0.  ]\n"," [0.15 0.15 0.14 0.17]\n"," [0.15 0.18 0.15 0.15]\n"," [0.15 0.2  0.13 0.12]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.15 0.17 0.2  0.17]\n"," [0.17 0.22 0.24 0.22]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 5000, 0.45, 0.06\n","Optimal Action Value Estimates:\n","[[0.24 0.22 0.21 0.19]\n"," [0.25 0.   0.2  0.  ]\n"," [0.27 0.29 0.3  0.  ]\n"," [0.   0.32 0.36 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.24 0.24 0.23 0.23]\n"," [0.22 0.2  0.19 0.21]\n"," [0.21 0.2  0.2  0.19]\n"," [0.12 0.15 0.11 0.19]\n"," [0.25 0.23 0.22 0.23]\n"," [0.   0.   0.   0.  ]\n"," [0.2  0.15 0.15 0.09]\n"," [0.   0.   0.   0.  ]\n"," [0.22 0.23 0.23 0.27]\n"," [0.21 0.29 0.24 0.22]\n"," [0.24 0.3  0.17 0.16]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.21 0.22 0.32 0.24]\n"," [0.25 0.32 0.36 0.29]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 6000, 0.34, 0.09\n","Optimal Action Value Estimates:\n","[[0.32 0.31 0.3  0.3 ]\n"," [0.32 0.   0.29 0.  ]\n"," [0.33 0.36 0.34 0.  ]\n"," [0.   0.38 0.43 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.32 0.32 0.32 0.32]\n"," [0.31 0.3  0.3  0.31]\n"," [0.3  0.3  0.28 0.29]\n"," [0.25 0.24 0.24 0.3 ]\n"," [0.32 0.32 0.29 0.31]\n"," [0.   0.   0.   0.  ]\n"," [0.29 0.2  0.25 0.17]\n"," [0.   0.   0.   0.  ]\n"," [0.3  0.33 0.32 0.33]\n"," [0.32 0.36 0.3  0.29]\n"," [0.32 0.34 0.28 0.3 ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.3  0.33 0.38 0.28]\n"," [0.37 0.38 0.43 0.37]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 7000, 0.23, 0.16\n","Optimal Action Value Estimates:\n","[[0.43 0.41 0.4  0.39]\n"," [0.43 0.   0.38 0.  ]\n"," [0.43 0.45 0.47 0.  ]\n"," [0.   0.47 0.53 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.43 0.42 0.42 0.42]\n"," [0.39 0.37 0.38 0.41]\n"," [0.4  0.38 0.38 0.39]\n"," [0.34 0.25 0.3  0.39]\n"," [0.43 0.4  0.41 0.4 ]\n"," [0.   0.   0.   0.  ]\n"," [0.38 0.23 0.34 0.23]\n"," [0.   0.   0.   0.  ]\n"," [0.4  0.4  0.4  0.43]\n"," [0.39 0.45 0.43 0.4 ]\n"," [0.44 0.47 0.38 0.38]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.42 0.4  0.47 0.38]\n"," [0.43 0.47 0.53 0.45]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 8000, 0.12, 0.23\n","Optimal Action Value Estimates:\n","[[0.55 0.54 0.55 0.55]\n"," [0.55 0.   0.55 0.  ]\n"," [0.56 0.57 0.56 0.  ]\n"," [0.   0.58 0.61 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.55 0.55 0.55 0.55]\n"," [0.53 0.53 0.53 0.54]\n"," [0.54 0.53 0.55 0.55]\n"," [0.53 0.53 0.5  0.55]\n"," [0.55 0.52 0.55 0.53]\n"," [0.   0.   0.   0.  ]\n"," [0.55 0.31 0.39 0.33]\n"," [0.   0.   0.   0.  ]\n"," [0.52 0.53 0.54 0.56]\n"," [0.52 0.57 0.53 0.52]\n"," [0.56 0.48 0.46 0.52]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.52 0.53 0.58 0.56]\n"," [0.52 0.61 0.55 0.55]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 9000, 0.01, 0.43\n","Optimal Action Value Estimates:\n","[[0.72 0.72 0.71 0.71]\n"," [0.72 0.   0.7  0.  ]\n"," [0.72 0.73 0.72 0.  ]\n"," [0.   0.73 0.74 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.72 0.7  0.7  0.69]\n"," [0.62 0.67 0.62 0.72]\n"," [0.64 0.66 0.65 0.71]\n"," [0.64 0.63 0.67 0.71]\n"," [0.72 0.69 0.68 0.7 ]\n"," [0.   0.   0.   0.  ]\n"," [0.7  0.36 0.49 0.57]\n"," [0.   0.   0.   0.  ]\n"," [0.61 0.66 0.67 0.72]\n"," [0.65 0.73 0.6  0.67]\n"," [0.72 0.61 0.58 0.62]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.62 0.67 0.73 0.67]\n"," [0.64 0.74 0.64 0.63]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.62\n","Optimal Action Value Estimates:\n","[[0.81 0.8  0.8  0.8 ]\n"," [0.81 0.   0.8  0.  ]\n"," [0.81 0.81 0.81 0.  ]\n"," [0.   0.81 0.84 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.81 0.8  0.79 0.8 ]\n"," [0.71 0.79 0.79 0.8 ]\n"," [0.77 0.78 0.78 0.8 ]\n"," [0.76 0.79 0.78 0.8 ]\n"," [0.81 0.78 0.79 0.79]\n"," [0.   0.   0.   0.  ]\n"," [0.8  0.39 0.52 0.57]\n"," [0.   0.   0.   0.  ]\n"," [0.75 0.77 0.75 0.81]\n"," [0.66 0.81 0.72 0.74]\n"," [0.81 0.63 0.73 0.6 ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.73 0.69 0.81 0.73]\n"," [0.71 0.84 0.73 0.71]\n"," [0.   0.   0.   0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"gHSP5ubAVAqt","colab_type":"code","outputId":"ee05b263-0787-4efb-9865-6b19be70bc9f","executionInfo":{"status":"ok","timestamp":1555642038859,"user_tz":240,"elapsed":1248250,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"cell_type":"code","source":["print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","optimal_action_estimates = np.max(action_value_array,axis=1)\n","print(\"Optimal Action Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(4,4),2))\n","print(\"estimate of the optimal state value at each state\")\n","print(\"\")\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_value_array.reshape((16,4)),2))\n","print(\"\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.62\n","Optimal Action Value Estimates:\n","[[0.81 0.8  0.8  0.8 ]\n"," [0.81 0.   0.8  0.  ]\n"," [0.81 0.81 0.81 0.  ]\n"," [0.   0.81 0.84 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.81 0.8  0.79 0.8 ]\n"," [0.71 0.79 0.79 0.8 ]\n"," [0.77 0.78 0.78 0.8 ]\n"," [0.76 0.79 0.78 0.8 ]\n"," [0.81 0.78 0.79 0.79]\n"," [0.   0.   0.   0.  ]\n"," [0.8  0.39 0.52 0.57]\n"," [0.   0.   0.   0.  ]\n"," [0.75 0.77 0.75 0.81]\n"," [0.66 0.81 0.72 0.74]\n"," [0.81 0.63 0.73 0.6 ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.73 0.69 0.81 0.73]\n"," [0.71 0.84 0.73 0.71]\n"," [0.   0.   0.   0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"Z2_A-Q6ea5W9","colab_type":"text"},"cell_type":"markdown","source":["** Example 3: FrozenLake 8x8 **\n","\n","FrozenLake on an 8x8 grid. Much harder to randomly find the goal. To make learning faster, we add a penalty for falling into a hole."]},{"metadata":{"id":"eQG7BVhNa4LY","colab_type":"code","colab":{}},"cell_type":"code","source":["#hyperparameters\n","episodes = 20000\n","learning_rate = 0.01\n","discount = 0.99\n","lambda_val = 0.75\n","epsilon_start = 1.0\n","epsilon_min = 0.01\n","epsilon_step = (epsilon_start - epsilon_min)/(episodes*.9)\n","hole_penalty = -0.1 #penalty for falling into a hole\n","\n","seed = 31\n","env = gym.make('FrozenLake8x8-v0')\n","env.seed(seed)\n","np.random.seed(seed)\n","num_actions = env.action_space.n\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","q_value_ = tf.placeholder(dtype=tf.float32, shape=[None, 1, num_actions], name=\"q_value\")\n","action_ = tf.placeholder(dtype=tf.int32, shape=[None, 1], name=\"action\")\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","q_next_ = tf.placeholder(dtype=tf.float32, shape=[None, 1, num_actions], name=\"q_next\")\n","lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\")\n","\n","q_lambda_return_ = trfl.qlambda(q_value_, action_, reward_, discount_, q_next_, lambda_)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Krd5YoIzeUUz","colab_type":"code","outputId":"bde052f5-0765-4408-821e-65e9d25f6bf2","executionInfo":{"status":"ok","timestamp":1553378845407,"user_tz":240,"elapsed":1247488,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":24086}},"cell_type":"code","source":["stats_success = []\n","epsilon = epsilon_start\n","\n","with tf.Session() as sess:\n","  #initialize the estimated state values to zero\n","  action_value_array = np.zeros((64,num_actions))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  q_list, action_list, reward_list, q_next_list, state_int_list = [], [], [], [], []\n","\n","  while current_episode < episodes:\n","    #take epsilon greedy action\n","    if np.random.rand() < epsilon:\n","      action = env.action_space.sample()\n","    else:\n","      #Choose a greedy action. If multiple greedy actions randomly choose between\n","      max_actions = np.argwhere(action_value_array[current_state] == np.max(action_value_array[current_state])).reshape((-1))\n","      action = np.random.choice(max_actions)\n","\n","    next_state, rew, done, info = env.step(action)\n","      \n","    if done and rew < 1:\n","      rew = hole_penalty\n","      \n","    q_list.append(action_value_array[current_state])\n","    reward_list.append(rew)\n","    action_list.append(action)\n","    q_next_list.append(action_value_array[next_state])\n","    state_int_list.append(current_state)\n","    \n","    current_state = next_state\n","    \n","    q_lambda_output = sess.run(q_lambda_return_, feed_dict={\n","        q_value_:np.array(q_list).reshape(-1,1,num_actions),\n","        action_:np.array(action_list).reshape(-1,1),\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(q_list)).reshape(-1,1),\n","        q_next_:np.array(q_next_list).reshape(-1,1,num_actions),\n","        lambda_:np.array([lambda_val]*len(q_list)).reshape(-1,1),\n","      })\n","\n","    #action_value_array[state_int_list, action_list] += np.squeeze(learning_rate*q_lambda_output.extra.td_error)\n","    for s, a, td in zip(state_int_list,action_list,q_lambda_output.extra.td_error.tolist()):\n","      action_value_array[s,a] += learning_rate*td[0]\n","\n","    if done:\n","      if next_state == 63:\n","        stats_success.append(1)\n","      else:\n","        stats_success.append(0)\n","        \n","      q_list, action_list, reward_list, q_next_list, state_int_list = [], [], [], [], []\n","      current_state = env.reset()\n","      current_episode += 1\n","      epsilon -= epsilon_step\n","      if epsilon < epsilon_min:\n","        epsilon = epsilon_min\n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","        optimal_action_estimates = np.max(action_value_array,axis=1)\n","        print(\"Optimal Action Value Estimates:\")\n","        print(np.round(optimal_action_estimates.reshape(8,8),2))\n","        print(\"estimate of the optimal state value at each state\")\n","        print(\"\")\n","        print(\"All Action Value Estimates:\")\n","        print(np.round(action_value_array.reshape((64,4)),2))\n","        print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 1000, 0.95, 0.00\n","Optimal Action Value Estimates:\n","[[-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.    0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.01 -0.01 -0.01  0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01  0.   -0.   -0.   -0.    0.  ]\n"," [-0.01  0.    0.    0.    0.    0.    0.    0.  ]\n"," [-0.01  0.    0.    0.    0.    0.    0.    0.01]\n"," [-0.   -0.    0.    0.    0.    0.    0.    0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.01]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.01]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.01 -0.  ]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [-0.01 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.    0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [ 0.   -0.   -0.   -0.  ]\n"," [-0.   -0.    0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.   -0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [-0.01 -0.   -0.   -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 2000, 0.89, 0.00\n","Optimal Action Value Estimates:\n","[[-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.    0.   -0.01 -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.01 -0.01 -0.01  0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01  0.   -0.01 -0.   -0.    0.  ]\n"," [-0.01  0.    0.    0.   -0.   -0.    0.    0.01]\n"," [-0.01  0.    0.    0.    0.   -0.    0.    0.02]\n"," [-0.01 -0.01 -0.    0.    0.    0.    0.    0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.   -0.01 -0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.02 -0.01 -0.03 -0.02]\n"," [-0.01 -0.02 -0.01 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.   -0.   -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [-0.01 -0.02 -0.02 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.   -0.01 -0.01]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.   -0.01  0.   -0.  ]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01 -0.    0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.   -0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.01 -0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.   -0.    0.    0.  ]\n"," [ 0.    0.   -0.    0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 3000, 0.84, 0.00\n","Optimal Action Value Estimates:\n","[[-0.   -0.   -0.   -0.   -0.   -0.   -0.    0.  ]\n"," [-0.   -0.   -0.   -0.   -0.   -0.    0.    0.  ]\n"," [-0.   -0.   -0.    0.   -0.01 -0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.01 -0.01  0.    0.    0.  ]\n"," [-0.   -0.01 -0.01  0.   -0.01 -0.01  0.    0.  ]\n"," [-0.01  0.    0.    0.   -0.    0.    0.    0.01]\n"," [-0.01  0.    0.    0.    0.    0.    0.    0.02]\n"," [-0.01 -0.01 -0.    0.    0.    0.01  0.01  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.    0.   -0.  ]\n"," [ 0.   -0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [-0.    0.    0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.   -0.01 -0.   -0.01]\n"," [-0.02 -0.02 -0.04 -0.01]\n"," [-0.01 -0.02 -0.02 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.01 -0.01 -0.02 -0.01]\n"," [-0.01 -0.02 -0.01 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.    0.  ]\n"," [ 0.   -0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.    0.  ]\n"," [-0.   -0.01 -0.01 -0.  ]\n"," [-0.    0.   -0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.01  0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.   -0.  ]\n"," [-0.    0.   -0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.01  0.01  0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [-0.    0.01  0.    0.  ]\n"," [ 0.    0.   -0.    0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 4000, 0.78, 0.00\n","Optimal Action Value Estimates:\n","[[-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.    0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.01 -0.01  0.   -0.   -0.  ]\n"," [-0.01 -0.   -0.01  0.   -0.01 -0.01 -0.    0.  ]\n"," [-0.01  0.    0.   -0.   -0.01 -0.    0.    0.01]\n"," [-0.01  0.   -0.    0.    0.    0.    0.    0.02]\n"," [-0.01 -0.01 -0.    0.    0.    0.01  0.01  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.01 -0.   -0.01 -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.   -0.   -0.01]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.01 -0.01 -0.03 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [-0.01 -0.03 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.02 -0.01 -0.01 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.    0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.   -0.  ]\n"," [-0.01 -0.02 -0.02 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.01  0.02 -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [-0.    0.01  0.    0.  ]\n"," [ 0.    0.   -0.    0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 5000, 0.73, 0.00\n","Optimal Action Value Estimates:\n","[[-0.   -0.   -0.   -0.   -0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.   -0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.    0.   -0.   -0.    0.    0.  ]\n"," [-0.   -0.   -0.01 -0.01 -0.01  0.    0.    0.  ]\n"," [-0.   -0.   -0.01  0.   -0.01 -0.01 -0.    0.01]\n"," [-0.01  0.    0.   -0.   -0.01 -0.    0.    0.02]\n"," [-0.01  0.   -0.    0.    0.    0.    0.    0.04]\n"," [-0.01 -0.01 -0.01  0.    0.    0.01  0.01  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.    0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.    0.   -0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.    0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.03 -0.01]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01  0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.01 -0.02 -0.03 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.02 -0.01 -0.01 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.   -0.01 -0.01]\n"," [-0.    0.01  0.    0.  ]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.01 -0.  ]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.04  0.01  0.03 -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [-0.    0.01  0.    0.  ]\n"," [ 0.    0.   -0.    0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 6000, 0.67, 0.01\n","Optimal Action Value Estimates:\n","[[ 0.    0.    0.    0.    0.01  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.    0.    0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.    0.    0.    0.01  0.01]\n"," [ 0.    0.    0.   -0.   -0.01  0.    0.01  0.02]\n"," [-0.   -0.   -0.01  0.   -0.   -0.    0.01  0.03]\n"," [-0.01  0.    0.   -0.   -0.01 -0.    0.    0.06]\n"," [-0.01  0.   -0.   -0.    0.    0.    0.    0.11]\n"," [-0.01 -0.01 -0.01  0.    0.    0.01  0.01  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.   -0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.   -0.   -0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.    0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [-0.   -0.    0.    0.  ]\n"," [ 0.   -0.    0.    0.  ]\n"," [-0.   -0.   -0.    0.  ]\n"," [-0.02 -0.   -0.03 -0.01]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.    0.01  0.01]\n"," [ 0.01  0.01  0.02  0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.01 -0.03 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.02 -0.01 -0.   -0.02]\n"," [-0.01 -0.   -0.01 -0.  ]\n"," [-0.    0.01  0.01  0.01]\n"," [ 0.02  0.03  0.03  0.03]\n"," [-0.01 -0.02 -0.01 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.06  0.03 -0.  ]\n"," [-0.01 -0.02 -0.02 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.11  0.04  0.04  0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.02 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [-0.    0.01  0.    0.  ]\n"," [ 0.    0.   -0.    0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 7000, 0.62, 0.01\n","Optimal Action Value Estimates:\n","[[ 0.02  0.02  0.02  0.02  0.02  0.03  0.03  0.03]\n"," [ 0.02  0.02  0.02  0.02  0.02  0.02  0.03  0.03]\n"," [ 0.02  0.02  0.02  0.    0.02  0.02  0.03  0.03]\n"," [ 0.02  0.01  0.01 -0.    0.01  0.    0.03  0.04]\n"," [ 0.01  0.01  0.01  0.   -0.01  0.01  0.02  0.04]\n"," [ 0.    0.    0.   -0.   -0.01 -0.01  0.    0.1 ]\n"," [-0.01  0.   -0.   -0.    0.    0.01  0.    0.17]\n"," [-0.01 -0.01 -0.01  0.    0.    0.01  0.02  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.01  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.01  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.02  0.01  0.02  0.02]\n"," [ 0.02  0.01  0.01  0.02]\n"," [ 0.02  0.01  0.02  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.    0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.02  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.    0.01  0.01]\n"," [-0.02 -0.   -0.03 -0.01]\n"," [-0.   -0.02  0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.03  0.01  0.03  0.03]\n"," [ 0.04  0.04  0.03  0.04]\n"," [ 0.    0.01  0.01  0.01]\n"," [ 0.01  0.    0.01  0.01]\n"," [-0.01 -0.02 -0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.02]\n"," [-0.01  0.01 -0.   -0.  ]\n"," [ 0.01  0.02  0.02  0.02]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.   -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.05  0.1   0.06  0.04]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01 -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.17  0.05  0.07  0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.02 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [-0.    0.01  0.    0.  ]\n"," [ 0.    0.   -0.    0.02]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 8000, 0.56, 0.01\n","Optimal Action Value Estimates:\n","[[ 0.01  0.01  0.02  0.02  0.02  0.02  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.01  0.02  0.02  0.03  0.02]\n"," [ 0.01  0.01  0.01  0.    0.02  0.02  0.03  0.03]\n"," [ 0.01  0.01  0.01 -0.    0.    0.    0.03  0.03]\n"," [ 0.01  0.01  0.    0.    0.01  0.    0.03  0.04]\n"," [ 0.01  0.    0.   -0.01 -0.01  0.    0.    0.1 ]\n"," [ 0.    0.   -0.   -0.    0.    0.01  0.    0.19]\n"," [-0.01 -0.01 -0.01  0.    0.    0.02  0.03  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.02  0.01]\n"," [ 0.01  0.01  0.02  0.01]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.03  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.01  0.01  0.02]\n"," [ 0.01  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.03  0.02]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.  ]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [ 0.   -0.02  0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.01  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.   -0.    0.    0.01]\n"," [-0.   -0.   -0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01  0.01 -0.  ]\n"," [-0.    0.   -0.    0.  ]\n"," [ 0.01  0.01  0.03  0.02]\n"," [ 0.03  0.03  0.04  0.03]\n"," [ 0.   -0.01  0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.1   0.09  0.08  0.04]\n"," [ 0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01 -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.19  0.07  0.09  0.04]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.02 -0.01 -0.02 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [-0.    0.02  0.   -0.  ]\n"," [ 0.    0.    0.01  0.03]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 9000, 0.51, 0.01\n","Optimal Action Value Estimates:\n","[[ 0.01  0.01  0.01  0.01  0.01  0.02  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.01  0.01  0.02  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.    0.01  0.01  0.02  0.02]\n"," [ 0.01  0.01  0.   -0.    0.01  0.    0.02  0.02]\n"," [ 0.01  0.   -0.    0.   -0.    0.01  0.03  0.03]\n"," [ 0.    0.    0.   -0.01 -0.01 -0.    0.    0.08]\n"," [-0.01  0.   -0.   -0.    0.    0.01  0.    0.17]\n"," [-0.01 -0.02 -0.01  0.    0.    0.03  0.03  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.02  0.01]\n"," [ 0.01  0.01  0.02  0.01]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.02]\n"," [ 0.02  0.02  0.02  0.01]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01 -0.    0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.    0.    0.01]\n"," [ 0.    0.   -0.    0.  ]\n"," [-0.01 -0.   -0.02 -0.  ]\n"," [-0.   -0.01 -0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.    0.    0.01  0.  ]\n"," [-0.    0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.    0.01 -0.   -0.  ]\n"," [ 0.01  0.02  0.02  0.03]\n"," [ 0.02  0.03  0.03  0.02]\n"," [ 0.   -0.01  0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.01 -0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.06  0.07  0.08  0.04]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01 -0.   -0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.17  0.08  0.1   0.04]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [-0.    0.03  0.   -0.  ]\n"," [ 0.    0.01  0.01  0.03]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 10000, 0.45, 0.02\n","Optimal Action Value Estimates:\n","[[ 0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02  0.02  0.02  0.03  0.03]\n"," [ 0.02  0.02  0.02  0.    0.02  0.02  0.03  0.03]\n"," [ 0.01  0.01  0.01  0.01  0.01  0.    0.03  0.03]\n"," [ 0.01  0.01  0.    0.    0.    0.01  0.03  0.04]\n"," [ 0.    0.    0.   -0.01 -0.    0.    0.    0.1 ]\n"," [-0.    0.   -0.   -0.    0.    0.01  0.    0.18]\n"," [-0.   -0.01 -0.01  0.    0.    0.01  0.02  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.01  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.03  0.02  0.02]\n"," [ 0.03  0.02  0.03  0.03]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.01  0.02  0.01  0.02]\n"," [ 0.02  0.01  0.01  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.    0.02  0.02]\n"," [ 0.01  0.02  0.02  0.02]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.    0.01  0.01]\n"," [-0.    0.01 -0.01  0.  ]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.   -0.01  0.    0.01]\n"," [ 0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01 -0.    0.  ]\n"," [-0.01  0.01 -0.   -0.  ]\n"," [ 0.01  0.02  0.03  0.03]\n"," [ 0.03  0.04  0.04  0.03]\n"," [ 0.   -0.02  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.02 -0.01 -0.01 -0.  ]\n"," [ 0.   -0.   -0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.06  0.1   0.07  0.04]\n"," [-0.   -0.01 -0.01 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01 -0.   -0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.18  0.07  0.12  0.05]\n"," [-0.   -0.01 -0.02 -0.01]\n"," [-0.01 -0.01 -0.02 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.01 -0.  ]\n"," [ 0.    0.01  0.01  0.02]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 11000, 0.40, 0.02\n","Optimal Action Value Estimates:\n","[[ 0.01  0.02  0.02  0.02  0.02  0.02  0.02  0.02]\n"," [ 0.01  0.02  0.02  0.02  0.02  0.02  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.    0.02  0.02  0.02  0.03]\n"," [ 0.01  0.01  0.01  0.01  0.02  0.    0.03  0.03]\n"," [ 0.01  0.01  0.01  0.    0.01  0.02  0.03  0.04]\n"," [ 0.    0.    0.   -0.01  0.    0.01  0.    0.1 ]\n"," [-0.    0.   -0.   -0.    0.    0.02  0.    0.19]\n"," [-0.   -0.01 -0.01  0.    0.02  0.04  0.05  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.02  0.01  0.01]\n"," [ 0.02  0.01  0.02  0.01]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.02  0.01]\n"," [ 0.01  0.01  0.02  0.01]\n"," [ 0.02  0.02  0.01  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.03  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01 -0.   -0.  ]\n"," [ 0.02  0.    0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.03  0.02  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01 -0.01 -0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01  0.    0.01]\n"," [-0.02  0.02 -0.01  0.01]\n"," [ 0.02  0.02  0.03  0.03]\n"," [ 0.04  0.03  0.04  0.03]\n"," [-0.   -0.01  0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.02 -0.01 -0.01  0.  ]\n"," [ 0.01 -0.   -0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.08  0.1   0.09  0.06]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02 -0.   -0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.19  0.12  0.14  0.06]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.02 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.02 -0.  ]\n"," [ 0.    0.04  0.01  0.01]\n"," [ 0.    0.01  0.03  0.05]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 12000, 0.34, 0.02\n","Optimal Action Value Estimates:\n","[[ 0.03  0.03  0.03  0.03  0.03  0.03  0.04  0.04]\n"," [ 0.03  0.03  0.03  0.03  0.03  0.03  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.    0.02  0.04  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.01  0.01  0.    0.05  0.06]\n"," [ 0.01  0.01  0.01  0.    0.01  0.02  0.05  0.08]\n"," [ 0.01  0.    0.   -0.01  0.    0.02  0.    0.1 ]\n"," [ 0.01  0.   -0.   -0.    0.    0.02  0.    0.16]\n"," [ 0.01 -0.01 -0.01  0.    0.02  0.05  0.06  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.02  0.03  0.03  0.03]\n"," [ 0.02  0.03  0.03  0.03]\n"," [ 0.02  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.04  0.03  0.04]\n"," [ 0.04  0.03  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.03]\n"," [ 0.02  0.02  0.02  0.03]\n"," [ 0.02  0.02  0.03  0.03]\n"," [ 0.02  0.02  0.02  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.04  0.03  0.03  0.03]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.01  0.02  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.01  0.02  0.02]\n"," [ 0.02  0.02  0.04  0.03]\n"," [ 0.03  0.03  0.04  0.03]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.01  0.01  0.02]\n"," [ 0.01  0.01  0.01  0.02]\n"," [ 0.    0.01 -0.    0.  ]\n"," [ 0.01  0.01  0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.03  0.05  0.04  0.04]\n"," [ 0.06  0.05  0.05  0.04]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01 -0.01 -0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01  0.01  0.01  0.01]\n"," [-0.01  0.02  0.    0.01]\n"," [ 0.02  0.02  0.05  0.03]\n"," [ 0.06  0.08  0.06  0.05]\n"," [ 0.01  0.01  0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.02 -0.   -0.    0.  ]\n"," [ 0.02 -0.    0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.08  0.1   0.1   0.05]\n"," [ 0.01 -0.01  0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.    0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.15  0.13  0.16  0.09]\n"," [ 0.01 -0.01 -0.01  0.  ]\n"," [-0.01 -0.01 -0.02 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.02 -0.  ]\n"," [ 0.    0.05  0.01  0.01]\n"," [-0.    0.01  0.03  0.06]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 13000, 0.29, 0.03\n","Optimal Action Value Estimates:\n","[[ 0.03  0.03  0.03  0.03  0.04  0.04  0.04  0.04]\n"," [ 0.03  0.03  0.03  0.03  0.04  0.04  0.04  0.04]\n"," [ 0.03  0.03  0.02  0.    0.04  0.04  0.04  0.05]\n"," [ 0.02  0.02  0.02  0.01  0.03  0.    0.05  0.05]\n"," [ 0.02  0.01  0.01  0.    0.02  0.04  0.05  0.08]\n"," [ 0.01  0.    0.   -0.01  0.01  0.03  0.    0.16]\n"," [ 0.    0.   -0.   -0.    0.    0.02  0.    0.25]\n"," [ 0.   -0.01 -0.01  0.    0.02  0.05  0.06  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.04  0.04  0.04  0.03]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.02  0.03]\n"," [ 0.02  0.01  0.02  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.04  0.03  0.04  0.04]\n"," [ 0.03  0.04  0.03  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.05  0.05  0.05  0.05]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.01  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.01]\n"," [ 0.    0.01  0.01  0.01]\n"," [ 0.03  0.    0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.04  0.04  0.05  0.04]\n"," [ 0.05  0.05  0.05  0.05]\n"," [ 0.01  0.01  0.01  0.02]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.02  0.02]\n"," [ 0.    0.04  0.01  0.02]\n"," [ 0.02  0.05  0.04  0.05]\n"," [ 0.07  0.08  0.06  0.06]\n"," [ 0.01  0.    0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.02 -0.    0.    0.01]\n"," [ 0.03  0.    0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.07  0.16  0.1   0.06]\n"," [ 0.   -0.01  0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.    0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.2   0.17  0.25  0.09]\n"," [ 0.   -0.   -0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.02 -0.  ]\n"," [ 0.    0.05  0.01  0.01]\n"," [-0.    0.01  0.03  0.06]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 14000, 0.23, 0.04\n","Optimal Action Value Estimates:\n","[[ 0.04  0.04  0.04  0.05  0.05  0.06  0.06  0.07]\n"," [ 0.04  0.04  0.04  0.04  0.06  0.06  0.07  0.07]\n"," [ 0.03  0.03  0.04  0.    0.05  0.06  0.08  0.07]\n"," [ 0.03  0.03  0.03  0.04  0.05  0.    0.1   0.1 ]\n"," [ 0.03  0.03  0.03  0.    0.04  0.04  0.11  0.15]\n"," [ 0.02  0.    0.   -0.01  0.02  0.04  0.    0.32]\n"," [ 0.01  0.   -0.   -0.    0.    0.03  0.    0.42]\n"," [ 0.01 -0.01 -0.01  0.    0.02  0.06  0.06  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.05  0.04]\n"," [ 0.05  0.05  0.05  0.05]\n"," [ 0.05  0.05  0.06  0.05]\n"," [ 0.05  0.05  0.06  0.05]\n"," [ 0.06  0.06  0.07  0.06]\n"," [ 0.04  0.03  0.03  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.05  0.06  0.05]\n"," [ 0.05  0.05  0.05  0.06]\n"," [ 0.06  0.07  0.06  0.06]\n"," [ 0.06  0.07  0.06  0.06]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.04  0.03  0.03  0.03]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.04  0.04  0.05  0.04]\n"," [ 0.05  0.05  0.05  0.06]\n"," [ 0.07  0.06  0.08  0.06]\n"," [ 0.06  0.07  0.07  0.06]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.02  0.02  0.01  0.04]\n"," [ 0.05  0.    0.03  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.04  0.08  0.1   0.07]\n"," [ 0.09  0.1   0.09  0.07]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.02  0.03  0.03]\n"," [ 0.02 -0.    0.02  0.03]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.01  0.04  0.01]\n"," [ 0.    0.04  0.03  0.02]\n"," [ 0.04  0.05  0.11  0.06]\n"," [ 0.15  0.07  0.07  0.08]\n"," [ 0.02  0.01  0.02  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.   -0.    0.02  0.02]\n"," [ 0.04  0.    0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.13  0.32  0.17  0.12]\n"," [ 0.01 -0.01 -0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.03  0.    0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.21  0.21  0.42  0.15]\n"," [ 0.01 -0.   -0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.02 -0.  ]\n"," [ 0.    0.06  0.01  0.02]\n"," [-0.    0.02  0.04  0.06]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 15000, 0.18, 0.04\n","Optimal Action Value Estimates:\n","[[ 0.05  0.05  0.06  0.07  0.08  0.08  0.08  0.08]\n"," [ 0.04  0.05  0.05  0.06  0.07  0.07  0.09  0.08]\n"," [ 0.04  0.04  0.04  0.    0.07  0.07  0.09  0.09]\n"," [ 0.04  0.04  0.04  0.04  0.06  0.    0.11  0.12]\n"," [ 0.04  0.04  0.03  0.    0.06  0.07  0.11  0.17]\n"," [ 0.04  0.    0.   -0.01  0.03  0.05  0.    0.32]\n"," [ 0.04  0.   -0.   -0.    0.    0.04  0.    0.38]\n"," [ 0.03  0.01 -0.01  0.    0.03  0.08  0.07  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.04  0.05  0.05  0.05]\n"," [ 0.05  0.05  0.05  0.04]\n"," [ 0.05  0.05  0.05  0.06]\n"," [ 0.06  0.05  0.07  0.05]\n"," [ 0.07  0.07  0.08  0.06]\n"," [ 0.07  0.07  0.08  0.07]\n"," [ 0.07  0.07  0.08  0.08]\n"," [ 0.07  0.07  0.08  0.07]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.05  0.05  0.05]\n"," [ 0.05  0.04  0.05  0.05]\n"," [ 0.06  0.05  0.06  0.06]\n"," [ 0.06  0.06  0.07  0.06]\n"," [ 0.07  0.06  0.07  0.07]\n"," [ 0.07  0.07  0.09  0.07]\n"," [ 0.08  0.08  0.07  0.07]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.05  0.05  0.07  0.05]\n"," [ 0.06  0.06  0.07  0.07]\n"," [ 0.09  0.08  0.08  0.08]\n"," [ 0.08  0.09  0.08  0.08]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.01  0.04  0.02  0.04]\n"," [ 0.06  0.02  0.03  0.05]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.08  0.11  0.09  0.07]\n"," [ 0.12  0.09  0.09  0.09]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.03  0.04  0.03  0.04]\n"," [ 0.03 -0.    0.03  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.03  0.06  0.02]\n"," [ 0.02  0.07  0.05  0.04]\n"," [ 0.05  0.07  0.11  0.07]\n"," [ 0.11  0.17  0.1   0.12]\n"," [ 0.04  0.02  0.03  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.01 -0.    0.03  0.02]\n"," [ 0.05  0.01 -0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.18  0.32  0.17  0.16]\n"," [ 0.04 -0.    0.01  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.04  0.    0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.22  0.25  0.38  0.18]\n"," [ 0.03  0.02  0.02  0.02]\n"," [ 0.01 -0.01 -0.01 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.03 -0.  ]\n"," [ 0.    0.08  0.01  0.02]\n"," [-0.    0.02  0.04  0.07]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 16000, 0.12, 0.05\n","Optimal Action Value Estimates:\n","[[ 0.09  0.1   0.1   0.11  0.11  0.12  0.13  0.14]\n"," [ 0.09  0.09  0.1   0.11  0.11  0.12  0.13  0.14]\n"," [ 0.09  0.09  0.08  0.    0.11  0.12  0.13  0.16]\n"," [ 0.09  0.09  0.08  0.05  0.09  0.    0.16  0.18]\n"," [ 0.06  0.06  0.06  0.    0.08  0.09  0.17  0.21]\n"," [ 0.05  0.    0.   -0.01  0.03  0.05  0.    0.26]\n"," [ 0.04  0.   -0.   -0.    0.    0.04  0.    0.36]\n"," [ 0.04  0.01 -0.01  0.    0.04  0.09  0.08  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.08  0.09  0.08  0.08]\n"," [ 0.09  0.09  0.1   0.08]\n"," [ 0.09  0.1   0.1   0.09]\n"," [ 0.1   0.1   0.11  0.1 ]\n"," [ 0.11  0.11  0.11  0.11]\n"," [ 0.12  0.12  0.12  0.12]\n"," [ 0.13  0.13  0.13  0.13]\n"," [ 0.13  0.13  0.14  0.13]\n"," [ 0.08  0.08  0.09  0.08]\n"," [ 0.08  0.09  0.09  0.09]\n"," [ 0.09  0.09  0.1   0.09]\n"," [ 0.1   0.1   0.11  0.1 ]\n"," [ 0.11  0.11  0.11  0.11]\n"," [ 0.09  0.12  0.12  0.12]\n"," [ 0.12  0.13  0.13  0.12]\n"," [ 0.13  0.13  0.14  0.13]\n"," [ 0.08  0.08  0.08  0.09]\n"," [ 0.08  0.08  0.09  0.08]\n"," [ 0.08  0.08  0.08  0.08]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.07  0.09  0.11  0.1 ]\n"," [ 0.08  0.09  0.09  0.12]\n"," [ 0.13  0.12  0.13  0.13]\n"," [ 0.14  0.16  0.14  0.14]\n"," [ 0.07  0.05  0.04  0.09]\n"," [ 0.05  0.05  0.06  0.09]\n"," [ 0.05  0.04  0.04  0.08]\n"," [ 0.01  0.03  0.02  0.05]\n"," [ 0.09  0.02  0.04  0.05]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.12  0.16  0.12  0.13]\n"," [ 0.14  0.18  0.17  0.15]\n"," [ 0.04  0.04  0.04  0.06]\n"," [ 0.04  0.03  0.04  0.06]\n"," [ 0.06 -0.    0.03  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.03  0.08  0.03]\n"," [ 0.02  0.09  0.06  0.04]\n"," [ 0.06  0.09  0.17  0.12]\n"," [ 0.15  0.21  0.17  0.18]\n"," [ 0.05  0.03  0.03  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.01 -0.    0.03  0.02]\n"," [ 0.05  0.01 -0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.2   0.26  0.22  0.19]\n"," [ 0.04  0.01  0.01  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.04  0.    0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.24  0.27  0.36  0.22]\n"," [ 0.04  0.02  0.02  0.02]\n"," [ 0.01 -0.01 -0.01 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.04 -0.  ]\n"," [ 0.    0.09  0.01  0.02]\n"," [ 0.01  0.02  0.04  0.08]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 17000, 0.07, 0.04\n","Optimal Action Value Estimates:\n","[[ 0.06  0.06  0.06  0.06  0.07  0.07  0.07  0.08]\n"," [ 0.06  0.06  0.06  0.06  0.07  0.07  0.08  0.08]\n"," [ 0.05  0.05  0.05  0.    0.07  0.07  0.08  0.08]\n"," [ 0.05  0.05  0.05  0.03  0.07  0.    0.08  0.08]\n"," [ 0.04  0.04  0.04  0.    0.08  0.07  0.08  0.1 ]\n"," [ 0.04  0.    0.   -0.01  0.05  0.07  0.    0.31]\n"," [ 0.03  0.   -0.   -0.    0.    0.06  0.    0.37]\n"," [ 0.04  0.03 -0.01  0.    0.05  0.08  0.09  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.05  0.06  0.06  0.06]\n"," [ 0.06  0.06  0.06  0.06]\n"," [ 0.06  0.06  0.06  0.06]\n"," [ 0.06  0.06  0.06  0.06]\n"," [ 0.06  0.07  0.06  0.06]\n"," [ 0.07  0.07  0.07  0.07]\n"," [ 0.07  0.07  0.07  0.07]\n"," [ 0.08  0.08  0.08  0.08]\n"," [ 0.05  0.05  0.05  0.06]\n"," [ 0.06  0.05  0.05  0.06]\n"," [ 0.06  0.05  0.06  0.06]\n"," [ 0.05  0.06  0.06  0.06]\n"," [ 0.07  0.07  0.07  0.07]\n"," [ 0.07  0.07  0.06  0.07]\n"," [ 0.07  0.08  0.07  0.07]\n"," [ 0.08  0.08  0.08  0.08]\n"," [ 0.05  0.04  0.04  0.05]\n"," [ 0.05  0.05  0.04  0.05]\n"," [ 0.04  0.05  0.04  0.04]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.05  0.05  0.07  0.06]\n"," [ 0.06  0.07  0.07  0.06]\n"," [ 0.08  0.08  0.08  0.07]\n"," [ 0.08  0.08  0.08  0.08]\n"," [ 0.04  0.04  0.05  0.04]\n"," [ 0.05  0.04  0.04  0.04]\n"," [ 0.05  0.04  0.04  0.04]\n"," [ 0.01  0.03  0.01  0.03]\n"," [ 0.05  0.03  0.07  0.05]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.08  0.08  0.08  0.07]\n"," [ 0.08  0.08  0.08  0.08]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.01  0.02  0.03]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.04  0.05  0.08  0.03]\n"," [ 0.02  0.07  0.06  0.04]\n"," [ 0.08  0.08  0.08  0.08]\n"," [ 0.1   0.1   0.09  0.09]\n"," [ 0.04  0.03  0.03  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.01 -0.    0.05  0.02]\n"," [ 0.07  0.01  0.02  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.21  0.31  0.24  0.19]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.06  0.01  0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.24  0.28  0.37  0.26]\n"," [ 0.04  0.03  0.03  0.03]\n"," [ 0.03 -0.01 -0.01 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.05 -0.  ]\n"," [ 0.01  0.08  0.01  0.02]\n"," [ 0.01  0.02  0.04  0.09]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 18000, 0.01, 0.10\n","Optimal Action Value Estimates:\n","[[ 0.07  0.07  0.07  0.08  0.09  0.09  0.09  0.1 ]\n"," [ 0.07  0.07  0.07  0.07  0.09  0.09  0.1   0.1 ]\n"," [ 0.07  0.07  0.07  0.    0.08  0.09  0.1   0.12]\n"," [ 0.07  0.07  0.06  0.05  0.08  0.    0.1   0.13]\n"," [ 0.05  0.06  0.06  0.    0.08  0.08  0.12  0.15]\n"," [ 0.04  0.    0.   -0.01  0.06  0.08  0.    0.37]\n"," [ 0.03  0.   -0.   -0.    0.    0.09  0.    0.46]\n"," [ 0.03  0.02 -0.01  0.    0.06  0.12  0.14  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.05  0.05  0.06  0.07]\n"," [ 0.06  0.05  0.07  0.05]\n"," [ 0.07  0.06  0.06  0.07]\n"," [ 0.07  0.07  0.08  0.08]\n"," [ 0.07  0.09  0.08  0.07]\n"," [ 0.08  0.08  0.09  0.08]\n"," [ 0.08  0.08  0.09  0.08]\n"," [ 0.08  0.08  0.1   0.08]\n"," [ 0.07  0.05  0.05  0.07]\n"," [ 0.06  0.06  0.07  0.06]\n"," [ 0.05  0.05  0.07  0.07]\n"," [ 0.06  0.04  0.06  0.07]\n"," [ 0.07  0.07  0.09  0.06]\n"," [ 0.08  0.07  0.09  0.08]\n"," [ 0.08  0.08  0.1   0.08]\n"," [ 0.1   0.08  0.08  0.08]\n"," [ 0.06  0.05  0.07  0.05]\n"," [ 0.05  0.05  0.05  0.07]\n"," [ 0.05  0.05  0.04  0.07]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.07  0.06  0.08  0.06]\n"," [ 0.08  0.08  0.08  0.09]\n"," [ 0.09  0.1   0.09  0.09]\n"," [ 0.08  0.12  0.08  0.09]\n"," [ 0.04  0.04  0.04  0.07]\n"," [ 0.04  0.05  0.07  0.05]\n"," [ 0.05  0.04  0.06  0.05]\n"," [ 0.01  0.03  0.01  0.05]\n"," [ 0.05  0.05  0.08  0.05]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.1   0.09  0.09  0.09]\n"," [ 0.1   0.09  0.13  0.09]\n"," [ 0.05  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.06]\n"," [ 0.06  0.01  0.02  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.04  0.05  0.08  0.03]\n"," [ 0.02  0.08  0.06  0.04]\n"," [ 0.08  0.08  0.12  0.1 ]\n"," [ 0.12  0.15  0.12  0.12]\n"," [ 0.04  0.03  0.04  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.01 -0.    0.06  0.02]\n"," [ 0.08  0.01  0.02  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.21  0.37  0.26  0.2 ]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.09  0.01  0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.25  0.28  0.46  0.26]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.02 -0.01 -0.01 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.06 -0.  ]\n"," [ 0.01  0.12  0.02  0.02]\n"," [ 0.01  0.02  0.04  0.14]\n"," [ 0.    0.    0.    0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"IAqLLdIjZs6y","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                np.mean(stats_success[-1000:])))\n","optimal_action_estimates = np.max(action_value_array,axis=1)\n","print(\"Optimal Action Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(8,8),2))\n","print(\"estimate of the optimal state value at each state\")\n","print(\"\")\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_value_array.reshape((64,4)),2))\n","print(\"\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ML0Jw8C0hVEH","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}