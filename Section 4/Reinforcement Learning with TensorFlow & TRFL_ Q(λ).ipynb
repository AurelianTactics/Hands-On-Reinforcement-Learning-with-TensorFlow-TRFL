{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reinforcement Learning with TensorFlow & TRFL: Q(λ).ipynb","version":"0.3.2","provenance":[{"file_id":"15y3NrJjPTRhzYm0OWfoc813cR9BtCTGl","timestamp":1553044789533},{"file_id":"1BmiEkGoqe_CAaZmggviivF7Ygbd1iPe4","timestamp":1552945822274},{"file_id":"110YMHk2yHqxguCj1yaJaXVmkRUadX9GV","timestamp":1552791755615},{"file_id":"1cMFan2NCLOZ8w_xKSEyXTF09paJ0F930","timestamp":1552618439794},{"file_id":"1SToTDuBpTdV2UVRN9bapskLUesVhBH1J","timestamp":1551831097681},{"file_id":"1ssliB1HogX4KFHRyNKLU2aeiAV865kLv","timestamp":1551578593280},{"file_id":"1N74NgQBdDCDRER81p_VdJA_lsZOR2Hjo","timestamp":1550336742833}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"npzJ574a6A94","colab_type":"text"},"cell_type":"markdown","source":["**Reinforcement Learning with TensorFlow & TRFL: Q(λ)**\n","\n","Outline:\n","* Q(λ) \n","* TRFL usage with trfl.qlambda()\n","\n","\n"]},{"metadata":{"id":"RyxlWytnVqJI","colab_type":"code","outputId":"dc7ad252-0357-4a97-a135-43529d435149","colab":{"base_uri":"https://localhost:8080/","height":534}},"cell_type":"code","source":["#TRFL works with TensorFlow 1.12\n","#installs TensorFlow version 1.12 then restarts the runtime\n","!pip install tensorflow==1.12\n","\n","import os\n","os.kill(os.getpid(), 9)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.12\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (83.1MB)\n","\u001b[K    100% |████████████████████████████████| 83.1MB 262kB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.33.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.15.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.2.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (3.7.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.11.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.7)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.9)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow==1.12)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n","\u001b[K    100% |████████████████████████████████| 3.1MB 10.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.14.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (40.8.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.0.1)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (0.14.1)\n","Installing collected packages: tensorboard, tensorflow\n","  Found existing installation: tensorboard 1.13.1\n","    Uninstalling tensorboard-1.13.1:\n","      Successfully uninstalled tensorboard-1.13.1\n","  Found existing installation: tensorflow 1.13.1\n","    Uninstalling tensorflow-1.13.1:\n","      Successfully uninstalled tensorflow-1.13.1\n","Successfully installed tensorboard-1.12.2 tensorflow-1.12.0\n"],"name":"stdout"}]},{"metadata":{"id":"XRS56AQDVybG","colab_type":"code","outputId":"9211cc6b-c707-4d13-af17-d8ad97ae4aaf","executionInfo":{"status":"ok","timestamp":1553373774348,"user_tz":240,"elapsed":7672,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":363}},"cell_type":"code","source":["#install tensorflow-probability 0.5.0 that works with TensorFlow 1.12\n","!pip install tensorflow-probability==0.5.0\n","\n","#install TRFL\n","!pip install trfl\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-probability==0.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/ca/6f213618b5f7d0bf6139e6ec928d412a5ca14e4776adfd41a59c74a34021/tensorflow_probability-0.5.0-py2.py3-none-any.whl (680kB)\n","\u001b[K    100% |████████████████████████████████| 686kB 18.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.14.6)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.11.0)\n","Installing collected packages: tensorflow-probability\n","  Found existing installation: tensorflow-probability 0.6.0\n","    Uninstalling tensorflow-probability-0.6.0:\n","      Successfully uninstalled tensorflow-probability-0.6.0\n","Successfully installed tensorflow-probability-0.5.0\n","Collecting trfl\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/22/86ae5bb7c5c134ddc0d50332bf8d7dd16c48bf82fad6dcba6c401ab2ca01/trfl-1.0-cp36-cp36m-manylinux1_x86_64.whl (78kB)\n","\u001b[K    100% |████████████████████████████████| 81kB 5.2MB/s \n","\u001b[?25hRequirement already satisfied: dm-sonnet in /usr/local/lib/python3.6/dist-packages (from trfl) (1.23)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trfl) (0.7.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trfl) (1.11.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trfl) (1.14.6)\n","Installing collected packages: trfl\n","Successfully installed trfl-1.0\n"],"name":"stdout"}]},{"metadata":{"id":"SGop2a_BZCBl","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import trfl\n","import tensorflow_probability as tfp"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KMY4yn1q3sfu","colab_type":"text"},"cell_type":"markdown","source":["** Q(λ) **\n","\n","Q(λ) has many variants. Some considerations are what values of λ to use, how to handle the next state max, and how to handle on-policy and off-policy actions. In Watkins’s Q(λ), the eligibility traces are set to 0 on the first non-greedy action and remains 0 for the rest of the trajectory. Naive Q(λ) and TB(λ) don’t set eligibility traces to 0 on non-greedy actions. Peng’s Q(λ) is a hybrid of SARSA(λ) and Watkins’s Q(λ).\n","\n","In this notebooks we'll use naive Q(λ), and ignore whether the action is on-policy or off-policy. We'll solve deterministic (ie not slippery) FrozenLake 4x4, non-deterministic FrozenLake 4x4, and FrozenLake 8x8.\n","\n","\n","** Example 1: FrozenLake 4x4 Not Slippery **\n","\n","First example we set is_slippery to False in FrozenLake. Every action the agent takes becomes deterministic, making the env much easier."]},{"metadata":{"id":"ttTuMdfpj74j","colab_type":"code","colab":{}},"cell_type":"code","source":["from gym.envs.registration import register\n","register(\n","    id='FrozenLakeNotSlippery-v0',\n","    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name' : '4x4', 'is_slippery': False}\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uu84Bgs1kISF","colab_type":"code","colab":{}},"cell_type":"code","source":["#hyperparameters\n","episodes = 10000\n","learning_rate = 0.01\n","discount = 0.99\n","lambda_val = 0.5\n","epsilon_start = 1.0\n","epsilon_min = 0.01\n","epsilon_step = (epsilon_start - epsilon_min)/(episodes*.9)\n","\n","env = gym.make('FrozenLakeNotSlippery-v0')\n","num_actions = env.action_space.n\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","#et up input tensors\n","q_value_ = tf.placeholder(dtype=tf.float32, shape=[None, 1, num_actions], name=\"q_value\")\n","action_ = tf.placeholder(dtype=tf.int32, shape=[None, 1], name=\"action\")\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","q_next_ = tf.placeholder(dtype=tf.float32, shape=[None, 1, num_actions], name=\"q_next\")\n","lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\")\n","#set up TRFL qlambda tensor\n","q_lambda_return_ = trfl.qlambda(q_value_, action_, reward_, discount_, q_next_, lambda_)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"58jYfaNgTCG_","colab_type":"text"},"cell_type":"markdown","source":["** TRFL Usage **\n","Q(λ) is similar to the λ methods we have gone over in earlier videos. Like in Section 1, we replace state values with q values and add a tensor for actions. The loss return or td error return can be used to perform updates."]},{"metadata":{"id":"A41lzKpAsxp3","colab_type":"code","outputId":"b3a2897e-1ddd-4bf5-e714-3807fb34aaeb","executionInfo":{"status":"ok","timestamp":1553374933355,"user_tz":240,"elapsed":70021,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":4474}},"cell_type":"code","source":["stats_success = []\n","epsilon = epsilon_start\n","\n","with tf.Session() as sess:\n","  action_value_array = np.zeros((16,num_actions))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  q_list, action_list, reward_list, q_next_list, state_int_list = [], [], [], [], []\n","\n","  while current_episode < episodes:\n","    #take epsilon greedy action\n","    if np.random.rand() < epsilon:\n","      action = env.action_space.sample()\n","    else:\n","      #Choose a greedy action. If multiple greedy actions randomly choose between\n","      max_actions = np.argwhere(action_value_array[current_state] == np.max(action_value_array[current_state])).reshape((-1))\n","      action = np.random.choice(max_actions)\n","\n","    next_state, rew, done, info = env.step(action)\n","      \n","    q_list.append(action_value_array[current_state])\n","    reward_list.append(rew)\n","    action_list.append(action)\n","    q_next_list.append(action_value_array[next_state])\n","    state_int_list.append(current_state)\n","    \n","    current_state = next_state\n","    #run TRFL qlambda tensor to get TD error\n","    q_lambda_output = sess.run(q_lambda_return_, feed_dict={\n","        q_value_:np.array(q_list).reshape(-1,1,num_actions),\n","        action_:np.array(action_list).reshape(-1,1),\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(q_list)).reshape(-1,1),\n","        q_next_:np.array(q_next_list).reshape(-1,1,num_actions),\n","        lambda_:np.array([lambda_val]*len(q_list)).reshape(-1,1),\n","      })\n","    #use TD error output update action values\n","    action_value_array[state_int_list, action_list] += np.squeeze(learning_rate*q_lambda_output.extra.td_error)\n","\n","    if done:\n","      if next_state == 15:\n","        stats_success.append(1)\n","      else:\n","        stats_success.append(0)\n","        \n","      q_list, action_list, reward_list, q_next_list, state_int_list = [], [], [], [], []\n","      current_state = env.reset()\n","      current_episode += 1\n","      \n","      #decrease epsilon\n","      epsilon -= epsilon_step\n","      if epsilon < epsilon_min:\n","        epsilon = epsilon_min\n","        \n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","        optimal_action_estimates = np.max(action_value_array,axis=1)\n","        print(\"Optimal Action Value Estimates:\")\n","        print(np.round(optimal_action_estimates.reshape(4,4),2))\n","        print(\"estimate of the optimal state value at each state\")\n","        print(\"\")\n","        print(\"All Action Value Estimates:\")\n","        print(np.round(action_value_array.reshape((16,4)),2))\n","        print(\"\")\n","        "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 1000, 0.89, 0.02\n","Optimal Action Value Estimates:\n","[[0.06 0.05 0.04 0.02]\n"," [0.06 0.   0.05 0.  ]\n"," [0.07 0.08 0.1  0.  ]\n"," [0.   0.11 0.21 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.05 0.06 0.04 0.05]\n"," [0.05 0.   0.03 0.03]\n"," [0.03 0.04 0.01 0.02]\n"," [0.02 0.   0.01 0.01]\n"," [0.05 0.06 0.   0.05]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.05 0.   0.02]\n"," [0.   0.   0.   0.  ]\n"," [0.05 0.   0.07 0.05]\n"," [0.04 0.08 0.05 0.  ]\n"," [0.03 0.1  0.   0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.03 0.11 0.03]\n"," [0.04 0.1  0.21 0.04]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 2000, 0.78, 0.06\n","Optimal Action Value Estimates:\n","[[0.32 0.3  0.24 0.14]\n"," [0.35 0.   0.23 0.  ]\n"," [0.36 0.39 0.42 0.  ]\n"," [0.   0.44 0.56 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.31 0.32 0.27 0.31]\n"," [0.3  0.   0.21 0.24]\n"," [0.24 0.18 0.11 0.14]\n"," [0.14 0.   0.07 0.08]\n"," [0.31 0.35 0.   0.3 ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.23 0.   0.08]\n"," [0.   0.   0.   0.  ]\n"," [0.29 0.   0.36 0.3 ]\n"," [0.29 0.39 0.32 0.  ]\n"," [0.2  0.42 0.   0.11]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.31 0.44 0.28]\n"," [0.24 0.32 0.56 0.26]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 3000, 0.67, 0.15\n","Optimal Action Value Estimates:\n","[[0.69 0.67 0.6  0.37]\n"," [0.72 0.   0.51 0.  ]\n"," [0.76 0.78 0.78 0.  ]\n"," [0.   0.83 0.9  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.67 0.69 0.63 0.67]\n"," [0.67 0.   0.56 0.59]\n"," [0.6  0.38 0.28 0.46]\n"," [0.37 0.   0.19 0.21]\n"," [0.69 0.72 0.   0.66]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.51 0.   0.3 ]\n"," [0.   0.   0.   0.  ]\n"," [0.68 0.   0.76 0.65]\n"," [0.67 0.78 0.67 0.  ]\n"," [0.53 0.78 0.   0.34]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.7  0.83 0.64]\n"," [0.63 0.64 0.9  0.59]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 4000, 0.56, 0.27\n","Optimal Action Value Estimates:\n","[[0.85 0.84 0.79 0.55]\n"," [0.88 0.   0.74 0.  ]\n"," [0.9  0.93 0.96 0.  ]\n"," [0.   0.97 0.99 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.84 0.85 0.82 0.84]\n"," [0.84 0.   0.73 0.79]\n"," [0.79 0.5  0.43 0.58]\n"," [0.55 0.   0.27 0.28]\n"," [0.84 0.88 0.   0.84]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.74 0.   0.48]\n"," [0.   0.   0.   0.  ]\n"," [0.87 0.   0.9  0.83]\n"," [0.85 0.93 0.9  0.  ]\n"," [0.8  0.96 0.   0.62]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.91 0.97 0.87]\n"," [0.89 0.91 0.99 0.89]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 5000, 0.45, 0.37\n","Optimal Action Value Estimates:\n","[[0.9  0.88 0.83 0.65]\n"," [0.91 0.   0.89 0.  ]\n"," [0.92 0.94 0.98 0.  ]\n"," [0.   0.98 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.88 0.9  0.86 0.87]\n"," [0.88 0.   0.81 0.84]\n"," [0.83 0.57 0.54 0.69]\n"," [0.65 0.   0.27 0.33]\n"," [0.89 0.91 0.   0.88]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.89 0.   0.58]\n"," [0.   0.   0.   0.  ]\n"," [0.91 0.   0.92 0.88]\n"," [0.9  0.94 0.94 0.  ]\n"," [0.92 0.98 0.   0.82]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.95 0.98 0.92]\n"," [0.95 0.97 1.   0.93]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 6000, 0.34, 0.52\n","Optimal Action Value Estimates:\n","[[0.9  0.88 0.85 0.66]\n"," [0.91 0.   0.93 0.  ]\n"," [0.94 0.96 0.98 0.  ]\n"," [0.   0.98 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.89 0.9  0.87 0.89]\n"," [0.88 0.   0.83 0.86]\n"," [0.85 0.58 0.55 0.76]\n"," [0.66 0.   0.27 0.33]\n"," [0.89 0.91 0.   0.88]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.93 0.   0.62]\n"," [0.   0.   0.   0.  ]\n"," [0.91 0.   0.94 0.9 ]\n"," [0.91 0.96 0.94 0.  ]\n"," [0.93 0.98 0.   0.88]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.97 0.98 0.93]\n"," [0.95 0.98 1.   0.96]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 7000, 0.23, 0.69\n","Optimal Action Value Estimates:\n","[[0.91 0.9  0.87 0.69]\n"," [0.92 0.   0.93 0.  ]\n"," [0.94 0.96 0.99 0.  ]\n"," [0.   0.98 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.9  0.91 0.88 0.9 ]\n"," [0.9  0.   0.84 0.87]\n"," [0.87 0.58 0.58 0.76]\n"," [0.69 0.   0.27 0.33]\n"," [0.91 0.92 0.   0.91]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.93 0.   0.62]\n"," [0.   0.   0.   0.  ]\n"," [0.92 0.   0.94 0.92]\n"," [0.93 0.95 0.96 0.  ]\n"," [0.93 0.99 0.   0.89]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.97 0.98 0.95]\n"," [0.97 0.99 1.   0.96]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 8000, 0.12, 0.81\n","Optimal Action Value Estimates:\n","[[0.94 0.92 0.87 0.69]\n"," [0.95 0.   0.95 0.  ]\n"," [0.96 0.97 0.98 0.  ]\n"," [0.   0.99 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.92 0.94 0.91 0.92]\n"," [0.92 0.   0.84 0.88]\n"," [0.87 0.58 0.58 0.76]\n"," [0.69 0.   0.27 0.33]\n"," [0.93 0.95 0.   0.92]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.95 0.   0.64]\n"," [0.   0.   0.   0.  ]\n"," [0.94 0.   0.96 0.93]\n"," [0.94 0.97 0.97 0.  ]\n"," [0.95 0.98 0.   0.92]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.97 0.99 0.96]\n"," [0.97 0.99 1.   0.97]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 9000, 0.01, 0.93\n","Optimal Action Value Estimates:\n","[[0.95 0.93 0.87 0.69]\n"," [0.96 0.   0.96 0.  ]\n"," [0.97 0.98 0.99 0.  ]\n"," [0.   0.99 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.93 0.95 0.92 0.93]\n"," [0.93 0.   0.84 0.88]\n"," [0.87 0.58 0.58 0.76]\n"," [0.69 0.   0.27 0.33]\n"," [0.94 0.96 0.   0.93]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.96 0.   0.64]\n"," [0.   0.   0.   0.  ]\n"," [0.95 0.   0.97 0.94]\n"," [0.95 0.97 0.98 0.  ]\n"," [0.96 0.99 0.   0.93]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.97 0.99 0.96]\n"," [0.97 0.99 1.   0.97]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.99\n","Optimal Action Value Estimates:\n","[[0.95 0.93 0.87 0.69]\n"," [0.96 0.   0.96 0.  ]\n"," [0.97 0.98 0.99 0.  ]\n"," [0.   0.99 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.93 0.95 0.92 0.93]\n"," [0.93 0.   0.84 0.88]\n"," [0.87 0.58 0.58 0.76]\n"," [0.69 0.   0.27 0.33]\n"," [0.94 0.96 0.   0.93]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.96 0.   0.64]\n"," [0.   0.   0.   0.  ]\n"," [0.95 0.   0.97 0.94]\n"," [0.95 0.97 0.98 0.  ]\n"," [0.96 0.99 0.   0.93]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.97 0.99 0.96]\n"," [0.97 0.99 1.   0.97]\n"," [0.   0.   0.   0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"OKktGDk_UIvd","colab_type":"code","outputId":"cc976b3e-9bdf-4c05-9436-4cdcb1f4ac0d","executionInfo":{"status":"ok","timestamp":1553374981312,"user_tz":240,"elapsed":432,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"cell_type":"code","source":["print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","optimal_action_estimates = np.max(action_value_array,axis=1)\n","print(\"Optimal Action Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(4,4),2))\n","print(\"estimate of the optimal state value at each state\")\n","print(\"\")\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_value_array.reshape((16,4)),2))\n","print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.99\n","Optimal Action Value Estimates:\n","[[0.95 0.93 0.87 0.69]\n"," [0.96 0.   0.96 0.  ]\n"," [0.97 0.98 0.99 0.  ]\n"," [0.   0.99 1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.93 0.95 0.92 0.93]\n"," [0.93 0.   0.84 0.88]\n"," [0.87 0.58 0.58 0.76]\n"," [0.69 0.   0.27 0.33]\n"," [0.94 0.96 0.   0.93]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.96 0.   0.64]\n"," [0.   0.   0.   0.  ]\n"," [0.95 0.   0.97 0.94]\n"," [0.95 0.97 0.98 0.  ]\n"," [0.96 0.99 0.   0.93]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.97 0.99 0.96]\n"," [0.97 0.99 1.   0.97]\n"," [0.   0.   0.   0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"5z58TlVUXyoP","colab_type":"text"},"cell_type":"markdown","source":["** Example 2: FrozenLake 4x4 Slippery **\n","\n","Standard FrozenLake env where slippery is enabled. Notice the increased failure rate and lower Q values"]},{"metadata":{"id":"FbqZr9dAUOhm","colab_type":"code","colab":{}},"cell_type":"code","source":["#hyperparameters\n","episodes = 10000\n","learning_rate = 0.01\n","discount = 1.\n","lambda_val = 0.5\n","epsilon_start = 1.0\n","epsilon_min = 0.01\n","epsilon_step = (epsilon_start - epsilon_min)/(episodes*.9)\n","\n","seed = 31\n","env = gym.make('FrozenLake-v0')\n","env.seed(seed)\n","np.random.seed(seed)\n","num_actions = env.action_space.n\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","q_value_ = tf.placeholder(dtype=tf.float32, shape=[None, 1, num_actions], name=\"q_value\")\n","action_ = tf.placeholder(dtype=tf.int32, shape=[None, 1], name=\"action\")\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","q_next_ = tf.placeholder(dtype=tf.float32, shape=[None, 1, num_actions], name=\"q_next\")\n","lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\")\n","\n","q_lambda_return_ = trfl.qlambda(q_value_, action_, reward_, discount_, q_next_, lambda_)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"017M07L0-eNx","colab_type":"code","outputId":"30affd17-6f26-44f7-fdbe-ccdbca948999","executionInfo":{"status":"ok","timestamp":1553377109666,"user_tz":240,"elapsed":247746,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":4474}},"cell_type":"code","source":["stats_success = []\n","epsilon = epsilon_start\n","\n","with tf.Session() as sess:\n","  #initialize the estimated state values to zero\n","  action_value_array = np.zeros((16,num_actions))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  q_list, action_list, reward_list, q_next_list, state_int_list = [], [], [], [], []\n","\n","  while current_episode < episodes:\n","    #take epsilon greedy action\n","    if np.random.rand() < epsilon:\n","      action = env.action_space.sample()\n","    else:\n","      #Choose a greedy action. If multiple greedy actions randomly choose between\n","      max_actions = np.argwhere(action_value_array[current_state] == np.max(action_value_array[current_state])).reshape((-1))\n","      action = np.random.choice(max_actions)\n","\n","    next_state, rew, done, info = env.step(action)\n","      \n","    q_list.append(action_value_array[current_state])\n","    reward_list.append(rew)\n","    action_list.append(action)\n","    q_next_list.append(action_value_array[next_state])\n","    state_int_list.append(current_state)\n","    \n","    current_state = next_state\n","    \n","    q_lambda_output = sess.run(q_lambda_return_, feed_dict={\n","        q_value_:np.array(q_list).reshape(-1,1,num_actions),\n","        action_:np.array(action_list).reshape(-1,1),\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(q_list)).reshape(-1,1),\n","        q_next_:np.array(q_next_list).reshape(-1,1,num_actions),\n","        lambda_:np.array([lambda_val]*len(q_list)).reshape(-1,1),\n","      })\n","\n","    #action_value_array[state_int_list, action_list] += np.squeeze(learning_rate*q_lambda_output.extra.td_error)\n","    for s, a, td in zip(state_int_list,action_list,q_lambda_output.extra.td_error.tolist()):\n","      action_value_array[s,a] += learning_rate*td[0]\n","\n","    if done:\n","      if next_state == 15:\n","        stats_success.append(1)\n","      else:\n","        stats_success.append(0)\n","        \n","      q_list, action_list, reward_list, q_next_list, state_int_list = [], [], [], [], []\n","      current_state = env.reset()\n","      current_episode += 1\n","      epsilon -= epsilon_step\n","      if epsilon < epsilon_min:\n","        epsilon = epsilon_min\n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","        optimal_action_estimates = np.max(action_value_array,axis=1)\n","        print(\"Optimal Action Value Estimates:\")\n","        print(np.round(optimal_action_estimates.reshape(4,4),2))\n","        print(\"estimate of the optimal state value at each state\")\n","        print(\"\")\n","        print(\"All Action Value Estimates:\")\n","        print(np.round(action_value_array.reshape((16,4)),2))\n","        print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 1000, 0.89, 0.02\n","Optimal Action Value Estimates:\n","[[0.02 0.01 0.01 0.01]\n"," [0.02 0.   0.02 0.  ]\n"," [0.03 0.03 0.04 0.  ]\n"," [0.   0.05 0.09 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.02 0.02 0.01 0.01]\n"," [0.01 0.01 0.01 0.01]\n"," [0.01 0.01 0.01 0.01]\n"," [0.   0.01 0.   0.01]\n"," [0.02 0.02 0.02 0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.02 0.01 0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.03 0.02 0.02]\n"," [0.02 0.03 0.03 0.01]\n"," [0.03 0.02 0.04 0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.02 0.04 0.05 0.03]\n"," [0.03 0.09 0.07 0.03]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 2000, 0.78, 0.02\n","Optimal Action Value Estimates:\n","[[0.04 0.04 0.03 0.03]\n"," [0.04 0.   0.03 0.  ]\n"," [0.05 0.07 0.06 0.  ]\n"," [0.   0.08 0.15 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.04 0.04 0.04 0.04]\n"," [0.03 0.03 0.03 0.04]\n"," [0.03 0.03 0.03 0.03]\n"," [0.02 0.02 0.02 0.03]\n"," [0.04 0.04 0.04 0.04]\n"," [0.   0.   0.   0.  ]\n"," [0.03 0.03 0.03 0.02]\n"," [0.   0.   0.   0.  ]\n"," [0.04 0.04 0.04 0.05]\n"," [0.04 0.04 0.07 0.03]\n"," [0.06 0.06 0.06 0.03]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.03 0.06 0.08 0.04]\n"," [0.07 0.13 0.15 0.07]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 3000, 0.67, 0.02\n","Optimal Action Value Estimates:\n","[[0.07 0.06 0.06 0.05]\n"," [0.07 0.   0.06 0.  ]\n"," [0.08 0.1  0.12 0.  ]\n"," [0.   0.12 0.2  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.07 0.07 0.07 0.07]\n"," [0.06 0.06 0.06 0.06]\n"," [0.06 0.06 0.06 0.06]\n"," [0.04 0.04 0.04 0.05]\n"," [0.07 0.07 0.07 0.06]\n"," [0.   0.   0.   0.  ]\n"," [0.06 0.04 0.06 0.04]\n"," [0.   0.   0.   0.  ]\n"," [0.07 0.08 0.07 0.08]\n"," [0.07 0.08 0.1  0.08]\n"," [0.08 0.12 0.07 0.04]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.06 0.09 0.12 0.07]\n"," [0.09 0.17 0.2  0.11]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 4000, 0.56, 0.03\n","Optimal Action Value Estimates:\n","[[0.13 0.13 0.12 0.11]\n"," [0.13 0.   0.12 0.  ]\n"," [0.14 0.16 0.17 0.  ]\n"," [0.   0.17 0.24 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.13 0.12 0.13 0.13]\n"," [0.12 0.11 0.12 0.13]\n"," [0.12 0.12 0.12 0.12]\n"," [0.09 0.07 0.08 0.11]\n"," [0.13 0.13 0.13 0.12]\n"," [0.   0.   0.   0.  ]\n"," [0.12 0.05 0.11 0.07]\n"," [0.   0.   0.   0.  ]\n"," [0.13 0.12 0.13 0.14]\n"," [0.13 0.15 0.16 0.12]\n"," [0.15 0.17 0.12 0.09]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.13 0.17 0.16 0.14]\n"," [0.14 0.2  0.24 0.17]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 5000, 0.45, 0.06\n","Optimal Action Value Estimates:\n","[[0.2  0.2  0.19 0.17]\n"," [0.21 0.   0.18 0.  ]\n"," [0.21 0.23 0.23 0.  ]\n"," [0.   0.28 0.35 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.2  0.2  0.2  0.2 ]\n"," [0.18 0.17 0.16 0.2 ]\n"," [0.19 0.18 0.17 0.17]\n"," [0.15 0.12 0.12 0.17]\n"," [0.21 0.2  0.2  0.2 ]\n"," [0.   0.   0.   0.  ]\n"," [0.18 0.09 0.15 0.09]\n"," [0.   0.   0.   0.  ]\n"," [0.2  0.19 0.2  0.21]\n"," [0.21 0.2  0.23 0.19]\n"," [0.21 0.23 0.18 0.13]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.2  0.24 0.28 0.22]\n"," [0.26 0.35 0.31 0.22]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 6000, 0.34, 0.09\n","Optimal Action Value Estimates:\n","[[0.34 0.33 0.33 0.28]\n"," [0.34 0.   0.32 0.  ]\n"," [0.35 0.36 0.37 0.  ]\n"," [0.   0.4  0.46 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.34 0.33 0.33 0.34]\n"," [0.31 0.31 0.3  0.33]\n"," [0.33 0.29 0.3  0.29]\n"," [0.22 0.19 0.18 0.28]\n"," [0.34 0.32 0.33 0.32]\n"," [0.   0.   0.   0.  ]\n"," [0.32 0.14 0.28 0.19]\n"," [0.   0.   0.   0.  ]\n"," [0.32 0.34 0.33 0.35]\n"," [0.31 0.36 0.35 0.29]\n"," [0.31 0.37 0.33 0.21]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.35 0.34 0.4  0.34]\n"," [0.35 0.46 0.38 0.33]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 7000, 0.23, 0.13\n","Optimal Action Value Estimates:\n","[[0.4  0.39 0.39 0.38]\n"," [0.41 0.   0.37 0.  ]\n"," [0.41 0.44 0.42 0.  ]\n"," [0.   0.46 0.51 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.4  0.4  0.39 0.4 ]\n"," [0.38 0.39 0.37 0.39]\n"," [0.37 0.37 0.38 0.39]\n"," [0.36 0.38 0.36 0.38]\n"," [0.41 0.39 0.39 0.38]\n"," [0.   0.   0.   0.  ]\n"," [0.37 0.22 0.31 0.17]\n"," [0.   0.   0.   0.  ]\n"," [0.4  0.4  0.4  0.41]\n"," [0.4  0.44 0.39 0.38]\n"," [0.41 0.42 0.33 0.37]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.39 0.43 0.46 0.41]\n"," [0.43 0.51 0.46 0.45]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 8000, 0.12, 0.23\n","Optimal Action Value Estimates:\n","[[0.54 0.53 0.53 0.53]\n"," [0.54 0.   0.51 0.  ]\n"," [0.54 0.56 0.54 0.  ]\n"," [0.   0.58 0.62 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.54 0.53 0.53 0.54]\n"," [0.52 0.52 0.52 0.53]\n"," [0.53 0.52 0.52 0.52]\n"," [0.51 0.52 0.52 0.53]\n"," [0.54 0.5  0.51 0.51]\n"," [0.   0.   0.   0.  ]\n"," [0.51 0.24 0.37 0.17]\n"," [0.   0.   0.   0.  ]\n"," [0.53 0.51 0.54 0.54]\n"," [0.54 0.56 0.52 0.51]\n"," [0.47 0.54 0.38 0.49]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.52 0.47 0.58 0.51]\n"," [0.53 0.62 0.56 0.54]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 9000, 0.01, 0.41\n","Optimal Action Value Estimates:\n","[[0.69 0.68 0.68 0.68]\n"," [0.69 0.   0.68 0.  ]\n"," [0.69 0.69 0.69 0.  ]\n"," [0.   0.69 0.72 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.69 0.68 0.68 0.68]\n"," [0.64 0.65 0.63 0.68]\n"," [0.64 0.64 0.64 0.68]\n"," [0.64 0.62 0.64 0.68]\n"," [0.69 0.66 0.64 0.66]\n"," [0.   0.   0.   0.  ]\n"," [0.68 0.4  0.36 0.27]\n"," [0.   0.   0.   0.  ]\n"," [0.67 0.6  0.61 0.69]\n"," [0.6  0.69 0.58 0.62]\n"," [0.69 0.57 0.41 0.53]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.5  0.6  0.69 0.64]\n"," [0.64 0.72 0.63 0.62]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.60\n","Optimal Action Value Estimates:\n","[[0.8  0.8  0.8  0.8 ]\n"," [0.8  0.   0.8  0.  ]\n"," [0.8  0.8  0.8  0.  ]\n"," [0.   0.81 0.84 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.8  0.79 0.79 0.79]\n"," [0.76 0.77 0.73 0.8 ]\n"," [0.78 0.77 0.77 0.8 ]\n"," [0.77 0.78 0.76 0.8 ]\n"," [0.8  0.74 0.77 0.78]\n"," [0.   0.   0.   0.  ]\n"," [0.8  0.38 0.61 0.27]\n"," [0.   0.   0.   0.  ]\n"," [0.79 0.76 0.75 0.8 ]\n"," [0.65 0.8  0.67 0.77]\n"," [0.8  0.57 0.51 0.53]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.51 0.69 0.81 0.68]\n"," [0.74 0.84 0.69 0.67]\n"," [0.   0.   0.   0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"gHSP5ubAVAqt","colab_type":"code","outputId":"3752dd08-4444-4670-885e-5fd0aea112d0","executionInfo":{"status":"ok","timestamp":1553377109669,"user_tz":240,"elapsed":142733,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"cell_type":"code","source":["print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","optimal_action_estimates = np.max(action_value_array,axis=1)\n","print(\"Optimal Action Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(4,4),2))\n","print(\"estimate of the optimal state value at each state\")\n","print(\"\")\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_value_array.reshape((16,4)),2))\n","print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.60\n","Optimal Action Value Estimates:\n","[[0.8  0.8  0.8  0.8 ]\n"," [0.8  0.   0.8  0.  ]\n"," [0.8  0.8  0.8  0.  ]\n"," [0.   0.81 0.84 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.8  0.79 0.79 0.79]\n"," [0.76 0.77 0.73 0.8 ]\n"," [0.78 0.77 0.77 0.8 ]\n"," [0.77 0.78 0.76 0.8 ]\n"," [0.8  0.74 0.77 0.78]\n"," [0.   0.   0.   0.  ]\n"," [0.8  0.38 0.61 0.27]\n"," [0.   0.   0.   0.  ]\n"," [0.79 0.76 0.75 0.8 ]\n"," [0.65 0.8  0.67 0.77]\n"," [0.8  0.57 0.51 0.53]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.51 0.69 0.81 0.68]\n"," [0.74 0.84 0.69 0.67]\n"," [0.   0.   0.   0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"Z2_A-Q6ea5W9","colab_type":"text"},"cell_type":"markdown","source":["** Example 3: FrozenLake 8x8 **\n","\n","FrozenLake on an 8x8 grid. Much harder to randomly find the goal. To make learning faster, we add a penalty for falling into a hole."]},{"metadata":{"id":"eQG7BVhNa4LY","colab_type":"code","colab":{}},"cell_type":"code","source":["#hyperparameters\n","episodes = 20000\n","learning_rate = 0.01\n","discount = 0.99\n","lambda_val = 0.75\n","epsilon_start = 1.0\n","epsilon_min = 0.01\n","epsilon_step = (epsilon_start - epsilon_min)/(episodes*.9)\n","hole_penalty = -0.1 #penalty for falling into a hole\n","\n","seed = 31\n","env = gym.make('FrozenLake8x8-v0')\n","env.seed(seed)\n","np.random.seed(seed)\n","num_actions = env.action_space.n\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","q_value_ = tf.placeholder(dtype=tf.float32, shape=[None, 1, num_actions], name=\"q_value\")\n","action_ = tf.placeholder(dtype=tf.int32, shape=[None, 1], name=\"action\")\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","q_next_ = tf.placeholder(dtype=tf.float32, shape=[None, 1, num_actions], name=\"q_next\")\n","lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\")\n","\n","q_lambda_return_ = trfl.qlambda(q_value_, action_, reward_, discount_, q_next_, lambda_)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Krd5YoIzeUUz","colab_type":"code","outputId":"1417e1cf-56f8-44e0-cfe3-2b0a6046381c","executionInfo":{"status":"ok","timestamp":1553378845407,"user_tz":240,"elapsed":1247488,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":26760}},"cell_type":"code","source":["stats_success = []\n","epsilon = epsilon_start\n","\n","with tf.Session() as sess:\n","  #initialize the estimated state values to zero\n","  action_value_array = np.zeros((64,num_actions))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  q_list, action_list, reward_list, q_next_list, state_int_list = [], [], [], [], []\n","\n","  while current_episode < episodes:\n","    #take epsilon greedy action\n","    if np.random.rand() < epsilon:\n","      action = env.action_space.sample()\n","    else:\n","      #Choose a greedy action. If multiple greedy actions randomly choose between\n","      max_actions = np.argwhere(action_value_array[current_state] == np.max(action_value_array[current_state])).reshape((-1))\n","      action = np.random.choice(max_actions)\n","\n","    next_state, rew, done, info = env.step(action)\n","      \n","    if done and rew < 1:\n","      rew = hole_penalty\n","      \n","    q_list.append(action_value_array[current_state])\n","    reward_list.append(rew)\n","    action_list.append(action)\n","    q_next_list.append(action_value_array[next_state])\n","    state_int_list.append(current_state)\n","    \n","    current_state = next_state\n","    \n","    q_lambda_output = sess.run(q_lambda_return_, feed_dict={\n","        q_value_:np.array(q_list).reshape(-1,1,num_actions),\n","        action_:np.array(action_list).reshape(-1,1),\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(q_list)).reshape(-1,1),\n","        q_next_:np.array(q_next_list).reshape(-1,1,num_actions),\n","        lambda_:np.array([lambda_val]*len(q_list)).reshape(-1,1),\n","      })\n","\n","    #action_value_array[state_int_list, action_list] += np.squeeze(learning_rate*q_lambda_output.extra.td_error)\n","    for s, a, td in zip(state_int_list,action_list,q_lambda_output.extra.td_error.tolist()):\n","      action_value_array[s,a] += learning_rate*td[0]\n","\n","    if done:\n","      if next_state == 63:\n","        stats_success.append(1)\n","      else:\n","        stats_success.append(0)\n","        \n","      q_list, action_list, reward_list, q_next_list, state_int_list = [], [], [], [], []\n","      current_state = env.reset()\n","      current_episode += 1\n","      epsilon -= epsilon_step\n","      if epsilon < epsilon_min:\n","        epsilon = epsilon_min\n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","        optimal_action_estimates = np.max(action_value_array,axis=1)\n","        print(\"Optimal Action Value Estimates:\")\n","        print(np.round(optimal_action_estimates.reshape(8,8),2))\n","        print(\"estimate of the optimal state value at each state\")\n","        print(\"\")\n","        print(\"All Action Value Estimates:\")\n","        print(np.round(action_value_array.reshape((64,4)),2))\n","        print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 1000, 0.95, 0.00\n","Optimal Action Value Estimates:\n","[[-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.    0.   -0.01 -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.01 -0.01 -0.01  0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01  0.   -0.   -0.   -0.   -0.  ]\n"," [-0.01  0.    0.    0.   -0.   -0.    0.    0.  ]\n"," [-0.    0.    0.    0.    0.    0.    0.    0.01]\n"," [-0.   -0.   -0.    0.    0.    0.    0.    0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.   -0.01 -0.01 -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.02 -0.02 -0.02 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.03 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.   -0.    0.   -0.  ]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.   -0.    0.  ]\n"," [ 0.    0.   -0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01 -0.    0.01 -0.  ]\n"," [-0.   -0.   -0.01 -0.01]\n"," [-0.   -0.   -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 2000, 0.89, 0.00\n","Optimal Action Value Estimates:\n","[[-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.01  0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.01 -0.01 -0.01  0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.   -0.  ]\n"," [-0.01  0.    0.   -0.   -0.   -0.    0.    0.  ]\n"," [-0.01  0.   -0.    0.    0.   -0.    0.    0.02]\n"," [-0.01 -0.01 -0.    0.    0.    0.    0.    0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.02 -0.01 -0.  ]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.02 -0.02 -0.02 -0.01]\n"," [-0.01 -0.02 -0.01 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.03 -0.03 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.    0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.02  0.01 -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 3000, 0.84, 0.00\n","Optimal Action Value Estimates:\n","[[-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.01  0.   -0.01 -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.01 -0.01 -0.01  0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01  0.   -0.01 -0.   -0.   -0.  ]\n"," [-0.01  0.    0.   -0.   -0.   -0.    0.    0.01]\n"," [-0.01  0.   -0.    0.    0.   -0.    0.    0.02]\n"," [-0.01 -0.01 -0.01  0.    0.    0.    0.    0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.01 -0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.01 -0.   -0.01 -0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.03 -0.02]\n"," [-0.01 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.   -0.01 -0.01]\n"," [-0.01 -0.   -0.01 -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01 -0.   -0.   -0.  ]\n"," [-0.01 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.    0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.02  0.01 -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 4000, 0.78, 0.00\n","Optimal Action Value Estimates:\n","[[-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.    0.   -0.01 -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.01 -0.01 -0.01  0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.   -0.  ]\n"," [-0.01  0.    0.   -0.   -0.   -0.    0.    0.  ]\n"," [-0.02  0.   -0.    0.    0.    0.    0.    0.02]\n"," [-0.01 -0.01 -0.01  0.    0.    0.    0.02  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.01 -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.02 -0.03 -0.01]\n"," [-0.01 -0.03 -0.01 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.02 -0.01]\n"," [-0.01 -0.03 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.02 -0.01 -0.01 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.   -0.  ]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.    0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.02  0.01 -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.02 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 5000, 0.73, 0.00\n","Optimal Action Value Estimates:\n","[[-0.   -0.   -0.   -0.   -0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.   -0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.    0.   -0.    0.   -0.    0.  ]\n"," [-0.   -0.   -0.01 -0.01 -0.    0.   -0.    0.  ]\n"," [-0.01 -0.01 -0.01  0.   -0.01 -0.01 -0.    0.  ]\n"," [-0.01  0.    0.   -0.   -0.   -0.01  0.    0.02]\n"," [-0.01  0.   -0.   -0.    0.   -0.    0.    0.04]\n"," [-0.01 -0.01 -0.01  0.    0.    0.    0.01  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.   -0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.   -0.    0.    0.  ]\n"," [ 0.   -0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.   -0.   -0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.02 -0.03 -0.01]\n"," [-0.01 -0.01 -0.02 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.    0.    0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.02 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.02 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.02 -0.01 -0.   -0.  ]\n"," [ 0.   -0.    0.    0.  ]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.    0.   -0.01]\n"," [-0.01 -0.02 -0.01 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.04  0.01  0.01]\n"," [-0.01 -0.02 -0.01 -0.01]\n"," [-0.01 -0.01 -0.02 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 6000, 0.67, 0.00\n","Optimal Action Value Estimates:\n","[[-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.01  0.   -0.   -0.   -0.   -0.  ]\n"," [-0.   -0.01 -0.01 -0.01 -0.    0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01  0.   -0.01 -0.   -0.   -0.  ]\n"," [-0.01  0.    0.   -0.   -0.01 -0.01  0.    0.  ]\n"," [-0.01  0.   -0.01 -0.    0.    0.    0.    0.03]\n"," [-0.01 -0.01 -0.01  0.    0.    0.02  0.02  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.02 -0.01]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.   -0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.02 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.   -0.01]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.   -0.01]\n"," [-0.01 -0.02 -0.01 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.    0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.03  0.02  0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.02 -0.01 -0.02 -0.02]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.02  0.    0.  ]\n"," [ 0.    0.01  0.02  0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 7000, 0.62, 0.01\n","Optimal Action Value Estimates:\n","[[ 0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01]\n"," [ 0.01  0.    0.    0.    0.    0.01  0.01  0.01]\n"," [ 0.    0.   -0.   -0.01 -0.01  0.    0.01  0.01]\n"," [ 0.   -0.   -0.    0.   -0.   -0.    0.01  0.03]\n"," [-0.    0.    0.   -0.   -0.   -0.    0.    0.06]\n"," [-0.02  0.   -0.01 -0.    0.    0.01  0.    0.08]\n"," [-0.02 -0.02 -0.02  0.    0.    0.02  0.04  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.    0.    0.01  0.01]\n"," [ 0.01  0.    0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.   -0.  ]\n"," [ 0.    0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.   -0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.02 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [-0.    0.   -0.    0.  ]\n"," [-0.   -0.   -0.01 -0.  ]\n"," [-0.01 -0.   -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.02]\n"," [-0.01 -0.01 -0.   -0.01]\n"," [ 0.01 -0.01  0.01  0.01]\n"," [ 0.02  0.03  0.02  0.02]\n"," [-0.01 -0.02 -0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.06  0.03  0.01]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.    0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.06  0.05  0.08  0.02]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.   -0.    0.  ]\n"," [ 0.01  0.02  0.01  0.  ]\n"," [ 0.    0.01  0.04  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 8000, 0.56, 0.01\n","Optimal Action Value Estimates:\n","[[ 0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.    0.    0.01  0.01  0.01  0.01]\n"," [ 0.01  0.    0.    0.    0.    0.    0.01  0.02]\n"," [ 0.    0.    0.    0.   -0.01  0.    0.01  0.02]\n"," [-0.    0.    0.    0.   -0.    0.    0.    0.08]\n"," [-0.01  0.   -0.01 -0.    0.    0.01  0.    0.14]\n"," [-0.02 -0.02 -0.02  0.    0.    0.02  0.04  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.    0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.    0.01]\n"," [ 0.    0.    0.01  0.  ]\n"," [ 0.   -0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.01  0.01]\n"," [ 0.    0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.02  0.   -0.02 -0.01]\n"," [-0.01 -0.01  0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.02  0.01]\n"," [ 0.   -0.    0.    0.  ]\n"," [ 0.   -0.01 -0.    0.  ]\n"," [-0.01 -0.01 -0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01  0.  ]\n"," [ 0.    0.01 -0.    0.01]\n"," [ 0.02  0.02  0.02  0.02]\n"," [-0.   -0.01 -0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01  0.   -0.01 -0.01]\n"," [-0.01 -0.01 -0.   -0.01]\n"," [ 0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.03  0.08  0.05  0.01]\n"," [-0.01 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.    0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.08  0.1   0.14  0.03]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.   -0.    0.  ]\n"," [ 0.01  0.02  0.01  0.  ]\n"," [ 0.    0.01  0.04  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 9000, 0.51, 0.02\n","Optimal Action Value Estimates:\n","[[ 0.01  0.01  0.01  0.02  0.02  0.03  0.03  0.03]\n"," [ 0.01  0.01  0.01  0.01  0.02  0.03  0.03  0.03]\n"," [ 0.01  0.01  0.01  0.    0.02  0.02  0.03  0.04]\n"," [ 0.01  0.01  0.01  0.01  0.01  0.    0.03  0.05]\n"," [ 0.01  0.01  0.01  0.   -0.    0.01  0.03  0.08]\n"," [ 0.    0.    0.   -0.   -0.01 -0.    0.    0.16]\n"," [ 0.    0.   -0.01 -0.    0.    0.    0.    0.23]\n"," [-0.   -0.02 -0.02  0.    0.    0.02  0.04  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.02  0.02  0.01]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.03  0.02  0.03  0.02]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.03  0.02]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.    0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01 -0.    0.01  0.02]\n"," [ 0.01  0.02  0.01  0.01]\n"," [ 0.02  0.02  0.03  0.02]\n"," [ 0.03  0.03  0.04  0.03]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.01  0.01  0.01  0.01]\n"," [-0.01  0.01 -0.01 -0.01]\n"," [ 0.01 -0.01 -0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.03  0.02  0.03]\n"," [ 0.04  0.05  0.04  0.03]\n"," [ 0.    0.01  0.    0.01]\n"," [ 0.   -0.    0.    0.01]\n"," [ 0.01 -0.01 -0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.02]\n"," [-0.01  0.01 -0.01  0.  ]\n"," [ 0.02  0.02  0.01  0.03]\n"," [ 0.05  0.03  0.08  0.03]\n"," [ 0.   -0.01 -0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.   -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.06  0.16  0.06  0.02]\n"," [ 0.   -0.02 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.    0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.09  0.13  0.23  0.09]\n"," [-0.01 -0.01 -0.01 -0.  ]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.   -0.    0.  ]\n"," [ 0.01  0.02  0.01  0.  ]\n"," [ 0.    0.01  0.04  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 10000, 0.45, 0.02\n","Optimal Action Value Estimates:\n","[[ 0.02  0.02  0.03  0.03  0.04  0.04  0.04  0.05]\n"," [ 0.02  0.02  0.03  0.03  0.03  0.04  0.04  0.05]\n"," [ 0.02  0.02  0.02  0.    0.04  0.04  0.05  0.06]\n"," [ 0.02  0.02  0.02  0.01  0.02  0.    0.05  0.06]\n"," [ 0.02  0.02  0.01  0.    0.01  0.04  0.07  0.08]\n"," [ 0.01  0.    0.   -0.   -0.01  0.    0.    0.13]\n"," [ 0.    0.   -0.01 -0.    0.   -0.    0.    0.18]\n"," [-0.01 -0.02 -0.02  0.   -0.    0.01  0.04  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.04  0.03  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.05  0.04]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.03  0.02  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.05  0.05  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.03  0.04  0.04  0.03]\n"," [ 0.04  0.03  0.04  0.04]\n"," [ 0.05  0.05  0.05  0.05]\n"," [ 0.05  0.05  0.06  0.05]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.    0.01  0.01  0.01]\n"," [ 0.02  0.   -0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.05  0.05  0.05  0.05]\n"," [ 0.06  0.06  0.06  0.05]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.01  0.01  0.02]\n"," [ 0.   -0.02 -0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01  0.    0.01 -0.  ]\n"," [-0.02  0.04  0.    0.01]\n"," [ 0.03  0.05  0.07  0.05]\n"," [ 0.06  0.08  0.07  0.05]\n"," [ 0.   -0.01 -0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.   -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.1   0.13  0.12  0.09]\n"," [-0.   -0.01 -0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.11  0.18  0.13  0.12]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.01  0.01  0.  ]\n"," [ 0.    0.01  0.04  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 11000, 0.40, 0.02\n","Optimal Action Value Estimates:\n","[[ 0.02  0.02  0.03  0.03  0.03  0.03  0.03  0.03]\n"," [ 0.02  0.02  0.02  0.02  0.03  0.03  0.03  0.04]\n"," [ 0.02  0.02  0.02  0.    0.03  0.03  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.01  0.02  0.    0.04  0.05]\n"," [ 0.02  0.02  0.02  0.    0.02  0.02  0.04  0.08]\n"," [ 0.01  0.    0.   -0.01 -0.01  0.    0.    0.13]\n"," [ 0.01  0.   -0.01 -0.    0.    0.    0.    0.22]\n"," [-0.   -0.01 -0.02  0.   -0.    0.02  0.05  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.03]\n"," [ 0.02  0.02  0.02  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.04  0.04  0.04  0.03]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.02  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.04  0.03]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.02  0.01  0.02  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.03  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.05  0.04]\n"," [ 0.01  0.02  0.02  0.02]\n"," [ 0.02  0.01  0.02  0.02]\n"," [ 0.01  0.    0.    0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01 -0.    0.02  0.  ]\n"," [-0.01  0.02  0.01  0.01]\n"," [ 0.03  0.04  0.04  0.04]\n"," [ 0.05  0.08  0.06  0.05]\n"," [ 0.01  0.    0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [ 0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.1   0.13  0.1   0.09]\n"," [ 0.01 -0.01 -0.02  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.    0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.11  0.22  0.15  0.11]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [-0.02 -0.01 -0.02 -0.02]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.01  0.02  0.01  0.  ]\n"," [ 0.    0.02  0.05  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 12000, 0.34, 0.03\n","Optimal Action Value Estimates:\n","[[ 0.03  0.04  0.04  0.04  0.05  0.05  0.05  0.05]\n"," [ 0.04  0.04  0.04  0.04  0.05  0.05  0.05  0.05]\n"," [ 0.04  0.03  0.03  0.    0.04  0.06  0.05  0.06]\n"," [ 0.03  0.03  0.03  0.01  0.03  0.    0.06  0.06]\n"," [ 0.03  0.03  0.03  0.    0.04  0.05  0.07  0.08]\n"," [ 0.02  0.    0.   -0.01  0.   -0.    0.    0.15]\n"," [ 0.01  0.   -0.01 -0.    0.   -0.    0.    0.26]\n"," [ 0.   -0.01 -0.02  0.   -0.    0.02  0.05  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.03  0.03  0.03  0.03]\n"," [ 0.04  0.03  0.03  0.03]\n"," [ 0.03  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.05  0.05  0.04]\n"," [ 0.05  0.05  0.05  0.05]\n"," [ 0.05  0.05  0.05  0.05]\n"," [ 0.05  0.05  0.05  0.05]\n"," [ 0.03  0.03  0.04  0.03]\n"," [ 0.03  0.03  0.04  0.03]\n"," [ 0.03  0.03  0.04  0.04]\n"," [ 0.03  0.04  0.04  0.04]\n"," [ 0.05  0.04  0.05  0.04]\n"," [ 0.05  0.05  0.05  0.05]\n"," [ 0.05  0.05  0.05  0.05]\n"," [ 0.05  0.05  0.05  0.05]\n"," [ 0.04  0.03  0.03  0.04]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.02  0.02  0.03]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.02  0.04  0.04]\n"," [ 0.04  0.03  0.03  0.06]\n"," [ 0.05  0.05  0.05  0.05]\n"," [ 0.06  0.06  0.05  0.06]\n"," [ 0.03  0.02  0.03  0.03]\n"," [ 0.02  0.02  0.03  0.02]\n"," [ 0.03  0.02  0.02  0.02]\n"," [ 0.    0.01 -0.    0.  ]\n"," [ 0.03  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.04  0.05  0.06  0.04]\n"," [ 0.06  0.06  0.06  0.06]\n"," [ 0.02  0.02  0.02  0.03]\n"," [ 0.02  0.02  0.02  0.03]\n"," [ 0.02 -0.   -0.    0.03]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.   -0.    0.04  0.01]\n"," [-0.01  0.05  0.01  0.01]\n"," [ 0.04  0.04  0.03  0.07]\n"," [ 0.06  0.07  0.08  0.07]\n"," [ 0.02  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.01  0.   -0.01]\n"," [-0.   -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.11  0.15  0.09  0.08]\n"," [ 0.01 -0.   -0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01 -0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.15  0.26  0.16  0.1 ]\n"," [ 0.   -0.01 -0.01 -0.  ]\n"," [-0.02 -0.01 -0.02 -0.02]\n"," [-0.02 -0.02 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.01  0.02  0.01  0.  ]\n"," [ 0.    0.02  0.05  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 13000, 0.29, 0.02\n","Optimal Action Value Estimates:\n","[[ 0.02  0.02  0.02  0.03  0.03  0.03  0.03  0.03]\n"," [ 0.02  0.02  0.02  0.03  0.03  0.03  0.03  0.03]\n"," [ 0.02  0.02  0.02  0.    0.03  0.03  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.01  0.02  0.    0.04  0.05]\n"," [ 0.02  0.02  0.01  0.    0.02  0.03  0.04  0.08]\n"," [ 0.01  0.    0.   -0.01  0.01  0.02  0.    0.21]\n"," [ 0.    0.   -0.01 -0.    0.    0.    0.    0.33]\n"," [ 0.    0.   -0.01  0.   -0.    0.02  0.06  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.03  0.02  0.02  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.01  0.02  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.03  0.02  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.04  0.03  0.03  0.03]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.    0.01 -0.01  0.01]\n"," [ 0.02  0.02  0.02  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.03  0.04  0.03  0.04]\n"," [ 0.05  0.05  0.05  0.04]\n"," [ 0.01  0.01  0.02  0.02]\n"," [ 0.02  0.    0.01  0.02]\n"," [ 0.01 -0.02  0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.   -0.01  0.02  0.01]\n"," [-0.    0.02  0.01  0.03]\n"," [ 0.03  0.04  0.04  0.04]\n"," [ 0.05  0.05  0.08  0.06]\n"," [ 0.01  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.02  0.01 -0.01]\n"," [ 0.02 -0.01 -0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.13  0.21  0.11  0.07]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01  0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.18  0.33  0.19  0.11]\n"," [ 0.   -0.01 -0.   -0.  ]\n"," [-0.01  0.   -0.02 -0.02]\n"," [-0.02 -0.01 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.01  0.02  0.01  0.  ]\n"," [ 0.    0.02  0.06  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 14000, 0.23, 0.03\n","Optimal Action Value Estimates:\n","[[ 0.03  0.03  0.03  0.03  0.04  0.04  0.04  0.04]\n"," [ 0.03  0.03  0.03  0.04  0.04  0.04  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.    0.04  0.04  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.02  0.03  0.    0.05  0.05]\n"," [ 0.02  0.02  0.02  0.    0.03  0.04  0.05  0.06]\n"," [ 0.02  0.    0.   -0.01  0.02  0.02  0.    0.21]\n"," [ 0.01  0.   -0.01 -0.    0.    0.    0.    0.33]\n"," [ 0.    0.   -0.01  0.   -0.    0.02  0.06  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.04  0.03]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.03]\n"," [ 0.02  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.03]\n"," [ 0.03  0.03  0.03  0.04]\n"," [ 0.03  0.03  0.04  0.03]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.01  0.02  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.03  0.03  0.03  0.04]\n"," [ 0.03  0.03  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [-0.01  0.01 -0.01  0.02]\n"," [ 0.02  0.02  0.03  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.05  0.04  0.05  0.04]\n"," [ 0.05  0.05  0.05  0.05]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.01  0.02  0.02  0.02]\n"," [ 0.01 -0.    0.01  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01 -0.01  0.03  0.01]\n"," [-0.    0.02  0.01  0.04]\n"," [ 0.04  0.04  0.05  0.04]\n"," [ 0.06  0.05  0.05  0.05]\n"," [ 0.02  0.01  0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.01 -0.02  0.02 -0.  ]\n"," [ 0.02 -0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.15  0.21  0.12  0.07]\n"," [ 0.    0.    0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01  0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.21  0.33  0.23  0.12]\n"," [ 0.   -0.01 -0.   -0.  ]\n"," [-0.01  0.   -0.02 -0.02]\n"," [-0.02 -0.01 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.01  0.02  0.01  0.  ]\n"," [ 0.    0.02  0.06  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 15000, 0.18, 0.03\n","Optimal Action Value Estimates:\n","[[ 0.03  0.03  0.04  0.04  0.04  0.04  0.04  0.05]\n"," [ 0.03  0.03  0.03  0.04  0.04  0.04  0.04  0.05]\n"," [ 0.03  0.03  0.03  0.    0.03  0.04  0.05  0.06]\n"," [ 0.02  0.02  0.02  0.02  0.02  0.    0.04  0.07]\n"," [ 0.02  0.02  0.02  0.    0.02  0.03  0.05  0.09]\n"," [ 0.02  0.    0.   -0.    0.02  0.02  0.    0.2 ]\n"," [ 0.01  0.   -0.01 -0.    0.    0.01  0.    0.39]\n"," [ 0.01 -0.   -0.01  0.   -0.    0.03  0.08  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.02  0.02  0.03  0.03]\n"," [ 0.03  0.02  0.03  0.03]\n"," [ 0.03  0.03  0.04  0.03]\n"," [ 0.03  0.03  0.03  0.04]\n"," [ 0.03  0.03  0.04  0.03]\n"," [ 0.03  0.04  0.04  0.03]\n"," [ 0.04  0.03  0.04  0.04]\n"," [ 0.04  0.04  0.05  0.04]\n"," [ 0.02  0.02  0.02  0.03]\n"," [ 0.02  0.03  0.03  0.02]\n"," [ 0.02  0.02  0.03  0.03]\n"," [ 0.02  0.02  0.02  0.04]\n"," [ 0.04  0.03  0.03  0.04]\n"," [ 0.03  0.03  0.04  0.03]\n"," [ 0.03  0.04  0.03  0.04]\n"," [ 0.04  0.04  0.05  0.04]\n"," [ 0.02  0.02  0.02  0.03]\n"," [ 0.02  0.02  0.02  0.03]\n"," [ 0.03  0.02  0.02  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.02  0.02  0.03]\n"," [ 0.03  0.03  0.03  0.04]\n"," [ 0.04  0.04  0.05  0.04]\n"," [ 0.06  0.04  0.04  0.05]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [-0.    0.01 -0.    0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.05  0.07  0.07  0.06]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.02  0.02  0.02]\n"," [ 0.02  0.01  0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01 -0.01  0.02  0.01]\n"," [-0.    0.03  0.01  0.03]\n"," [ 0.04  0.04  0.05  0.04]\n"," [ 0.05  0.05  0.09  0.06]\n"," [ 0.02  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.01]\n"," [-0.01 -0.02  0.02 -0.  ]\n"," [ 0.02 -0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.14  0.2   0.13  0.09]\n"," [ 0.01  0.    0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01  0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.2   0.39  0.23  0.14]\n"," [ 0.01 -0.    0.   -0.  ]\n"," [-0.01 -0.   -0.02 -0.02]\n"," [-0.02 -0.01 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.03  0.01  0.  ]\n"," [ 0.    0.02  0.08  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 16000, 0.12, 0.05\n","Optimal Action Value Estimates:\n","[[ 0.04  0.04  0.05  0.06  0.06  0.07  0.07  0.07]\n"," [ 0.04  0.05  0.05  0.06  0.06  0.07  0.08  0.08]\n"," [ 0.04  0.05  0.04  0.    0.06  0.07  0.09  0.08]\n"," [ 0.04  0.04  0.04  0.03  0.05  0.    0.09  0.11]\n"," [ 0.04  0.04  0.04  0.    0.03  0.03  0.09  0.15]\n"," [ 0.03  0.    0.   -0.    0.03  0.02  0.    0.23]\n"," [ 0.01  0.   -0.01 -0.    0.    0.01  0.    0.41]\n"," [ 0.   -0.   -0.01  0.   -0.    0.03  0.08  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.05  0.04  0.05  0.05]\n"," [ 0.05  0.05  0.06  0.05]\n"," [ 0.06  0.06  0.06  0.06]\n"," [ 0.06  0.06  0.07  0.06]\n"," [ 0.06  0.07  0.07  0.06]\n"," [ 0.06  0.07  0.07  0.06]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.05  0.04]\n"," [ 0.05  0.04  0.05  0.04]\n"," [ 0.05  0.05  0.05  0.06]\n"," [ 0.06  0.06  0.06  0.06]\n"," [ 0.06  0.07  0.06  0.06]\n"," [ 0.07  0.07  0.08  0.07]\n"," [ 0.07  0.08  0.07  0.07]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.05]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.06  0.05  0.06  0.05]\n"," [ 0.06  0.06  0.06  0.07]\n"," [ 0.07  0.07  0.09  0.07]\n"," [ 0.08  0.07  0.07  0.07]\n"," [ 0.03  0.03  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.03  0.04  0.04]\n"," [-0.    0.03 -0.    0.02]\n"," [ 0.03  0.02  0.03  0.05]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.06  0.09  0.07  0.06]\n"," [ 0.09  0.11  0.09  0.09]\n"," [ 0.03  0.03  0.02  0.04]\n"," [ 0.03  0.02  0.04  0.04]\n"," [ 0.04  0.    0.02  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.    0.03  0.01]\n"," [-0.    0.02  0.01  0.03]\n"," [ 0.06  0.07  0.09  0.07]\n"," [ 0.09  0.15  0.1   0.09]\n"," [ 0.03  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.01]\n"," [-0.01 -0.02  0.03 -0.  ]\n"," [ 0.02 -0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.16  0.23  0.18  0.13]\n"," [ 0.01  0.    0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01  0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.21  0.41  0.27  0.14]\n"," [ 0.   -0.    0.   -0.  ]\n"," [-0.01 -0.   -0.02 -0.02]\n"," [-0.02 -0.01 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.03  0.01  0.  ]\n"," [ 0.    0.02  0.08  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 17000, 0.07, 0.05\n","Optimal Action Value Estimates:\n","[[ 0.08  0.09  0.09  0.1   0.11  0.12  0.12  0.12]\n"," [ 0.08  0.08  0.08  0.09  0.1   0.11  0.12  0.12]\n"," [ 0.08  0.08  0.08  0.    0.1   0.1   0.12  0.13]\n"," [ 0.07  0.07  0.07  0.07  0.09  0.    0.13  0.14]\n"," [ 0.03  0.06  0.06  0.    0.06  0.06  0.14  0.17]\n"," [ 0.03  0.    0.   -0.    0.03  0.03  0.    0.24]\n"," [ 0.03  0.   -0.01 -0.    0.    0.01  0.    0.45]\n"," [ 0.   -0.   -0.01  0.   -0.    0.03  0.08  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.07  0.07  0.08  0.07]\n"," [ 0.07  0.08  0.09  0.07]\n"," [ 0.08  0.07  0.09  0.08]\n"," [ 0.09  0.09  0.1   0.09]\n"," [ 0.1   0.09  0.11  0.09]\n"," [ 0.1   0.1   0.12  0.1 ]\n"," [ 0.1   0.1   0.12  0.1 ]\n"," [ 0.1   0.12  0.1   0.11]\n"," [ 0.07  0.07  0.08  0.07]\n"," [ 0.07  0.07  0.08  0.08]\n"," [ 0.08  0.08  0.08  0.08]\n"," [ 0.08  0.08  0.08  0.09]\n"," [ 0.09  0.09  0.1   0.09]\n"," [ 0.1   0.1   0.11  0.1 ]\n"," [ 0.11  0.1   0.12  0.1 ]\n"," [ 0.1   0.11  0.12  0.11]\n"," [ 0.07  0.07  0.07  0.08]\n"," [ 0.08  0.07  0.07  0.08]\n"," [ 0.08  0.05  0.04  0.07]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.05  0.07  0.06  0.1 ]\n"," [ 0.08  0.06  0.1   0.08]\n"," [ 0.11  0.1   0.12  0.11]\n"," [ 0.12  0.13  0.12  0.11]\n"," [ 0.04  0.03  0.05  0.07]\n"," [ 0.05  0.05  0.07  0.06]\n"," [ 0.05  0.03  0.06  0.07]\n"," [ 0.02  0.07 -0.    0.03]\n"," [ 0.09  0.02  0.03  0.04]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.08  0.13  0.08  0.08]\n"," [ 0.11  0.14  0.13  0.1 ]\n"," [ 0.03  0.02  0.02  0.03]\n"," [ 0.03  0.02  0.06  0.03]\n"," [ 0.06  0.    0.03  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.    0.06  0.01]\n"," [-0.    0.02  0.01  0.06]\n"," [ 0.06  0.08  0.14  0.11]\n"," [ 0.14  0.17  0.14  0.14]\n"," [ 0.03  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.01]\n"," [-0.01 -0.02  0.03 -0.  ]\n"," [ 0.03 -0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.16  0.24  0.2   0.14]\n"," [ 0.03 -0.    0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01  0.01 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.22  0.45  0.28  0.18]\n"," [ 0.   -0.    0.   -0.  ]\n"," [-0.01 -0.   -0.02 -0.02]\n"," [-0.02 -0.01 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.03  0.01  0.  ]\n"," [ 0.    0.02  0.08  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 18000, 0.01, 0.07\n","Optimal Action Value Estimates:\n","[[ 0.12  0.13  0.15  0.15  0.16  0.17  0.17  0.17]\n"," [ 0.13  0.13  0.14  0.15  0.16  0.16  0.17  0.19]\n"," [ 0.09  0.1   0.08  0.    0.13  0.16  0.18  0.2 ]\n"," [ 0.07  0.09  0.09  0.07  0.08  0.    0.19  0.23]\n"," [ 0.05  0.08  0.08  0.    0.07  0.07  0.2   0.25]\n"," [ 0.03  0.    0.   -0.    0.05  0.04  0.    0.34]\n"," [ 0.03  0.   -0.01 -0.    0.    0.    0.    0.5 ]\n"," [ 0.   -0.   -0.01  0.   -0.    0.03  0.08  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.11  0.1   0.11  0.12]\n"," [ 0.09  0.09  0.13  0.1 ]\n"," [ 0.1   0.11  0.15  0.09]\n"," [ 0.08  0.11  0.15  0.1 ]\n"," [ 0.08  0.11  0.16  0.12]\n"," [ 0.13  0.12  0.17  0.1 ]\n"," [ 0.08  0.17  0.09  0.11]\n"," [ 0.11  0.14  0.17  0.13]\n"," [ 0.08  0.08  0.1   0.13]\n"," [ 0.09  0.07  0.08  0.13]\n"," [ 0.07  0.1   0.11  0.14]\n"," [ 0.08  0.09  0.09  0.15]\n"," [ 0.1   0.09  0.16  0.1 ]\n"," [ 0.12  0.1   0.16  0.11]\n"," [ 0.1   0.09  0.17  0.09]\n"," [ 0.13  0.13  0.19  0.14]\n"," [ 0.07  0.07  0.07  0.09]\n"," [ 0.08  0.06  0.1   0.06]\n"," [ 0.06  0.08  0.08  0.08]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.05  0.08  0.07  0.13]\n"," [ 0.07  0.09  0.16  0.1 ]\n"," [ 0.1   0.18  0.09  0.1 ]\n"," [ 0.11  0.2   0.11  0.1 ]\n"," [ 0.05  0.05  0.05  0.07]\n"," [ 0.06  0.06  0.09  0.07]\n"," [ 0.05  0.05  0.05  0.09]\n"," [ 0.05  0.07 -0.01  0.03]\n"," [ 0.08  0.03  0.03  0.04]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.09  0.19  0.1   0.09]\n"," [ 0.13  0.23  0.14  0.14]\n"," [ 0.03  0.02  0.02  0.05]\n"," [ 0.04  0.03  0.08  0.04]\n"," [ 0.08  0.    0.03  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.    0.07  0.01]\n"," [-0.    0.02  0.01  0.07]\n"," [ 0.06  0.08  0.2   0.11]\n"," [ 0.15  0.15  0.25  0.16]\n"," [ 0.03  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.01]\n"," [-0.01 -0.02  0.05 -0.  ]\n"," [ 0.04 -0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.16  0.34  0.2   0.14]\n"," [ 0.03 -0.    0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01  0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.22  0.5   0.31  0.17]\n"," [ 0.   -0.    0.   -0.  ]\n"," [-0.01 -0.   -0.02 -0.02]\n"," [-0.02 -0.01 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.03  0.01  0.  ]\n"," [ 0.    0.02  0.08  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 19000, 0.01, 0.04\n","Optimal Action Value Estimates:\n","[[ 0.06  0.06  0.06  0.07  0.08  0.09  0.09  0.09]\n"," [ 0.05  0.06  0.06  0.06  0.08  0.08  0.09  0.09]\n"," [ 0.05  0.05  0.05  0.    0.07  0.08  0.1   0.1 ]\n"," [ 0.05  0.04  0.04  0.04  0.05  0.    0.1   0.11]\n"," [ 0.04  0.04  0.04  0.    0.05  0.06  0.1   0.14]\n"," [ 0.03  0.    0.   -0.    0.04  0.04  0.    0.21]\n"," [ 0.03  0.   -0.01 -0.    0.    0.    0.    0.43]\n"," [ 0.   -0.   -0.01  0.   -0.    0.03  0.08  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.04  0.04  0.06  0.04]\n"," [ 0.05  0.04  0.06  0.04]\n"," [ 0.05  0.05  0.06  0.05]\n"," [ 0.05  0.05  0.05  0.07]\n"," [ 0.06  0.06  0.08  0.06]\n"," [ 0.07  0.07  0.09  0.07]\n"," [ 0.07  0.09  0.07  0.07]\n"," [ 0.09  0.07  0.07  0.07]\n"," [ 0.04  0.05  0.05  0.05]\n"," [ 0.05  0.05  0.04  0.06]\n"," [ 0.05  0.04  0.05  0.06]\n"," [ 0.05  0.06  0.05  0.05]\n"," [ 0.06  0.06  0.06  0.08]\n"," [ 0.07  0.07  0.08  0.07]\n"," [ 0.08  0.08  0.09  0.08]\n"," [ 0.08  0.09  0.08  0.08]\n"," [ 0.04  0.04  0.04  0.05]\n"," [ 0.04  0.04  0.05  0.04]\n"," [ 0.04  0.04  0.04  0.05]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.05  0.06  0.07  0.06]\n"," [ 0.07  0.07  0.07  0.08]\n"," [ 0.08  0.08  0.1   0.08]\n"," [ 0.08  0.1   0.08  0.08]\n"," [ 0.04  0.04  0.04  0.05]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04  0.04  0.04]\n"," [ 0.04  0.04 -0.01  0.03]\n"," [ 0.05  0.03  0.03  0.04]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.08  0.09  0.1   0.09]\n"," [ 0.11  0.09  0.09  0.09]\n"," [ 0.03  0.02  0.02  0.04]\n"," [ 0.04  0.03  0.04  0.04]\n"," [ 0.04  0.    0.03  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.    0.05  0.01]\n"," [-0.    0.02  0.01  0.06]\n"," [ 0.06  0.09  0.1   0.09]\n"," [ 0.11  0.14  0.11  0.11]\n"," [ 0.03  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.01]\n"," [-0.01 -0.02  0.04 -0.  ]\n"," [ 0.04 -0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.16  0.21  0.2   0.15]\n"," [ 0.03 -0.    0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.01  0.   -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.22  0.43  0.31  0.17]\n"," [ 0.   -0.    0.   -0.  ]\n"," [-0.01 -0.   -0.02 -0.02]\n"," [-0.02 -0.01 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.03  0.01  0.  ]\n"," [ 0.    0.02  0.08  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 20000, 0.01, 0.07\n","Optimal Action Value Estimates:\n","[[ 0.07  0.07  0.08  0.08  0.12  0.13  0.13  0.14]\n"," [ 0.07  0.07  0.07  0.1   0.12  0.13  0.13  0.13]\n"," [ 0.06  0.07  0.06  0.    0.1   0.13  0.14  0.15]\n"," [ 0.06  0.06  0.06  0.05  0.06  0.    0.15  0.18]\n"," [ 0.05  0.05  0.05  0.    0.06  0.09  0.17  0.21]\n"," [ 0.03  0.    0.   -0.    0.06  0.06  0.    0.32]\n"," [ 0.03  0.   -0.01 -0.    0.    0.03  0.    0.49]\n"," [ 0.   -0.   -0.01  0.    0.    0.06  0.11  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.05  0.04  0.05  0.07]\n"," [ 0.06  0.05  0.07  0.06]\n"," [ 0.06  0.06  0.08  0.06]\n"," [ 0.05  0.05  0.05  0.08]\n"," [ 0.06  0.06  0.12  0.07]\n"," [ 0.07  0.05  0.13  0.05]\n"," [ 0.05  0.05  0.13  0.05]\n"," [ 0.08  0.06  0.14  0.09]\n"," [ 0.05  0.05  0.07  0.05]\n"," [ 0.04  0.06  0.05  0.07]\n"," [ 0.04  0.06  0.06  0.07]\n"," [ 0.03  0.04  0.04  0.1 ]\n"," [ 0.05  0.04  0.12  0.04]\n"," [ 0.05  0.07  0.13  0.06]\n"," [ 0.08  0.06  0.13  0.05]\n"," [ 0.09  0.13  0.09  0.06]\n"," [ 0.05  0.05  0.06  0.05]\n"," [ 0.07  0.04  0.05  0.04]\n"," [ 0.06  0.03  0.03  0.03]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.1   0.05  0.04  0.04]\n"," [ 0.05  0.05  0.05  0.13]\n"," [ 0.05  0.14  0.05  0.05]\n"," [ 0.15  0.08  0.06  0.07]\n"," [ 0.05  0.05  0.05  0.06]\n"," [ 0.05  0.04  0.05  0.06]\n"," [ 0.05  0.04  0.05  0.06]\n"," [ 0.03  0.05 -0.01  0.03]\n"," [ 0.06  0.03  0.03  0.04]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.07  0.15  0.07  0.06]\n"," [ 0.08  0.18  0.07  0.08]\n"," [ 0.03  0.02  0.02  0.05]\n"," [ 0.03  0.03  0.03  0.05]\n"," [ 0.05  0.    0.03  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.    0.06  0.01]\n"," [ 0.    0.02  0.01  0.09]\n"," [ 0.06  0.07  0.07  0.17]\n"," [ 0.11  0.08  0.21  0.08]\n"," [ 0.03  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.01]\n"," [-0.01 -0.02  0.06 -0.  ]\n"," [ 0.06 -0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.16  0.32  0.2   0.15]\n"," [ 0.03 -0.    0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.   -0.01  0.03 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.22  0.49  0.31  0.17]\n"," [ 0.   -0.    0.   -0.  ]\n"," [-0.01 -0.   -0.02 -0.02]\n"," [-0.02 -0.01 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.   -0.   -0.  ]\n"," [ 0.    0.06  0.01  0.  ]\n"," [ 0.    0.02  0.11  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"IAqLLdIjZs6y","colab_type":"code","outputId":"3fd669c0-1ac2-4a09-c307-a688404ab279","executionInfo":{"status":"ok","timestamp":1553379566900,"user_tz":240,"elapsed":443,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":1354}},"cell_type":"code","source":["print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                np.mean(stats_success[-1000:])))\n","optimal_action_estimates = np.max(action_value_array,axis=1)\n","print(\"Optimal Action Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(8,8),2))\n","print(\"estimate of the optimal state value at each state\")\n","print(\"\")\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_value_array.reshape((64,4)),2))\n","print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 20000, 0.01, 0.07\n","Optimal Action Value Estimates:\n","[[ 0.07  0.07  0.08  0.08  0.12  0.13  0.13  0.14]\n"," [ 0.07  0.07  0.07  0.1   0.12  0.13  0.13  0.13]\n"," [ 0.06  0.07  0.06  0.    0.1   0.13  0.14  0.15]\n"," [ 0.06  0.06  0.06  0.05  0.06  0.    0.15  0.18]\n"," [ 0.05  0.05  0.05  0.    0.06  0.09  0.17  0.21]\n"," [ 0.03  0.    0.   -0.    0.06  0.06  0.    0.32]\n"," [ 0.03  0.   -0.01 -0.    0.    0.03  0.    0.49]\n"," [ 0.   -0.   -0.01  0.    0.    0.06  0.11  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.05  0.04  0.05  0.07]\n"," [ 0.06  0.05  0.07  0.06]\n"," [ 0.06  0.06  0.08  0.06]\n"," [ 0.05  0.05  0.05  0.08]\n"," [ 0.06  0.06  0.12  0.07]\n"," [ 0.07  0.05  0.13  0.05]\n"," [ 0.05  0.05  0.13  0.05]\n"," [ 0.08  0.06  0.14  0.09]\n"," [ 0.05  0.05  0.07  0.05]\n"," [ 0.04  0.06  0.05  0.07]\n"," [ 0.04  0.06  0.06  0.07]\n"," [ 0.03  0.04  0.04  0.1 ]\n"," [ 0.05  0.04  0.12  0.04]\n"," [ 0.05  0.07  0.13  0.06]\n"," [ 0.08  0.06  0.13  0.05]\n"," [ 0.09  0.13  0.09  0.06]\n"," [ 0.05  0.05  0.06  0.05]\n"," [ 0.07  0.04  0.05  0.04]\n"," [ 0.06  0.03  0.03  0.03]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.1   0.05  0.04  0.04]\n"," [ 0.05  0.05  0.05  0.13]\n"," [ 0.05  0.14  0.05  0.05]\n"," [ 0.15  0.08  0.06  0.07]\n"," [ 0.05  0.05  0.05  0.06]\n"," [ 0.05  0.04  0.05  0.06]\n"," [ 0.05  0.04  0.05  0.06]\n"," [ 0.03  0.05 -0.01  0.03]\n"," [ 0.06  0.03  0.03  0.04]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.07  0.15  0.07  0.06]\n"," [ 0.08  0.18  0.07  0.08]\n"," [ 0.03  0.02  0.02  0.05]\n"," [ 0.03  0.03  0.03  0.05]\n"," [ 0.05  0.    0.03  0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.    0.06  0.01]\n"," [ 0.    0.02  0.01  0.09]\n"," [ 0.06  0.07  0.07  0.17]\n"," [ 0.11  0.08  0.21  0.08]\n"," [ 0.03  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.   -0.01]\n"," [-0.01 -0.02  0.06 -0.  ]\n"," [ 0.06 -0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.16  0.32  0.2   0.15]\n"," [ 0.03 -0.    0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01 -0.01 -0.01 -0.01]\n"," [-0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.   -0.01  0.03 -0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.22  0.49  0.31  0.17]\n"," [ 0.   -0.    0.   -0.  ]\n"," [-0.01 -0.   -0.02 -0.02]\n"," [-0.02 -0.01 -0.02 -0.02]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.   -0.   -0.  ]\n"," [ 0.    0.06  0.01  0.  ]\n"," [ 0.    0.02  0.11  0.01]\n"," [ 0.    0.    0.    0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"ML0Jw8C0hVEH","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}