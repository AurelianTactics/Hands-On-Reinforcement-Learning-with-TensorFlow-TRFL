{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reinforcement Learning with TensorFlow & TRFL: TD(λ).ipynb","version":"0.3.2","provenance":[{"file_id":"1BmiEkGoqe_CAaZmggviivF7Ygbd1iPe4","timestamp":1552945822274},{"file_id":"110YMHk2yHqxguCj1yaJaXVmkRUadX9GV","timestamp":1552791755615},{"file_id":"1cMFan2NCLOZ8w_xKSEyXTF09paJ0F930","timestamp":1552618439794},{"file_id":"1SToTDuBpTdV2UVRN9bapskLUesVhBH1J","timestamp":1551831097681},{"file_id":"1ssliB1HogX4KFHRyNKLU2aeiAV865kLv","timestamp":1551578593280},{"file_id":"1N74NgQBdDCDRER81p_VdJA_lsZOR2Hjo","timestamp":1550336742833}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"npzJ574a6A94","colab_type":"text"},"cell_type":"markdown","source":["**Reinforcement Learning with TensorFlow & TRFL: TD(λ)**\n","\n","Outline:\n","1. TD(λ)\n","1. TD(λ) with trfl.td_lambda() toy example\n","1. FrozenLake\n","* TD(λ) with trfl.td_lambda() on FrozenLake\n"]},{"metadata":{"id":"RyxlWytnVqJI","colab_type":"code","outputId":"fcfc73e2-58fc-496b-9af0-67976c449c2d","colab":{"base_uri":"https://localhost:8080/","height":534}},"cell_type":"code","source":["#TRFL works with TensorFlow 1.12\n","#installs TensorFlow version 1.12 then restarts the runtime\n","!pip install tensorflow==1.12\n","\n","import os\n","os.kill(os.getpid(), 9)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.12\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (83.1MB)\n","\u001b[K    100% |████████████████████████████████| 83.1MB 290kB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.15.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.14.6)\n","Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow==1.12)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n","\u001b[K    100% |████████████████████████████████| 3.1MB 11.5MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.11.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.33.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (3.7.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.7)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.2.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.9)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (0.14.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.0.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (40.8.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.8.0)\n","Installing collected packages: tensorboard, tensorflow\n","  Found existing installation: tensorboard 1.13.1\n","    Uninstalling tensorboard-1.13.1:\n","      Successfully uninstalled tensorboard-1.13.1\n","  Found existing installation: tensorflow 1.13.1\n","    Uninstalling tensorflow-1.13.1:\n","      Successfully uninstalled tensorflow-1.13.1\n","Successfully installed tensorboard-1.12.2 tensorflow-1.12.0\n"],"name":"stdout"}]},{"metadata":{"id":"XRS56AQDVybG","colab_type":"code","outputId":"3fb31e55-b3e3-47cf-e75a-60811ebad252","executionInfo":{"status":"ok","timestamp":1553340996194,"user_tz":240,"elapsed":7426,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":363}},"cell_type":"code","source":["#install tensorflow-probability 0.5.0 that works with TensorFlow 1.12\n","!pip install tensorflow-probability==0.5.0\n","\n","#install TRFL\n","!pip install trfl"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-probability==0.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/ca/6f213618b5f7d0bf6139e6ec928d412a5ca14e4776adfd41a59c74a34021/tensorflow_probability-0.5.0-py2.py3-none-any.whl (680kB)\n","\u001b[K    100% |████████████████████████████████| 686kB 14.9MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.11.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.14.6)\n","Installing collected packages: tensorflow-probability\n","  Found existing installation: tensorflow-probability 0.6.0\n","    Uninstalling tensorflow-probability-0.6.0:\n","      Successfully uninstalled tensorflow-probability-0.6.0\n","Successfully installed tensorflow-probability-0.5.0\n","Collecting trfl\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/22/86ae5bb7c5c134ddc0d50332bf8d7dd16c48bf82fad6dcba6c401ab2ca01/trfl-1.0-cp36-cp36m-manylinux1_x86_64.whl (78kB)\n","\u001b[K    100% |████████████████████████████████| 81kB 4.9MB/s \n","\u001b[?25hRequirement already satisfied: dm-sonnet in /usr/local/lib/python3.6/dist-packages (from trfl) (1.23)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trfl) (0.7.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trfl) (1.11.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trfl) (1.14.6)\n","Installing collected packages: trfl\n","Successfully installed trfl-1.0\n"],"name":"stdout"}]},{"metadata":{"id":"SGop2a_BZCBl","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import trfl\n","import tensorflow_probability as tfp"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FI6L75U7UWtj","colab_type":"text"},"cell_type":"markdown","source":["** TD(λ) **\n","\n","TD(λ) is a way of weighting n-step returns and relating Monte Carlo and TD methods. TD(0), ie λ = 0, is the one step return. TD(1), ie λ = 1, is the Monte Carlo return. In the toy example and FrozenLake examples in this notebook, you can modify the λ values to see the changes in state estimation.\n","\n","\n","** TD(λ) Toy Example **\n","\n","Below is a toy example to illustrate the affects of different values of λ, discount, and learning_rate. Imagine a number line where the agent starts at 0. The agent can only move right and the episode ends when the agent reaches 10. The agent receives a reward of 1 at state 10. The toy example lets you change various hyperparameters to see how the TD(λ) updates change."]},{"metadata":{"id":"gCi2qKhny9Mv","colab_type":"code","outputId":"b54853ec-d8c8-4982-ee1b-00d29f363719","executionInfo":{"status":"ok","timestamp":1553377414137,"user_tz":240,"elapsed":645,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":188}},"cell_type":"code","source":["#hyperparamters\n","learning_rate = 0.1 #update state values with this size learning rate\n","lambda_value = 0.9 #lambda value. range: [0,1]. weight lambda returns by this amount\n","discount = 0.9 #discount factor (gamma). range: [0,1]. Decay rewards by this amount each step\n","n_episode = 10 #number of episodes to run toy example\n","done_state = 10 #episode ends after reaching this state\n","\n","#set up env, tensors and TRFL\n","state_value_array = np.zeros((done_state+1,1)) #estimated values of states\n","\n","tf.reset_default_graph()\n","\n","state_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"state_value\")\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","bootstrap_ = tf.placeholder(dtype=tf.float32, shape=[1], name=\"bootstrap\")\n","\n","#TRFL usage: TD Lambda\n","td_lambda_return_ = trfl.td_lambda(state_, reward_, discount_, bootstrap_, lambda_=lambda_value)\n","\n","with tf.Session() as sess:\n","  for i in range(n_episode):\n","    current_state = 0\n","    state_list, reward_list, state_int_list = [], [], []\n","    done = 0\n","    \n","    while not done:\n","      state_list.append(state_value_array[current_state])\n","      state_int_list.append(current_state)\n","\n","      current_state += 1\n","\n","      if current_state < done_state+1:\n","        reward = 0\n","        done = 0\n","        bootstrap_v = state_value_array[current_state]\n","      else:\n","        reward = 1\n","        done = 1\n","        bootstrap_v = 0.\n","      reward_list.append(reward)   \n","      \n","      td_lambda_output = sess.run(td_lambda_return_, feed_dict={\n","          state_:np.array(state_list).reshape(-1,1),\n","          reward_:np.array(reward_list).reshape(-1,1),\n","          discount_:np.array([discount]*len(state_list)).reshape(-1,1),\n","          bootstrap_:np.array(bootstrap_v).reshape((1,))\n","        })\n","\n","      state_value_array[state_int_list] += learning_rate*td_lambda_output.extra.temporal_differences\n","\n","    print(\"Finished episode {}: state values are {}\".format(i,np.round(np.squeeze(state_value_array),3)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Finished episode 0: state values are [0.012 0.015 0.019 0.023 0.028 0.035 0.043 0.053 0.066 0.081 0.1  ]\n","Finished episode 1: state values are [0.03  0.036 0.043 0.051 0.061 0.073 0.088 0.106 0.128 0.156 0.19 ]\n","Finished episode 2: state values are [0.051 0.059 0.069 0.081 0.095 0.112 0.132 0.157 0.187 0.224 0.271]\n","Finished episode 3: state values are [0.072 0.084 0.096 0.111 0.129 0.15  0.175 0.205 0.242 0.287 0.344]\n","Finished episode 4: state values are [0.094 0.108 0.123 0.141 0.162 0.187 0.216 0.251 0.293 0.344 0.41 ]\n","Finished episode 5: state values are [0.115 0.131 0.149 0.17  0.194 0.222 0.255 0.293 0.34  0.397 0.469]\n","Finished episode 6: state values are [0.135 0.153 0.174 0.197 0.224 0.255 0.291 0.333 0.383 0.445 0.522]\n","Finished episode 7: state values are [0.154 0.174 0.196 0.222 0.251 0.285 0.324 0.369 0.423 0.488 0.57 ]\n","Finished episode 8: state values are [0.172 0.193 0.218 0.245 0.277 0.313 0.355 0.403 0.46  0.528 0.613]\n","Finished episode 9: state values are [0.188 0.211 0.237 0.267 0.301 0.339 0.383 0.434 0.493 0.563 0.651]\n"],"name":"stdout"}]},{"metadata":{"id":"IlXBfRia0b1C","colab_type":"text"},"cell_type":"markdown","source":["** TRFL Usage **\n","\n","The lambda_ in trfl.td_lambda() defaults to 1.0. The optional argument can be a constant between 0 and 1 or a tensor of values. The shape of the inputs [sequence_length, batch_size] where sequence_length is the time dimension where index 0 is the start of the sequence. The exception is bootstrap_ input which is shape batch_size and is the bootstrap value for each sequence.\n","\n","** FrozenLake **\n","\n","FrozenLake is a GridWorld like environment (env) where the agent tries to navigate a two-dimensional world from a starting point a goal. The agent has to avoid falling into holes while navigating a slippery surface that sometimes causes the agent to move unexpectedly."]},{"metadata":{"id":"4G9w9t6O0a0Z","colab_type":"code","outputId":"37f1122a-f8ad-4443-ccef-8f3bb9eebff3","executionInfo":{"status":"ok","timestamp":1553356905476,"user_tz":240,"elapsed":454,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":960}},"cell_type":"code","source":["env = gym.make('FrozenLake-v0')\n","env.reset()\n","env.reset()\n","env.render()\n","\n","for i in range(10):\n","  action = env.action_space.sample()\n","  obs, reward, done, info = env.step(action)\n","  env.render()\n","  if done:\n","    env.reset()\n","    \n","env.close()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","  (Down)\n","SFFF\n","F\u001b[41mH\u001b[0mFH\n","FFFH\n","HFFG\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","  (Up)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","  (Down)\n","SFFF\n","FHF\u001b[41mH\u001b[0m\n","FFFH\n","HFFG\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n"],"name":"stdout"}]},{"metadata":{"id":"KMY4yn1q3sfu","colab_type":"text"},"cell_type":"markdown","source":["** TD(λ) with trfl.td_lambda() **\n","\n","We'll do two examples. In both cases the agent will move under a random policy. In the first example, the agent will act deterministically. In the second, the agent will act non-deterministically and sometimes move in an unexpected way."]},{"metadata":{"id":"atNhOPAJlWtb","colab_type":"code","colab":{}},"cell_type":"code","source":["#deterministic env\n","from gym.envs.registration import register\n","register(\n","    id='FrozenLakeNotSlippery-v0',\n","    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name' : '4x4', 'is_slippery': False}\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"s6LkhpKyLH7X","colab_type":"code","colab":{}},"cell_type":"code","source":["#env = gym.make('FrozenLake-v0')\n","env = gym.make('FrozenLakeNotSlippery-v0')\n","\n","#hyperparameters\n","episodes = 10000\n","learning_rate = 0.01\n","discount = 1.\n","lambda_val = 0.5\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","state_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"state_value\")\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","bootstrap_ = tf.placeholder(dtype=tf.float32, shape=[1], name=\"bootstrap\")\n","#lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\") #optionally can do lambda placeholder\n","td_lambda_return_ = trfl.td_lambda(state_, reward_, discount_, bootstrap_, lambda_=lambda_val)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GFfd4RTI30M7","colab_type":"text"},"cell_type":"markdown","source":["** TRFL Usage **\n","\n","The lambda_ in trfl.td_lambda() defaults to 1.0. The optional argument can be a constant between 0 and 1 or a tensor of values. The shape of the inputs [sequence_length, batch_size] where sequence_length is the time dimension where index 0 is the start of the sequence. The exception is bootstrap_ input which is shape batch_size and is the bootstrap value for each sequence."]},{"metadata":{"id":"69dhkdh5nMNL","colab_type":"code","outputId":"58f3c1f2-a659-44d6-fa6c-2aab6528fe39","executionInfo":{"status":"ok","timestamp":1553377994042,"user_tz":240,"elapsed":105567,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":1217}},"cell_type":"code","source":["with tf.Session() as sess:\n","  \n","  #initialize the estimated state values to zero\n","  state_value_array = np.zeros((16,1))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  state_list, reward_list, state_int_list = [], [], []\n","\n","  while current_episode < episodes:\n","    #take a random action\n","    random_action = env.action_space.sample()\n","    next_state, rew, done, info = env.step(random_action)\n","\n","    state_list.append(state_value_array[current_state])\n","    state_int_list.append(current_state)\n","    reward_list.append(rew)\n","    bootstrap_v = state_value_array[next_state]\n","    \n","    current_state = next_state\n","    \n","    #run td lambda in the session to get lambda returns\n","    td_lambda_output = sess.run(td_lambda_return_, feed_dict={\n","        state_:np.array(state_list).reshape(-1,1),\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(state_list)).reshape(-1,1),\n","        bootstrap_:np.array(bootstrap_v).reshape((1,))\n","      })\n","    #use the lambda returns to update the tabular state value esimates\n","    state_value_array[state_int_list] += learning_rate*td_lambda_output.extra.temporal_differences\n","    \n","    if done:\n","      state_list, reward_list, state_int_list = [], [], []\n","      current_state = env.reset()\n","      current_episode += 1\n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode: {}\".format(current_episode))\n","        print(\"Reshaped State Value Estimates:\")\n","        print(np.round(state_value_array.reshape(4,4),3))\n","        print(\"\")  \n","          "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode: 1000\n","Reshaped State Value Estimates:\n","[[0.008 0.007 0.009 0.005]\n"," [0.008 0.    0.013 0.   ]\n"," [0.013 0.022 0.031 0.   ]\n"," [0.    0.03  0.102 0.   ]]\n","\n","Current Episode: 2000\n","Reshaped State Value Estimates:\n","[[0.017 0.016 0.019 0.017]\n"," [0.019 0.    0.025 0.   ]\n"," [0.023 0.037 0.048 0.   ]\n"," [0.    0.054 0.132 0.   ]]\n","\n","Current Episode: 3000\n","Reshaped State Value Estimates:\n","[[0.027 0.027 0.036 0.024]\n"," [0.027 0.    0.053 0.   ]\n"," [0.035 0.056 0.094 0.   ]\n"," [0.    0.076 0.19  0.   ]]\n","\n","Current Episode: 4000\n","Reshaped State Value Estimates:\n","[[0.039 0.039 0.05  0.035]\n"," [0.037 0.    0.069 0.   ]\n"," [0.045 0.069 0.112 0.   ]\n"," [0.    0.11  0.213 0.   ]]\n","\n","Current Episode: 5000\n","Reshaped State Value Estimates:\n","[[0.038 0.037 0.044 0.04 ]\n"," [0.039 0.    0.055 0.   ]\n"," [0.053 0.077 0.111 0.   ]\n"," [0.    0.12  0.222 0.   ]]\n","\n","Current Episode: 6000\n","Reshaped State Value Estimates:\n","[[0.045 0.044 0.057 0.047]\n"," [0.046 0.    0.078 0.   ]\n"," [0.06  0.095 0.132 0.   ]\n"," [0.    0.14  0.285 0.   ]]\n","\n","Current Episode: 7000\n","Reshaped State Value Estimates:\n","[[0.059 0.053 0.057 0.047]\n"," [0.066 0.    0.074 0.   ]\n"," [0.089 0.129 0.143 0.   ]\n"," [0.    0.182 0.241 0.   ]]\n","\n","Current Episode: 8000\n","Reshaped State Value Estimates:\n","[[0.057 0.052 0.055 0.05 ]\n"," [0.065 0.    0.064 0.   ]\n"," [0.103 0.146 0.118 0.   ]\n"," [0.    0.184 0.219 0.   ]]\n","\n","Current Episode: 9000\n","Reshaped State Value Estimates:\n","[[0.055 0.05  0.055 0.041]\n"," [0.057 0.    0.075 0.   ]\n"," [0.077 0.12  0.144 0.   ]\n"," [0.    0.167 0.284 0.   ]]\n","\n","Current Episode: 10000\n","Reshaped State Value Estimates:\n","[[0.065 0.064 0.069 0.061]\n"," [0.068 0.    0.095 0.   ]\n"," [0.093 0.13  0.16  0.   ]\n"," [0.    0.168 0.275 0.   ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"Ja90iYZ0iL9-","colab_type":"code","colab":{}},"cell_type":"code","source":["#non deterministic env\n","env = gym.make('FrozenLake-v0')\n","#env = gym.make('FrozenLakeNotSlippery-v0')\n","\n","#hyperparameters\n","episodes = 10000\n","learning_rate = 0.01\n","discount = 0.99\n","lambda_val = 0.5\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","state_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"state_value\")\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","bootstrap_ = tf.placeholder(dtype=tf.float32, shape=[1], name=\"bootstrap\")\n","#lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\") #optionally can do lambda placeholder\n","td_lambda_return_ = trfl.td_lambda(state_, reward_, discount_, bootstrap_, lambda_=lambda_val)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cPeY43Y6ib1q","colab_type":"code","outputId":"1d47d46b-8370-4f78-8480-7fb3ab4b49ef","executionInfo":{"status":"ok","timestamp":1553378147713,"user_tz":240,"elapsed":107208,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":1217}},"cell_type":"code","source":["with tf.Session() as sess:\n","  \n","  #initialize the estimated state values to zero\n","  state_value_array = np.zeros((16,1))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  state_list, reward_list, state_int_list = [], [], []\n","\n","  while current_episode < episodes:\n","    #take a random action\n","    random_action = env.action_space.sample()\n","    next_state, rew, done, info = env.step(random_action)\n","\n","    state_list.append(state_value_array[current_state])\n","    state_int_list.append(current_state)\n","    reward_list.append(rew)\n","    bootstrap_v = state_value_array[next_state]\n","    \n","    current_state = next_state\n","    #run td lambda in the session to get lambda returns\n","    td_lambda_output = sess.run(td_lambda_return_, feed_dict={\n","        state_:np.array(state_list).reshape(-1,1),\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(state_list)).reshape(-1,1),\n","        bootstrap_:np.array(bootstrap_v).reshape((1,))\n","      })\n","    #use the lambda returns to update the tabular state value esimates\n","    state_value_array[state_int_list] += learning_rate*td_lambda_output.extra.temporal_differences\n","    \n","    if done:\n","      state_list, reward_list, state_int_list = [], [], []\n","      current_state = env.reset()\n","      current_episode += 1\n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode: {}\".format(current_episode))\n","        print(\"Reshaped State Value Estimates:\")\n","        print(np.round(state_value_array.reshape(4,4),3))\n","        print(\"\")  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode: 1000\n","Reshaped State Value Estimates:\n","[[0.01  0.01  0.012 0.009]\n"," [0.011 0.    0.019 0.   ]\n"," [0.015 0.027 0.035 0.   ]\n"," [0.    0.044 0.094 0.   ]]\n","\n","Current Episode: 2000\n","Reshaped State Value Estimates:\n","[[0.016 0.014 0.016 0.013]\n"," [0.018 0.    0.022 0.   ]\n"," [0.03  0.041 0.054 0.   ]\n"," [0.    0.063 0.14  0.   ]]\n","\n","Current Episode: 3000\n","Reshaped State Value Estimates:\n","[[0.028 0.03  0.034 0.031]\n"," [0.026 0.    0.04  0.   ]\n"," [0.035 0.065 0.078 0.   ]\n"," [0.    0.11  0.196 0.   ]]\n","\n","Current Episode: 4000\n","Reshaped State Value Estimates:\n","[[0.032 0.03  0.034 0.027]\n"," [0.034 0.    0.045 0.   ]\n"," [0.046 0.09  0.098 0.   ]\n"," [0.    0.127 0.181 0.   ]]\n","\n","Current Episode: 5000\n","Reshaped State Value Estimates:\n","[[0.041 0.04  0.045 0.032]\n"," [0.044 0.    0.061 0.   ]\n"," [0.053 0.085 0.116 0.   ]\n"," [0.    0.14  0.215 0.   ]]\n","\n","Current Episode: 6000\n","Reshaped State Value Estimates:\n","[[0.042 0.039 0.043 0.04 ]\n"," [0.047 0.    0.055 0.   ]\n"," [0.065 0.094 0.097 0.   ]\n"," [0.    0.13  0.208 0.   ]]\n","\n","Current Episode: 7000\n","Reshaped State Value Estimates:\n","[[0.034 0.032 0.035 0.03 ]\n"," [0.038 0.    0.046 0.   ]\n"," [0.054 0.075 0.096 0.   ]\n"," [0.    0.133 0.218 0.   ]]\n","\n","Current Episode: 8000\n","Reshaped State Value Estimates:\n","[[0.032 0.031 0.039 0.033]\n"," [0.035 0.    0.056 0.   ]\n"," [0.047 0.072 0.091 0.   ]\n"," [0.    0.143 0.247 0.   ]]\n","\n","Current Episode: 9000\n","Reshaped State Value Estimates:\n","[[0.036 0.035 0.039 0.03 ]\n"," [0.039 0.    0.051 0.   ]\n"," [0.056 0.083 0.101 0.   ]\n"," [0.    0.111 0.205 0.   ]]\n","\n","Current Episode: 10000\n","Reshaped State Value Estimates:\n","[[0.034 0.029 0.033 0.032]\n"," [0.039 0.    0.037 0.   ]\n"," [0.053 0.086 0.086 0.   ]\n"," [0.    0.139 0.205 0.   ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"mvr0pGmRiqIU","colab_type":"text"},"cell_type":"markdown","source":["The agent is not very successful when acting randomly. When we do Q(λ) later this section, we'll see the agent perform better with a learned policy and see how changing values like discount, lambda, and learning rate can change performance."]}]}