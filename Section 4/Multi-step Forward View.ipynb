{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reinforcement Learning with TensorFlow & TRFL: Multi-step Forward View.ipynb","version":"0.3.2","provenance":[{"file_id":"1VSxFkfFGUF-xijRu3nDa39WyEcjwpz1-","timestamp":1553437836799},{"file_id":"15y3NrJjPTRhzYm0OWfoc813cR9BtCTGl","timestamp":1553044789533},{"file_id":"1BmiEkGoqe_CAaZmggviivF7Ygbd1iPe4","timestamp":1552945822274},{"file_id":"110YMHk2yHqxguCj1yaJaXVmkRUadX9GV","timestamp":1552791755615},{"file_id":"1cMFan2NCLOZ8w_xKSEyXTF09paJ0F930","timestamp":1552618439794},{"file_id":"1SToTDuBpTdV2UVRN9bapskLUesVhBH1J","timestamp":1551831097681},{"file_id":"1ssliB1HogX4KFHRyNKLU2aeiAV865kLv","timestamp":1551578593280},{"file_id":"1N74NgQBdDCDRER81p_VdJA_lsZOR2Hjo","timestamp":1550336742833}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"npzJ574a6A94","colab_type":"text"},"cell_type":"markdown","source":["**Reinforcement Learning with TensorFlow & TRFL: Multi-step Forward View**\n","\n","Outline:\n","* Watkins Q(λ) with trfl.multistep_forward_view()\n","\n"]},{"metadata":{"id":"RyxlWytnVqJI","colab_type":"code","outputId":"69542c02-1bca-4b79-e930-d839ba1705be","colab":{"base_uri":"https://localhost:8080/","height":534}},"cell_type":"code","source":["#TRFL works with TensorFlow 1.12\n","#installs TensorFlow version 1.12 then restarts the runtime\n","!pip install tensorflow==1.12\n","\n","import os\n","os.kill(os.getpid(), 9)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.12\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (83.1MB)\n","\u001b[K    100% |████████████████████████████████| 83.1MB 329kB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.15.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.33.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.7)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.14.6)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.11.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.9)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.2.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (3.7.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow==1.12)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n","\u001b[K    100% |████████████████████████████████| 3.1MB 11.6MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (40.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.0.1)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (0.14.1)\n","Installing collected packages: tensorboard, tensorflow\n","  Found existing installation: tensorboard 1.13.1\n","    Uninstalling tensorboard-1.13.1:\n","      Successfully uninstalled tensorboard-1.13.1\n","  Found existing installation: tensorflow 1.13.1\n","    Uninstalling tensorflow-1.13.1:\n","      Successfully uninstalled tensorflow-1.13.1\n","Successfully installed tensorboard-1.12.2 tensorflow-1.12.0\n"],"name":"stdout"}]},{"metadata":{"id":"XRS56AQDVybG","colab_type":"code","outputId":"991e7907-8ebf-4987-ffb5-05d360978f46","executionInfo":{"status":"ok","timestamp":1553448408198,"user_tz":240,"elapsed":10370,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":174}},"cell_type":"code","source":["#install tensorflow-probability 0.5.0 that works with TensorFlow 1.12\n","!pip install tensorflow-probability==0.5.0\n","\n","#install TRFL\n","!pip install trfl\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-probability==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.14.6)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.11.0)\n","Requirement already satisfied: trfl in /usr/local/lib/python3.6/dist-packages (1.0)\n","Requirement already satisfied: dm-sonnet in /usr/local/lib/python3.6/dist-packages (from trfl) (1.23)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trfl) (1.14.6)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trfl) (0.7.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trfl) (1.11.0)\n"],"name":"stdout"}]},{"metadata":{"id":"SGop2a_BZCBl","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import trfl\n","import tensorflow_probability as tfp"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KMY4yn1q3sfu","colab_type":"text"},"cell_type":"markdown","source":["** Multi-step Forward View **\n","\n","trfl.td_lambda() calls trfl.generalized_lambda_returns() which calls trfl.multistep_forward_view(). trfl.qlambda() also calls trfl.multistep_forward_view(). You can alter the state_values argument in trfl.multistep_forward_view() to implement Q(λ) or SARSA(λ) and alter the lambda_ argument in trfl.multistep_forward_view to implement Peng’s Q(λ), Watkins’ Q(λ) and Retrace (more on this in Section 5). \n","\n","In this notebook we'll implemnt Watkins’ Q(λ) by setting the eligibility trace to 0 after the first non-greedy action is taken.\n","\n","\n","** Example 1: FrozenLake 4x4 Not Slippery **\n","\n","First example we set is_slippery to False in FrozenLake. Every action the agent takes becomes deterministic, making the env much easier."]},{"metadata":{"id":"ttTuMdfpj74j","colab_type":"code","colab":{}},"cell_type":"code","source":["from gym.envs.registration import register\n","register(\n","    id='FrozenLakeNotSlippery-v0',\n","    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name' : '4x4', 'is_slippery': False}\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uu84Bgs1kISF","colab_type":"code","colab":{}},"cell_type":"code","source":["#hyperparameters\n","episodes = 10000\n","learning_rate = 0.01\n","discount = 0.99\n","lambda_val = 0.8\n","epsilon_start = 1.0\n","epsilon_min = 0.01\n","epsilon_step = (epsilon_start - epsilon_min)/(episodes*.9)\n","\n","env = gym.make('FrozenLakeNotSlippery-v0')\n","num_actions = env.action_space.n\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","state_value_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"state_value\")\n","lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\")\n","                              \n","mfv_return_ = trfl.multistep_forward_view(reward_, discount_, state_value_, lambda_, back_prop=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"58jYfaNgTCG_","colab_type":"text"},"cell_type":"markdown","source":["** TRFL Usage **\n","\n","We input tensors for reward_, discount_, state_value_, and lambda_ into trfl.multistep_forward_view(). The state_value_ tensor_ in this case is the max value from the next state Q value. We run the mfv_return_ tensor in the session and use return minus the current Q value to get the TD error.\n","\n","For Watkins’ Q(λ) we'll feed in a list of lambda_ values to trfl.multistep_forward_view(). The list of lambda_ values will be 0 from the point of the first non-greedy action onwards, as you can see below in the lambda_list.append() code in the action selection section."]},{"metadata":{"id":"A41lzKpAsxp3","colab_type":"code","outputId":"faf29712-51ce-4183-8de5-632c5a003e4f","executionInfo":{"status":"ok","timestamp":1553455869053,"user_tz":240,"elapsed":41857,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":4474}},"cell_type":"code","source":["stats_success = []\n","epsilon = epsilon_start\n","\n","with tf.Session() as sess:\n","  #initialize the estimated state values to zero\n","  action_value_array = np.zeros((16,num_actions))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  state_value_list, q_action_list, reward_list, state_int_list, action_list = [], [], [], [], []\n","  eligibility_cutoff = False\n","  \n","  while current_episode < episodes:\n","    #take epsilon greedy action\n","    if np.random.rand() < epsilon:\n","      action = env.action_space.sample()\n","      #if random action is not max action, cutoff eligibility cutoffs\n","      if action_value_array[current_state, action] != np.max(action_value_array[current_state]):\n","        eligibility_cutoff = True\n","    else:\n","      #Choose a greedy action. If multiple greedy actions randomly choose between\n","      max_actions = np.argwhere(action_value_array[current_state] == np.max(action_value_array[current_state])).reshape((-1))\n","      action = np.random.choice(max_actions)\n","\n","    next_state, rew, done, info = env.step(action)\n","      \n","    reward_list.append(rew)\n","    action_list.append(action)\n","    q_action_list.append(action_value_array[current_state, action])\n","    state_int_list.append(current_state)\n","    arg_action = np.argmax(action_value_array[next_state])\n","    state_value_list = action_value_array[next_state, arg_action]\n","    \n","    current_state = next_state\n","    \n","    #run TRFL tensor\n","    mfv_output = sess.run(mfv_return_, feed_dict={\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(reward_list)).reshape(-1,1),\n","        state_value_:np.array(state_value_list).reshape(-1,1),\n","        lambda_:np.array([lambda_val]*len(reward_list)).reshape(-1,1),\n","      })\n","    #use mfv output and subtract q_value to get td_error\n","    td_error = mfv_output - np.array(q_action_list).reshape(-1,1)\n","\n","    #update action values\n","    #action_value_array[state_int_list, action_list] += learning_rate*td_error\n","    for s, a, td in zip(state_int_list, action_list, td_error.tolist()):\n","      action_value_array[s,a] += learning_rate*td[0]\n","    \n","    # cut off one action past the first exploration\n","    if eligibility_cutoff:\n","      done = 1\n","      \n","    if done:\n","      if next_state == 15:\n","        stats_success.append(1)\n","      else:\n","        stats_success.append(0)\n","        \n","      state_value_list, q_action_list, reward_list, state_int_list, action_list = [], [], [], [], []\n","      eligibility_cutoff = False\n","      current_state = env.reset()\n","      current_episode += 1\n","      #decrease epsilon\n","      epsilon -= epsilon_step\n","      if epsilon < epsilon_min:\n","        epsilon = epsilon_min\n","        \n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","        optimal_action_estimates = np.max(action_value_array,axis=1)\n","        print(\"Optimal Action Value Estimates:\")\n","        print(np.round(optimal_action_estimates.reshape(4,4),2))\n","        print(\"estimate of the optimal state value at each state\")\n","        print(\"\")\n","        print(\"All Action Value Estimates:\")\n","        print(np.round(action_value_array.reshape((16,4)),2))\n","        print(\"\")\n","        "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 1000, 0.89, 0.00\n","Optimal Action Value Estimates:\n","[[0.01 0.02 0.02 0.  ]\n"," [0.   0.   0.03 0.  ]\n"," [0.   0.   0.03 0.  ]\n"," [0.   0.   0.04 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.01 0.   0.01 0.01]\n"," [0.   0.   0.02 0.  ]\n"," [0.   0.02 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.03 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.03 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.04 0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 2000, 0.78, 0.00\n","Optimal Action Value Estimates:\n","[[0.03 0.03 0.03 0.  ]\n"," [0.   0.   0.04 0.  ]\n"," [0.   0.   0.05 0.  ]\n"," [0.   0.   0.07 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.02 0.   0.03 0.02]\n"," [0.01 0.   0.03 0.02]\n"," [0.01 0.03 0.   0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.04 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.05 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.07 0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 3000, 0.67, 0.01\n","Optimal Action Value Estimates:\n","[[0.07 0.08 0.09 0.  ]\n"," [0.   0.   0.11 0.  ]\n"," [0.   0.   0.13 0.  ]\n"," [0.   0.   0.17 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.05 0.   0.07 0.05]\n"," [0.04 0.   0.08 0.04]\n"," [0.02 0.09 0.   0.03]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.11 0.   0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.13 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.17 0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 4000, 0.56, 0.03\n","Optimal Action Value Estimates:\n","[[0.21 0.22 0.23 0.  ]\n"," [0.   0.   0.27 0.  ]\n"," [0.   0.   0.33 0.  ]\n"," [0.   0.   0.4  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.12 0.   0.21 0.14]\n"," [0.09 0.   0.22 0.1 ]\n"," [0.06 0.23 0.   0.08]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.27 0.   0.03]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.33 0.   0.02]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.02 0.4  0.02]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 5000, 0.45, 0.06\n","Optimal Action Value Estimates:\n","[[0.38 0.4  0.42 0.  ]\n"," [0.   0.   0.47 0.  ]\n"," [0.   0.   0.56 0.  ]\n"," [0.   0.   0.67 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.25 0.   0.38 0.25]\n"," [0.19 0.   0.4  0.23]\n"," [0.17 0.42 0.   0.18]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.47 0.   0.1 ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.56 0.   0.09]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.06 0.67 0.06]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 6000, 0.34, 0.12\n","Optimal Action Value Estimates:\n","[[0.55 0.59 0.62 0.  ]\n"," [0.   0.   0.66 0.  ]\n"," [0.   0.   0.77 0.  ]\n"," [0.   0.   0.9  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.41 0.   0.55 0.39]\n"," [0.34 0.   0.59 0.35]\n"," [0.31 0.62 0.   0.32]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.66 0.   0.2 ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.77 0.   0.15]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.19 0.9  0.13]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 7000, 0.23, 0.23\n","Optimal Action Value Estimates:\n","[[0.66 0.69 0.72 0.  ]\n"," [0.   0.   0.77 0.  ]\n"," [0.   0.   0.86 0.  ]\n"," [0.   0.   0.99 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.53 0.   0.66 0.54]\n"," [0.47 0.   0.69 0.49]\n"," [0.45 0.72 0.   0.45]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.77 0.   0.35]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.86 0.   0.27]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.36 0.99 0.24]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 8000, 0.12, 0.47\n","Optimal Action Value Estimates:\n","[[0.72 0.75 0.78 0.  ]\n"," [0.   0.   0.82 0.  ]\n"," [0.   0.   0.89 0.  ]\n"," [0.   0.   1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.59 0.   0.72 0.6 ]\n"," [0.54 0.   0.75 0.56]\n"," [0.53 0.78 0.   0.52]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.82 0.   0.44]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.89 0.   0.38]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.47 1.   0.37]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 9000, 0.01, 0.75\n","Optimal Action Value Estimates:\n","[[0.75 0.77 0.79 0.  ]\n"," [0.   0.   0.83 0.  ]\n"," [0.   0.   0.89 0.  ]\n"," [0.   0.   1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.61 0.   0.75 0.61]\n"," [0.57 0.   0.77 0.59]\n"," [0.56 0.79 0.   0.56]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.83 0.   0.47]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.89 0.   0.44]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.53 1.   0.44]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.96\n","Optimal Action Value Estimates:\n","[[0.74 0.76 0.78 0.  ]\n"," [0.   0.   0.82 0.  ]\n"," [0.   0.   0.89 0.  ]\n"," [0.   0.   1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.62 0.   0.74 0.61]\n"," [0.57 0.   0.76 0.59]\n"," [0.56 0.78 0.   0.57]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.82 0.   0.47]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.89 0.   0.45]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.55 1.   0.46]\n"," [0.   0.   0.   0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"OKktGDk_UIvd","colab_type":"code","outputId":"963693a6-3e54-48e5-814e-0b0c2f0d3874","executionInfo":{"status":"ok","timestamp":1553455869058,"user_tz":240,"elapsed":20868,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"cell_type":"code","source":["print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","optimal_action_estimates = np.max(action_value_array,axis=1)\n","print(\"Optimal Action Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(4,4),2))\n","print(\"estimate of the optimal state value at each state\")\n","print(\"\")\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_value_array.reshape((16,4)),2))\n","print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.96\n","Optimal Action Value Estimates:\n","[[0.74 0.76 0.78 0.  ]\n"," [0.   0.   0.82 0.  ]\n"," [0.   0.   0.89 0.  ]\n"," [0.   0.   1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.62 0.   0.74 0.61]\n"," [0.57 0.   0.76 0.59]\n"," [0.56 0.78 0.   0.57]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.82 0.   0.47]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.89 0.   0.45]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.55 1.   0.46]\n"," [0.   0.   0.   0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"5z58TlVUXyoP","colab_type":"text"},"cell_type":"markdown","source":["** Example 2: FrozenLake 4x4 Slippery **\n","\n","Standard FrozenLake env where slippery is enabled. Notice the increased failure rate and lower Q values"]},{"metadata":{"id":"FbqZr9dAUOhm","colab_type":"code","colab":{}},"cell_type":"code","source":["#hyperparameters\n","episodes = 10000\n","learning_rate = 0.01\n","discount = 0.99\n","lambda_val = 0.5\n","epsilon_start = 1.0\n","epsilon_min = 0.01\n","epsilon_step = (epsilon_start - epsilon_min)/(episodes*.9)\n","\n","env = gym.make('FrozenLake-v0')\n","num_actions = env.action_space.n\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","state_value_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"state_value\")\n","lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\")\n","                              \n","mfv_return_ = trfl.multistep_forward_view(reward_, discount_, state_value_, lambda_, back_prop=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"017M07L0-eNx","colab_type":"code","outputId":"af3d5d2d-3158-48fc-d49f-7282dab48417","executionInfo":{"status":"ok","timestamp":1553456012663,"user_tz":240,"elapsed":57760,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":4474}},"cell_type":"code","source":["stats_success = []\n","epsilon = epsilon_start\n","\n","with tf.Session() as sess:\n","  #initialize the estimated state values to zero\n","  action_value_array = np.zeros((16,num_actions))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  state_value_list, q_action_list, reward_list, state_int_list, action_list = [], [], [], [], []\n","  eligibility_cutoff = False\n","  \n","  while current_episode < episodes:\n","    #take epsilon greedy action\n","    if np.random.rand() < epsilon:\n","      action = env.action_space.sample()\n","      #if random action is not max action, cutoff eligibility cutoffs\n","      if action_value_array[current_state, action] != np.max(action_value_array[current_state]):\n","        eligibility_cutoff = True\n","    else:\n","      #Choose a greedy action. If multiple greedy actions randomly choose between\n","      max_actions = np.argwhere(action_value_array[current_state] == np.max(action_value_array[current_state])).reshape((-1))\n","      action = np.random.choice(max_actions)\n","\n","    next_state, rew, done, info = env.step(action)\n","      \n","    reward_list.append(rew)\n","    action_list.append(action)\n","    q_action_list.append(action_value_array[current_state, action])\n","    state_int_list.append(current_state)\n","    arg_action = np.argmax(action_value_array[next_state])\n","    state_value_list = action_value_array[next_state, arg_action]\n","    \n","    current_state = next_state\n","    \n","    \n","    mfv_output = sess.run(mfv_return_, feed_dict={\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(reward_list)).reshape(-1,1),\n","        state_value_:np.array(state_value_list).reshape(-1,1),\n","        lambda_:np.array([lambda_val]*len(reward_list)).reshape(-1,1),\n","      })\n","    td_error = mfv_output - np.array(q_action_list).reshape(-1,1)\n","\n","    #action_value_array[state_int_list, action_list] += learning_rate*td_error\n","    for s, a, td in zip(state_int_list, action_list, td_error.tolist()):\n","      action_value_array[s,a] += learning_rate*td[0]\n","    \n","    # cut off one action past the first exploration\n","    if eligibility_cutoff:\n","      done = 1\n","      \n","    if done:\n","      if next_state == 15:\n","        stats_success.append(1)\n","      else:\n","        stats_success.append(0)\n","        \n","      state_value_list, q_action_list, reward_list, state_int_list, action_list = [], [], [], [], []\n","      eligibility_cutoff = False\n","      current_state = env.reset()\n","      current_episode += 1\n","      epsilon -= epsilon_step\n","      if epsilon < epsilon_min:\n","        epsilon = epsilon_min\n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","        optimal_action_estimates = np.max(action_value_array,axis=1)\n","        print(\"Optimal Action Value Estimates:\")\n","        print(np.round(optimal_action_estimates.reshape(4,4),2))\n","        print(\"estimate of the optimal state value at each state\")\n","        print(\"\")\n","        print(\"All Action Value Estimates:\")\n","        print(np.round(action_value_array.reshape((16,4)),2))\n","        print(\"\")     "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 1000, 0.89, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 2000, 0.78, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 3000, 0.67, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 4000, 0.56, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 5000, 0.45, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.01 0.03 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.01]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.01]\n"," [0.   0.03 0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 6000, 0.34, 0.01\n","Optimal Action Value Estimates:\n","[[0.01 0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.01 0.02 0.01 0.  ]\n"," [0.   0.03 0.05 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.01 0.01 0.01 0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.01]\n"," [0.   0.02 0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.03]\n"," [0.   0.05 0.   0.02]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 7000, 0.23, 0.01\n","Optimal Action Value Estimates:\n","[[0.02 0.   0.   0.  ]\n"," [0.02 0.   0.   0.  ]\n"," [0.02 0.03 0.03 0.  ]\n"," [0.   0.03 0.09 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.02 0.01 0.01 0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.02 0.01 0.01 0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.01 0.02]\n"," [0.   0.03 0.   0.  ]\n"," [0.   0.03 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.03]\n"," [0.   0.09 0.   0.03]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 8000, 0.12, 0.02\n","Optimal Action Value Estimates:\n","[[0.04 0.04 0.01 0.  ]\n"," [0.04 0.   0.01 0.  ]\n"," [0.05 0.05 0.07 0.  ]\n"," [0.   0.06 0.16 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.01 0.04 0.02 0.03]\n"," [0.   0.   0.   0.04]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.04 0.02 0.02 0.02]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.01 0.02 0.05]\n"," [0.01 0.05 0.01 0.01]\n"," [0.   0.07 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.06]\n"," [0.01 0.16 0.   0.05]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 9000, 0.01, 0.08\n","Optimal Action Value Estimates:\n","[[0.07 0.08 0.08 0.  ]\n"," [0.07 0.   0.08 0.  ]\n"," [0.09 0.1  0.11 0.  ]\n"," [0.   0.11 0.22 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.03 -0.    0.07  0.03]\n"," [ 0.01  0.01  0.01  0.08]\n"," [ 0.08  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.02  0.07  0.03  0.03]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.08  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.09  0.02  0.01]\n"," [ 0.01  0.1   0.01  0.01]\n"," [ 0.01  0.11  0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.01  0.01  0.11]\n"," [ 0.01  0.22  0.01  0.07]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.07\n","Optimal Action Value Estimates:\n","[[0.08 0.08 0.08 0.  ]\n"," [0.08 0.   0.1  0.  ]\n"," [0.11 0.11 0.13 0.  ]\n"," [0.   0.13 0.21 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.04  0.    0.08  0.03]\n"," [ 0.01  0.01  0.01  0.08]\n"," [ 0.08  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01  0.08  0.03  0.03]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.1   0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.11  0.02  0.02]\n"," [ 0.01  0.11  0.01  0.01]\n"," [ 0.01  0.13  0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.01  0.01  0.13]\n"," [ 0.01  0.21  0.01  0.07]\n"," [ 0.    0.    0.    0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"gHSP5ubAVAqt","colab_type":"code","outputId":"41d94e09-bcb8-4174-bb22-6e47707e2fa5","executionInfo":{"status":"ok","timestamp":1553456029931,"user_tz":240,"elapsed":386,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"cell_type":"code","source":["print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","optimal_action_estimates = np.max(action_value_array,axis=1)\n","print(\"Optimal Action Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(4,4),2))\n","print(\"estimate of the optimal state value at each state\")\n","print(\"\")\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_value_array.reshape((16,4)),2))\n","print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.07\n","Optimal Action Value Estimates:\n","[[0.08 0.08 0.08 0.  ]\n"," [0.08 0.   0.1  0.  ]\n"," [0.11 0.11 0.13 0.  ]\n"," [0.   0.13 0.21 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.04  0.    0.08  0.03]\n"," [ 0.01  0.01  0.01  0.08]\n"," [ 0.08  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.01  0.08  0.03  0.03]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.1   0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.11  0.02  0.02]\n"," [ 0.01  0.11  0.01  0.01]\n"," [ 0.01  0.13  0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.01  0.01  0.13]\n"," [ 0.01  0.21  0.01  0.07]\n"," [ 0.    0.    0.    0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"Z2_A-Q6ea5W9","colab_type":"text"},"cell_type":"markdown","source":["** Example 3: FrozenLake 8x8 **\n","\n","FrozenLake on an 8x8 grid. Much harder to randomly find the goal. To make learning faster, we add a penalty for falling into a hole."]},{"metadata":{"id":"eQG7BVhNa4LY","colab_type":"code","colab":{}},"cell_type":"code","source":["#hyperparameters\n","episodes = 20000\n","learning_rate = 0.01\n","discount = 0.99\n","lambda_val = 0.95\n","epsilon_start = 1.0\n","epsilon_min = 0.01\n","epsilon_step = (epsilon_start - epsilon_min)/(episodes*.9)\n","hole_penalty = -0.1 #penalty for falling into a hole\n","\n","seed = 31\n","env = gym.make('FrozenLake8x8-v0')\n","env.seed(seed)\n","np.random.seed(seed)\n","num_actions = env.action_space.n\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","state_value_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"state_value\")\n","lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\")\n","                              \n","mfv_return_ = trfl.multistep_forward_view(reward_, discount_, state_value_, lambda_, back_prop=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Krd5YoIzeUUz","colab_type":"code","outputId":"19c50411-e0d1-4016-bdb6-93944ae72a3a","executionInfo":{"status":"ok","timestamp":1553456548968,"user_tz":240,"elapsed":218272,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":26760}},"cell_type":"code","source":["stats_success = []\n","epsilon = epsilon_start\n","\n","with tf.Session() as sess:\n","  #initialize the estimated state values to zero\n","  action_value_array = np.zeros((64,num_actions))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  state_value_list, q_action_list, reward_list, state_int_list, action_list = [], [], [], [], []\n","  eligibility_cutoff = False\n","  \n","  while current_episode < episodes:\n","    #take epsilon greedy action\n","    if np.random.rand() < epsilon:\n","      action = env.action_space.sample()\n","      #if random action is not max action, cutoff eligibility cutoffs\n","      if action_value_array[current_state, action] != np.max(action_value_array[current_state]):\n","        eligibility_cutoff = True\n","    else:\n","      #Choose a greedy action. If multiple greedy actions randomly choose between\n","      max_actions = np.argwhere(action_value_array[current_state] == np.max(action_value_array[current_state])).reshape((-1))\n","      action = np.random.choice(max_actions)\n","\n","    next_state, rew, done, info = env.step(action)\n","    \n","#     if done and rew < 1:\n","#       rew = hole_penalty\n","      \n","    reward_list.append(rew)\n","    action_list.append(action)\n","    q_action_list.append(action_value_array[current_state, action])\n","    state_int_list.append(current_state)\n","    arg_action = np.argmax(action_value_array[next_state])\n","    state_value_list = action_value_array[next_state, arg_action]\n","    \n","    current_state = next_state\n","    \n","    \n","    mfv_output = sess.run(mfv_return_, feed_dict={\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(reward_list)).reshape(-1,1),\n","        state_value_:np.array(state_value_list).reshape(-1,1),\n","        lambda_:np.array([lambda_val]*len(reward_list)).reshape(-1,1),\n","      })\n","    td_error = mfv_output - np.array(q_action_list).reshape(-1,1)\n","\n","    #action_value_array[state_int_list, action_list] += learning_rate*td_error\n","    for s, a, td in zip(state_int_list, action_list, td_error.tolist()):\n","      action_value_array[s,a] += learning_rate*td[0]\n","    \n","    # cut off one action past the first exploration\n","    if eligibility_cutoff:\n","      done = 1\n","      \n","    if done:\n","      if next_state == 63:\n","        stats_success.append(1)\n","      else:\n","        stats_success.append(0)\n","        \n","      state_value_list, q_action_list, reward_list, state_int_list, action_list = [], [], [], [], []\n","      eligibility_cutoff = False\n","      current_state = env.reset()\n","      current_episode += 1\n","      epsilon -= epsilon_step\n","      if epsilon < epsilon_min:\n","        epsilon = epsilon_min\n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","        optimal_action_estimates = np.max(action_value_array,axis=1)\n","        print(\"Optimal Action Value Estimates:\")\n","        print(np.round(optimal_action_estimates.reshape(8,8),2))\n","        print(\"estimate of the optimal state value at each state\")\n","        print(\"\")\n","        print(\"All Action Value Estimates:\")\n","        print(np.round(action_value_array.reshape((64,4)),2))\n","        print(\"\")  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 1000, 0.95, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 2000, 0.89, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 3000, 0.84, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 4000, 0.78, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 5000, 0.73, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 6000, 0.67, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 7000, 0.62, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 8000, 0.56, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 9000, 0.51, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 10000, 0.45, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 11000, 0.40, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 12000, 0.34, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.01 0.01 0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 13000, 0.29, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.01 0.01 0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 14000, 0.23, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.01 0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.    0.    0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.01  0.  ]\n"," [ 0.01  0.    0.01  0.  ]\n"," [ 0.01  0.01  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.01  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 15000, 0.18, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.01 0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.01 0.   0.01 0.  ]\n"," [0.01 0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 16000, 0.12, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.01 0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.01 0.01 0.01]\n"," [0.   0.   0.   0.   0.01 0.01 0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.02]\n"," [0.   0.   0.   0.   0.   0.   0.   0.02]\n"," [0.   0.   0.   0.   0.   0.   0.   0.02]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.01  0.  ]\n"," [ 0.01  0.    0.01  0.  ]\n"," [ 0.01  0.01  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.    0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.    0.    0.01  0.  ]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.01  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 17000, 0.07, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.02]\n"," [0.   0.   0.   0.   0.   0.   0.   0.02]\n"," [0.   0.   0.   0.   0.   0.   0.   0.02]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.    0.    0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.   -0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.01  0.  ]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.01  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 18000, 0.01, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.   0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.   0.01]\n"," [0.   0.   0.   0.   0.   0.   0.01 0.01]\n"," [0.   0.   0.   0.   0.   0.   0.   0.02]\n"," [0.   0.   0.   0.   0.   0.   0.   0.02]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.    0.   -0.   -0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [ 0.   -0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.   -0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.   -0.    0.    0.  ]\n"," [ 0.    0.    0.   -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.01  0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.02  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 19000, 0.01, 0.01\n","Optimal Action Value Estimates:\n","[[0.   0.01 0.01 0.01 0.01 0.02 0.03 0.03]\n"," [0.   0.01 0.01 0.01 0.01 0.01 0.03 0.03]\n"," [0.   0.01 0.01 0.   0.02 0.03 0.03 0.03]\n"," [0.   0.01 0.01 0.01 0.01 0.   0.02 0.04]\n"," [0.   0.01 0.   0.   0.01 0.01 0.03 0.05]\n"," [0.   0.   0.   0.   0.   0.   0.   0.08]\n"," [0.   0.   0.   0.   0.   0.   0.   0.06]\n"," [0.   0.   0.   0.   0.   0.   0.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.01 -0.02  0.   -0.02]\n"," [-0.02  0.01 -0.04 -0.03]\n"," [-0.01 -0.01  0.01 -0.04]\n"," [ 0.01 -0.01 -0.05 -0.03]\n"," [ 0.01  0.    0.   -0.02]\n"," [ 0.    0.    0.    0.02]\n"," [ 0.    0.    0.03  0.  ]\n"," [ 0.    0.03 -0.   -0.  ]\n"," [-0.01 -0.01  0.   -0.  ]\n"," [-0.    0.01 -0.02 -0.  ]\n"," [-0.    0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.   -0.    0.01  0.  ]\n"," [-0.    0.    0.    0.01]\n"," [ 0.    0.03  0.    0.  ]\n"," [-0.    0.03  0.   -0.01]\n"," [ 0.   -0.01  0.    0.  ]\n"," [-0.    0.01  0.   -0.01]\n"," [ 0.01  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.    0.  ]\n"," [ 0.    0.    0.    0.03]\n"," [ 0.    0.    0.03  0.  ]\n"," [ 0.    0.    0.03 -0.05]\n"," [ 0.   -0.   -0.    0.  ]\n"," [ 0.    0.    0.01  0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.01  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.    0.  ]\n"," [ 0.04  0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [ 0.    0.    0.01  0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.01  0.  ]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.    0.    0.03  0.  ]\n"," [ 0.    0.05  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.08  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.06  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 20000, 0.01, 0.00\n","Optimal Action Value Estimates:\n","[[-0.    0.    0.    0.01  0.01  0.02  0.02  0.03]\n"," [-0.    0.   -0.    0.01  0.01  0.01  0.02  0.03]\n"," [ 0.    0.    0.    0.    0.02  0.02  0.03  0.03]\n"," [ 0.    0.    0.    0.01  0.01  0.    0.02  0.04]\n"," [ 0.    0.    0.    0.    0.02  0.02  0.03  0.05]\n"," [ 0.    0.    0.    0.    0.01  0.01  0.    0.06]\n"," [ 0.    0.    0.    0.    0.    0.    0.    0.06]\n"," [ 0.    0.    0.    0.    0.    0.    0.    0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.05 -0.   -0.02 -0.06]\n"," [ 0.   -0.02 -0.03 -0.03]\n"," [-0.   -0.01  0.   -0.04]\n"," [ 0.01 -0.01 -0.05 -0.03]\n"," [ 0.01  0.    0.   -0.02]\n"," [ 0.    0.    0.    0.02]\n"," [ 0.    0.    0.02  0.  ]\n"," [ 0.    0.03 -0.   -0.  ]\n"," [-0.02 -0.   -0.03 -0.02]\n"," [ 0.   -0.01 -0.02 -0.01]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.   -0.    0.01  0.  ]\n"," [-0.    0.    0.    0.01]\n"," [ 0.    0.02  0.    0.  ]\n"," [-0.    0.03  0.   -0.01]\n"," [-0.01  0.   -0.01 -0.01]\n"," [ 0.   -0.   -0.01 -0.01]\n"," [-0.01  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.    0.  ]\n"," [ 0.    0.    0.    0.02]\n"," [ 0.    0.    0.03  0.  ]\n"," [ 0.    0.    0.03 -0.05]\n"," [ 0.   -0.    0.   -0.  ]\n"," [ 0.   -0.   -0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.01  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.    0.  ]\n"," [ 0.04  0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [ 0.    0.   -0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.02  0.  ]\n"," [ 0.    0.    0.    0.02]\n"," [ 0.    0.    0.03  0.  ]\n"," [ 0.    0.05  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.06  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.06  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"IAqLLdIjZs6y","colab_type":"code","outputId":"1aa2d92d-c0ef-40a6-f883-90d098387acb","executionInfo":{"status":"ok","timestamp":1553456758867,"user_tz":240,"elapsed":357,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":1354}},"cell_type":"code","source":["print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                np.mean(stats_success[-1000:])))\n","optimal_action_estimates = np.max(action_value_array,axis=1)\n","print(\"Optimal Action Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(8,8),2))\n","print(\"estimate of the optimal state value at each state\")\n","print(\"\")\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_value_array.reshape((64,4)),2))\n","print(\"\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 20000, 0.01, 0.00\n","Optimal Action Value Estimates:\n","[[-0.    0.    0.    0.01  0.01  0.02  0.02  0.03]\n"," [-0.    0.   -0.    0.01  0.01  0.01  0.02  0.03]\n"," [ 0.    0.    0.    0.    0.02  0.02  0.03  0.03]\n"," [ 0.    0.    0.    0.01  0.01  0.    0.02  0.04]\n"," [ 0.    0.    0.    0.    0.02  0.02  0.03  0.05]\n"," [ 0.    0.    0.    0.    0.01  0.01  0.    0.06]\n"," [ 0.    0.    0.    0.    0.    0.    0.    0.06]\n"," [ 0.    0.    0.    0.    0.    0.    0.    0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.05 -0.   -0.02 -0.06]\n"," [ 0.   -0.02 -0.03 -0.03]\n"," [-0.   -0.01  0.   -0.04]\n"," [ 0.01 -0.01 -0.05 -0.03]\n"," [ 0.01  0.    0.   -0.02]\n"," [ 0.    0.    0.    0.02]\n"," [ 0.    0.    0.02  0.  ]\n"," [ 0.    0.03 -0.   -0.  ]\n"," [-0.02 -0.   -0.03 -0.02]\n"," [ 0.   -0.01 -0.02 -0.01]\n"," [-0.   -0.01 -0.   -0.  ]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.   -0.    0.01  0.  ]\n"," [-0.    0.    0.    0.01]\n"," [ 0.    0.02  0.    0.  ]\n"," [-0.    0.03  0.   -0.01]\n"," [-0.01  0.   -0.01 -0.01]\n"," [ 0.   -0.   -0.01 -0.01]\n"," [-0.01  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.    0.  ]\n"," [ 0.    0.    0.    0.02]\n"," [ 0.    0.    0.03  0.  ]\n"," [ 0.    0.    0.03 -0.05]\n"," [ 0.   -0.    0.   -0.  ]\n"," [ 0.   -0.   -0.01 -0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.01  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.    0.  ]\n"," [ 0.04  0.    0.    0.  ]\n"," [-0.    0.    0.    0.  ]\n"," [ 0.    0.   -0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.02  0.  ]\n"," [ 0.    0.    0.    0.02]\n"," [ 0.    0.    0.03  0.  ]\n"," [ 0.    0.05  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.06  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.06  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"ML0Jw8C0hVEH","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}