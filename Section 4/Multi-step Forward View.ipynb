{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Multi-step Forward View.ipynb","version":"0.3.2","provenance":[{"file_id":"1VSxFkfFGUF-xijRu3nDa39WyEcjwpz1-","timestamp":1553437836799},{"file_id":"15y3NrJjPTRhzYm0OWfoc813cR9BtCTGl","timestamp":1553044789533},{"file_id":"1BmiEkGoqe_CAaZmggviivF7Ygbd1iPe4","timestamp":1552945822274},{"file_id":"110YMHk2yHqxguCj1yaJaXVmkRUadX9GV","timestamp":1552791755615},{"file_id":"1cMFan2NCLOZ8w_xKSEyXTF09paJ0F930","timestamp":1552618439794},{"file_id":"1SToTDuBpTdV2UVRN9bapskLUesVhBH1J","timestamp":1551831097681},{"file_id":"1ssliB1HogX4KFHRyNKLU2aeiAV865kLv","timestamp":1551578593280},{"file_id":"1N74NgQBdDCDRER81p_VdJA_lsZOR2Hjo","timestamp":1550336742833}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"npzJ574a6A94","colab_type":"text"},"cell_type":"markdown","source":["**Reinforcement Learning with TensorFlow & TRFL: Multi-step Forward View**\n","\n","Outline:\n","* Watkins Q(λ) with trfl.multistep_forward_view()\n","\n"]},{"metadata":{"id":"RyxlWytnVqJI","colab_type":"code","outputId":"e7962831-9190-473f-b8f5-db812f5da5f4","colab":{"base_uri":"https://localhost:8080/","height":328}},"cell_type":"code","source":["#TRFL works with TensorFlow 1.12\n","#installs TensorFlow version 1.12 then restarts the runtime\n","!pip install tensorflow==1.12\n","\n","import os\n","os.kill(os.getpid(), 9)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==1.12 in /usr/local/lib/python3.6/dist-packages (1.12.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.33.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.7)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.9)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.15.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (3.7.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.11.0)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.12.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.16.2)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.2.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (40.9.0)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (0.15.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.1)\n"],"name":"stdout"}]},{"metadata":{"id":"XRS56AQDVybG","colab_type":"code","outputId":"a6317525-bb7b-4fdf-84d7-ef69bef1a6c9","executionInfo":{"status":"ok","timestamp":1555717616951,"user_tz":240,"elapsed":13858,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":174}},"cell_type":"code","source":["#install tensorflow-probability 0.5.0 that works with TensorFlow 1.12\n","!pip install tensorflow-probability==0.5.0\n","\n","#install TRFL\n","!pip install trfl==1.0\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-probability==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.16.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.11.0)\n","Requirement already satisfied: trfl==1.0 in /usr/local/lib/python3.6/dist-packages (1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.16.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.11.0)\n","Requirement already satisfied: dm-sonnet in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.23)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (0.7.1)\n"],"name":"stdout"}]},{"metadata":{"id":"SGop2a_BZCBl","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import trfl\n","import tensorflow_probability as tfp"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KMY4yn1q3sfu","colab_type":"text"},"cell_type":"markdown","source":["** Multi-step Forward View **\n","\n","trfl.td_lambda() calls trfl.generalized_lambda_returns() which calls trfl.multistep_forward_view(). trfl.qlambda() also calls trfl.multistep_forward_view(). You can alter the state_values argument in trfl.multistep_forward_view() to implement Q(λ) or SARSA(λ) and alter the lambda_ argument in trfl.multistep_forward_view to implement Peng’s Q(λ), Watkins’ Q(λ) and Retrace (more on this in Section 5). \n","\n","In this notebook we'll implemnt Watkins’ Q(λ) by setting the eligibility trace to 0 after the first non-greedy action is taken.\n","\n","\n","** Example 1: FrozenLake 4x4 Not Slippery **\n","\n","First example we set is_slippery to False in FrozenLake. Every action the agent takes becomes deterministic, making the env much easier."]},{"metadata":{"id":"ttTuMdfpj74j","colab_type":"code","colab":{}},"cell_type":"code","source":["from gym.envs.registration import register\n","register(\n","    id='FrozenLakeNotSlippery-v0',\n","    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name' : '4x4', 'is_slippery': False}\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uu84Bgs1kISF","colab_type":"code","colab":{}},"cell_type":"code","source":["#hyperparameters\n","episodes = 10000\n","learning_rate = 0.01\n","discount = 0.99\n","lambda_val = 0.8\n","epsilon_start = 1.0\n","epsilon_min = 0.01\n","epsilon_step = (epsilon_start - epsilon_min)/(episodes*.9)\n","\n","env = gym.make('FrozenLakeNotSlippery-v0')\n","num_actions = env.action_space.n\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","state_value_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"state_value\")\n","lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\")\n","                              \n","mfv_return_ = trfl.multistep_forward_view(reward_, discount_, state_value_, lambda_, back_prop=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"58jYfaNgTCG_","colab_type":"text"},"cell_type":"markdown","source":["** TRFL Usage **\n","\n","We input tensors for reward_, discount_, state_value_, and lambda_ into trfl.multistep_forward_view(). The state_value_ tensor_ in this case is the max value from the next state Q value. We run the mfv_return_ tensor in the session and use return minus the current Q value to get the TD error.\n","\n","For Watkins’ Q(λ) we'll feed in a list of lambda_ values to trfl.multistep_forward_view(). The list of lambda_ values will be 0 from the point of the first non-greedy action onwards, as you can see below in the lambda_list.append() code in the action selection section."]},{"metadata":{"id":"A41lzKpAsxp3","colab_type":"code","outputId":"6919de11-3583-452b-c47a-32024b9356d3","executionInfo":{"status":"ok","timestamp":1555717692187,"user_tz":240,"elapsed":84460,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":4474}},"cell_type":"code","source":["stats_success = []\n","epsilon = epsilon_start\n","\n","with tf.Session() as sess:\n","  #initialize the estimated state values to zero\n","  action_value_array = np.zeros((16,num_actions))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  state_value_list, q_action_list, reward_list, state_int_list, action_list = [], [], [], [], []\n","  eligibility_cutoff = False\n","  \n","  while current_episode < episodes:\n","    #take epsilon greedy action\n","    if np.random.rand() < epsilon:\n","      action = env.action_space.sample()\n","      #if random action is not max action, cutoff eligibility cutoffs\n","      if action_value_array[current_state, action] != np.max(action_value_array[current_state]):\n","        eligibility_cutoff = True\n","    else:\n","      #Choose a greedy action. If multiple greedy actions randomly choose between\n","      max_actions = np.argwhere(action_value_array[current_state] == np.max(action_value_array[current_state])).reshape((-1))\n","      action = np.random.choice(max_actions)\n","\n","    next_state, rew, done, info = env.step(action)\n","      \n","    reward_list.append(rew)\n","    action_list.append(action)\n","    q_action_list.append(action_value_array[current_state, action])\n","    state_int_list.append(current_state)\n","    arg_action = np.argmax(action_value_array[next_state])\n","    state_value_list = action_value_array[next_state, arg_action]\n","    \n","    current_state = next_state\n","    \n","    #run TRFL tensor\n","    mfv_output = sess.run(mfv_return_, feed_dict={\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(reward_list)).reshape(-1,1),\n","        state_value_:np.array(state_value_list).reshape(-1,1),\n","        lambda_:np.array([lambda_val]*len(reward_list)).reshape(-1,1),\n","      })\n","    #use mfv output and subtract q_value to get td_error\n","    td_error = mfv_output - np.array(q_action_list).reshape(-1,1)\n","\n","    #update action values\n","    #action_value_array[state_int_list, action_list] += learning_rate*td_error\n","    for s, a, td in zip(state_int_list, action_list, td_error.tolist()):\n","      action_value_array[s,a] += learning_rate*td[0]\n","    \n","    # cut off one action past the first exploration\n","    if eligibility_cutoff:\n","      done = 1\n","      \n","    if done:\n","      if next_state == 15:\n","        stats_success.append(1)\n","      else:\n","        stats_success.append(0)\n","        \n","      state_value_list, q_action_list, reward_list, state_int_list, action_list = [], [], [], [], []\n","      eligibility_cutoff = False\n","      current_state = env.reset()\n","      current_episode += 1\n","      #decrease epsilon\n","      epsilon -= epsilon_step\n","      if epsilon < epsilon_min:\n","        epsilon = epsilon_min\n","        \n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","        optimal_action_estimates = np.max(action_value_array,axis=1)\n","        print(\"Optimal Action Value Estimates:\")\n","        print(np.round(optimal_action_estimates.reshape(4,4),2))\n","        print(\"estimate of the optimal state value at each state\")\n","        print(\"\")\n","        print(\"All Action Value Estimates:\")\n","        print(np.round(action_value_array.reshape((16,4)),2))\n","        print(\"\")\n","        "],"execution_count":5,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 1000, 0.89, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.01 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 2000, 0.78, 0.01\n","Optimal Action Value Estimates:\n","[[0.03 0.03 0.04 0.  ]\n"," [0.   0.   0.04 0.  ]\n"," [0.   0.   0.05 0.  ]\n"," [0.   0.   0.07 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.01 0.   0.03 0.01]\n"," [0.   0.   0.03 0.01]\n"," [0.   0.04 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.04 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.05 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.07 0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 3000, 0.67, 0.01\n","Optimal Action Value Estimates:\n","[[0.07 0.07 0.08 0.  ]\n"," [0.   0.   0.1  0.  ]\n"," [0.   0.   0.12 0.  ]\n"," [0.   0.   0.15 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.05 0.   0.07 0.05]\n"," [0.03 0.   0.07 0.03]\n"," [0.02 0.08 0.   0.02]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.1  0.   0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.12 0.   0.01]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.15 0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 4000, 0.56, 0.02\n","Optimal Action Value Estimates:\n","[[0.15 0.16 0.18 0.  ]\n"," [0.   0.   0.21 0.  ]\n"," [0.   0.   0.25 0.  ]\n"," [0.   0.   0.31 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.1  0.   0.15 0.1 ]\n"," [0.07 0.   0.16 0.07]\n"," [0.05 0.18 0.   0.06]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.21 0.   0.04]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.25 0.   0.02]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.31 0.02]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 5000, 0.45, 0.06\n","Optimal Action Value Estimates:\n","[[0.33 0.35 0.37 0.  ]\n"," [0.   0.   0.44 0.  ]\n"," [0.   0.   0.51 0.  ]\n"," [0.   0.   0.61 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.23 0.   0.33 0.22]\n"," [0.19 0.   0.35 0.18]\n"," [0.14 0.37 0.   0.15]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.44 0.   0.1 ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.51 0.   0.07]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.06 0.61 0.05]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 6000, 0.34, 0.13\n","Optimal Action Value Estimates:\n","[[0.58 0.6  0.63 0.  ]\n"," [0.   0.   0.69 0.  ]\n"," [0.   0.   0.78 0.  ]\n"," [0.   0.   0.9  0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.39 0.   0.58 0.38]\n"," [0.31 0.   0.6  0.35]\n"," [0.29 0.63 0.   0.28]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.69 0.   0.22]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.78 0.   0.17]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.16 0.9  0.14]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 7000, 0.23, 0.21\n","Optimal Action Value Estimates:\n","[[0.7  0.71 0.74 0.  ]\n"," [0.   0.   0.78 0.  ]\n"," [0.   0.   0.89 0.  ]\n"," [0.   0.   0.99 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.52 0.   0.7  0.51]\n"," [0.45 0.   0.71 0.48]\n"," [0.42 0.74 0.   0.44]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.78 0.   0.32]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.89 0.   0.34]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.43 0.99 0.31]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 8000, 0.12, 0.43\n","Optimal Action Value Estimates:\n","[[0.72 0.73 0.76 0.  ]\n"," [0.   0.   0.8  0.  ]\n"," [0.   0.   0.87 0.  ]\n"," [0.   0.   1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.58 0.   0.72 0.58]\n"," [0.53 0.   0.73 0.56]\n"," [0.51 0.76 0.   0.52]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.8  0.   0.4 ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.87 0.   0.44]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.51 1.   0.41]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 9000, 0.01, 0.74\n","Optimal Action Value Estimates:\n","[[0.74 0.76 0.78 0.  ]\n"," [0.   0.   0.83 0.  ]\n"," [0.   0.   0.89 0.  ]\n"," [0.   0.   1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.61 0.   0.74 0.6 ]\n"," [0.55 0.   0.76 0.59]\n"," [0.54 0.78 0.   0.56]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.83 0.   0.46]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.89 0.   0.49]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.56 1.   0.45]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.96\n","Optimal Action Value Estimates:\n","[[0.74 0.75 0.78 0.  ]\n"," [0.   0.   0.82 0.  ]\n"," [0.   0.   0.89 0.  ]\n"," [0.   0.   1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.61 0.   0.74 0.6 ]\n"," [0.56 0.   0.75 0.59]\n"," [0.54 0.78 0.   0.57]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.82 0.   0.46]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.89 0.   0.5 ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.56 1.   0.45]\n"," [0.   0.   0.   0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"OKktGDk_UIvd","colab_type":"code","outputId":"453245ef-2c3c-402d-95e7-7e3b2c496380","executionInfo":{"status":"ok","timestamp":1555717692188,"user_tz":240,"elapsed":83559,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"cell_type":"code","source":["print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","optimal_action_estimates = np.max(action_value_array,axis=1)\n","print(\"Optimal Action Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(4,4),2))\n","print(\"estimate of the optimal state value at each state\")\n","print(\"\")\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_value_array.reshape((16,4)),2))\n","print(\"\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.96\n","Optimal Action Value Estimates:\n","[[0.74 0.75 0.78 0.  ]\n"," [0.   0.   0.82 0.  ]\n"," [0.   0.   0.89 0.  ]\n"," [0.   0.   1.   0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.61 0.   0.74 0.6 ]\n"," [0.56 0.   0.75 0.59]\n"," [0.54 0.78 0.   0.57]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.82 0.   0.46]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.89 0.   0.5 ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.56 1.   0.45]\n"," [0.   0.   0.   0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"5z58TlVUXyoP","colab_type":"text"},"cell_type":"markdown","source":["** Example 2: FrozenLake 4x4 Slippery **\n","\n","Standard FrozenLake env where slippery is enabled. Notice the increased failure rate and lower Q values"]},{"metadata":{"id":"FbqZr9dAUOhm","colab_type":"code","colab":{}},"cell_type":"code","source":["#hyperparameters\n","episodes = 10000\n","learning_rate = 0.01\n","discount = 0.99\n","lambda_val = 0.5\n","epsilon_start = 1.0\n","epsilon_min = 0.01\n","epsilon_step = (epsilon_start - epsilon_min)/(episodes*.9)\n","\n","env = gym.make('FrozenLake-v0')\n","num_actions = env.action_space.n\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","state_value_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"state_value\")\n","lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\")\n","                              \n","mfv_return_ = trfl.multistep_forward_view(reward_, discount_, state_value_, lambda_, back_prop=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"017M07L0-eNx","colab_type":"code","outputId":"0b4dff06-c9a9-40cd-845e-af89d22f6ca3","executionInfo":{"status":"ok","timestamp":1555717754247,"user_tz":240,"elapsed":142726,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":4474}},"cell_type":"code","source":["stats_success = []\n","epsilon = epsilon_start\n","\n","with tf.Session() as sess:\n","  #initialize the estimated state values to zero\n","  action_value_array = np.zeros((16,num_actions))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  state_value_list, q_action_list, reward_list, state_int_list, action_list = [], [], [], [], []\n","  eligibility_cutoff = False\n","  \n","  while current_episode < episodes:\n","    #take epsilon greedy action\n","    if np.random.rand() < epsilon:\n","      action = env.action_space.sample()\n","      #if random action is not max action, cutoff eligibility cutoffs\n","      if action_value_array[current_state, action] != np.max(action_value_array[current_state]):\n","        eligibility_cutoff = True\n","    else:\n","      #Choose a greedy action. If multiple greedy actions randomly choose between\n","      max_actions = np.argwhere(action_value_array[current_state] == np.max(action_value_array[current_state])).reshape((-1))\n","      action = np.random.choice(max_actions)\n","\n","    next_state, rew, done, info = env.step(action)\n","      \n","    reward_list.append(rew)\n","    action_list.append(action)\n","    q_action_list.append(action_value_array[current_state, action])\n","    state_int_list.append(current_state)\n","    arg_action = np.argmax(action_value_array[next_state])\n","    state_value_list = action_value_array[next_state, arg_action]\n","    \n","    current_state = next_state\n","    \n","    \n","    mfv_output = sess.run(mfv_return_, feed_dict={\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(reward_list)).reshape(-1,1),\n","        state_value_:np.array(state_value_list).reshape(-1,1),\n","        lambda_:np.array([lambda_val]*len(reward_list)).reshape(-1,1),\n","      })\n","    td_error = mfv_output - np.array(q_action_list).reshape(-1,1)\n","\n","    #action_value_array[state_int_list, action_list] += learning_rate*td_error\n","    for s, a, td in zip(state_int_list, action_list, td_error.tolist()):\n","      action_value_array[s,a] += learning_rate*td[0]\n","    \n","    # cut off one action past the first exploration\n","    if eligibility_cutoff:\n","      done = 1\n","      \n","    if done:\n","      if next_state == 15:\n","        stats_success.append(1)\n","      else:\n","        stats_success.append(0)\n","        \n","      state_value_list, q_action_list, reward_list, state_int_list, action_list = [], [], [], [], []\n","      eligibility_cutoff = False\n","      current_state = env.reset()\n","      current_episode += 1\n","      epsilon -= epsilon_step\n","      if epsilon < epsilon_min:\n","        epsilon = epsilon_min\n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","        optimal_action_estimates = np.max(action_value_array,axis=1)\n","        print(\"Optimal Action Value Estimates:\")\n","        print(np.round(optimal_action_estimates.reshape(4,4),2))\n","        print(\"estimate of the optimal state value at each state\")\n","        print(\"\")\n","        print(\"All Action Value Estimates:\")\n","        print(np.round(action_value_array.reshape((16,4)),2))\n","        print(\"\")     "],"execution_count":8,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 1000, 0.89, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 2000, 0.78, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 3000, 0.67, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 4000, 0.56, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.02 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.02 0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 5000, 0.45, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.02 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.02 0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 6000, 0.34, 0.00\n","Optimal Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.01 0.  ]\n"," [0.   0.   0.02 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.01 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.02 0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 7000, 0.23, 0.00\n","Optimal Action Value Estimates:\n","[[0.01 0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.01 0.01 0.02 0.  ]\n"," [0.   0.   0.06 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.01 0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.01]\n"," [0.   0.   0.   0.01]\n"," [0.   0.02 0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.  ]\n"," [0.   0.   0.06 0.  ]\n"," [0.   0.   0.   0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 8000, 0.12, 0.01\n","Optimal Action Value Estimates:\n","[[0.01 0.01 0.01 0.01]\n"," [0.02 0.   0.01 0.  ]\n"," [0.02 0.03 0.06 0.  ]\n"," [0.   0.   0.12 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.    0.01  0.01  0.01]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.    0.01  0.    0.  ]\n"," [ 0.    0.    0.    0.01]\n"," [ 0.02  0.01  0.01  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.01  0.    0.02]\n"," [ 0.    0.    0.    0.03]\n"," [ 0.    0.06  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.01  0.12  0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 9000, 0.01, 0.02\n","Optimal Action Value Estimates:\n","[[0.03 0.03 0.03 0.  ]\n"," [0.03 0.   0.04 0.  ]\n"," [0.03 0.03 0.07 0.  ]\n"," [0.   0.   0.22 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.    0.03  0.01  0.01]\n"," [ 0.    0.03  0.    0.  ]\n"," [ 0.    0.03  0.    0.  ]\n"," [ 0.    0.    0.   -0.03]\n"," [ 0.01  0.01  0.03  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.04  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.01  0.01  0.03]\n"," [ 0.    0.    0.    0.03]\n"," [ 0.    0.07  0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.22  0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.03\n","Optimal Action Value Estimates:\n","[[0.02 0.03 0.03 0.03]\n"," [0.03 0.   0.05 0.  ]\n"," [0.03 0.05 0.1  0.  ]\n"," [0.   0.   0.19 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.01  0.02  0.01  0.01]\n"," [ 0.    0.03  0.    0.  ]\n"," [ 0.    0.03  0.    0.  ]\n"," [ 0.03  0.    0.   -0.03]\n"," [ 0.01  0.01  0.03  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.05  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.01  0.01  0.03]\n"," [ 0.    0.    0.    0.05]\n"," [ 0.    0.1   0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.19  0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"gHSP5ubAVAqt","colab_type":"code","outputId":"1e195ea2-d806-4400-8bcd-d823f15868d3","executionInfo":{"status":"ok","timestamp":1555717754249,"user_tz":240,"elapsed":141749,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"cell_type":"code","source":["print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.2f}, {:.2f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","optimal_action_estimates = np.max(action_value_array,axis=1)\n","print(\"Optimal Action Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(4,4),2))\n","print(\"estimate of the optimal state value at each state\")\n","print(\"\")\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_value_array.reshape((16,4)),2))\n","print(\"\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 10000, 0.01, 0.03\n","Optimal Action Value Estimates:\n","[[0.02 0.03 0.03 0.03]\n"," [0.03 0.   0.05 0.  ]\n"," [0.03 0.05 0.1  0.  ]\n"," [0.   0.   0.19 0.  ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.01  0.02  0.01  0.01]\n"," [ 0.    0.03  0.    0.  ]\n"," [ 0.    0.03  0.    0.  ]\n"," [ 0.03  0.    0.   -0.03]\n"," [ 0.01  0.01  0.03  0.01]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.05  0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.01  0.01  0.01  0.03]\n"," [ 0.    0.    0.    0.05]\n"," [ 0.    0.1   0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.    0.    0.  ]\n"," [ 0.    0.02  0.19  0.  ]\n"," [ 0.    0.    0.    0.  ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"Z2_A-Q6ea5W9","colab_type":"text"},"cell_type":"markdown","source":["** Example 3: FrozenLake 8x8 **\n","\n","FrozenLake on an 8x8 grid. Much harder to randomly find the goal. To make learning faster, we add a penalty for falling into a hole."]},{"metadata":{"id":"eQG7BVhNa4LY","colab_type":"code","colab":{}},"cell_type":"code","source":["#hyperparameters\n","episodes = 20000\n","learning_rate = 0.005\n","discount = 0.99\n","lambda_val = 0.95\n","epsilon_start = 1.0\n","epsilon_min = 0.01\n","epsilon_step = (epsilon_start - epsilon_min)/(episodes*.9)\n","hole_penalty = -0.01 #penalty for falling into a hole\n","\n","seed = 31\n","env = gym.make('FrozenLake8x8-v0')\n","env.seed(seed)\n","np.random.seed(seed)\n","num_actions = env.action_space.n\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","state_value_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"state_value\")\n","lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\")\n","                              \n","mfv_return_ = trfl.multistep_forward_view(reward_, discount_, state_value_, lambda_, back_prop=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Krd5YoIzeUUz","colab_type":"code","outputId":"71a55c1d-1d8c-42bf-a6c9-6a6cf9ca8b7c","executionInfo":{"status":"ok","timestamp":1555718157806,"user_tz":240,"elapsed":315365,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":26760}},"cell_type":"code","source":["stats_success = []\n","epsilon = epsilon_start\n","\n","with tf.Session() as sess:\n","  #initialize the estimated state values to zero\n","  action_value_array = np.zeros((64,num_actions))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  state_value_list, q_action_list, reward_list, state_int_list, action_list = [], [], [], [], []\n","  eligibility_cutoff = False\n","  \n","  while current_episode < episodes:\n","    #take epsilon greedy action\n","    if np.random.rand() < epsilon:\n","      action = env.action_space.sample()\n","      #if random action is not max action, cutoff eligibility cutoffs\n","      if action_value_array[current_state, action] != np.max(action_value_array[current_state]):\n","        eligibility_cutoff = True\n","    else:\n","      #Choose a greedy action. If multiple greedy actions randomly choose between\n","      max_actions = np.argwhere(action_value_array[current_state] == np.max(action_value_array[current_state])).reshape((-1))\n","      action = np.random.choice(max_actions)\n","\n","    next_state, rew, done, info = env.step(action)\n","    \n","#     if done and rew < 1:\n","#       rew = hole_penalty\n","      \n","    reward_list.append(rew)\n","    action_list.append(action)\n","    q_action_list.append(action_value_array[current_state, action])\n","    state_int_list.append(current_state)\n","    arg_action = np.argmax(action_value_array[next_state])\n","    state_value_list = action_value_array[next_state, arg_action]\n","    \n","    current_state = next_state\n","    \n","    \n","    mfv_output = sess.run(mfv_return_, feed_dict={\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(reward_list)).reshape(-1,1),\n","        state_value_:np.array(state_value_list).reshape(-1,1),\n","        lambda_:np.array([lambda_val]*len(reward_list)).reshape(-1,1),\n","      })\n","    td_error = mfv_output - np.array(q_action_list).reshape(-1,1)\n","\n","    #action_value_array[state_int_list, action_list] += learning_rate*td_error\n","    for s, a, td in zip(state_int_list, action_list, td_error.tolist()):\n","      action_value_array[s,a] += learning_rate*td[0]\n","    \n","    # cut off one action past the first exploration\n","    if eligibility_cutoff:\n","      done = 1\n","      \n","    if done:\n","      if next_state == 63:\n","        stats_success.append(1)\n","      else:\n","        stats_success.append(0)\n","        \n","      state_value_list, q_action_list, reward_list, state_int_list, action_list = [], [], [], [], []\n","      eligibility_cutoff = False\n","      current_state = env.reset()\n","      current_episode += 1\n","      epsilon -= epsilon_step\n","      if epsilon < epsilon_min:\n","        epsilon = epsilon_min\n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.3f}, {:.3f}\".format(current_episode, epsilon,\n","                                                                                        np.mean(stats_success[-1000:])))\n","        optimal_action_estimates = np.max(action_value_array,axis=1)\n","        print(\"Optimal Action Value Estimates:\")\n","        print(np.round(optimal_action_estimates.reshape(8,8),3))\n","        print(\"estimate of the optimal state value at each state\")\n","        print(\"\")\n","        print(\"All Action Value Estimates:\")\n","        print(np.round(action_value_array.reshape((64,4)),3))\n","        print(\"\")  "],"execution_count":13,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 1000, 0.945, 0.001\n","Optimal Action Value Estimates:\n","[[0.002 0.002 0.001 0.002 0.002 0.002 0.002 0.005]\n"," [0.001 0.002 0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.    0.001 0.    0.002]\n"," [0.001 0.002 0.002 0.002]\n"," [0.    0.001 0.    0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.005 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.002]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 2000, 0.890, 0.000\n","Optimal Action Value Estimates:\n","[[0.002 0.002 0.001 0.002 0.002 0.002 0.002 0.005]\n"," [0.001 0.002 0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.001 0.001 0.001 0.002]\n"," [0.001 0.002 0.002 0.002]\n"," [0.    0.001 0.    0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.005 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.002]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 3000, 0.835, 0.000\n","Optimal Action Value Estimates:\n","[[0.002 0.002 0.001 0.002 0.002 0.002 0.002 0.005]\n"," [0.001 0.002 0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.001 0.001 0.001 0.002]\n"," [0.001 0.002 0.002 0.002]\n"," [0.    0.001 0.    0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.005 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.002]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 4000, 0.780, 0.000\n","Optimal Action Value Estimates:\n","[[0.001 0.002 0.001 0.002 0.002 0.002 0.002 0.005]\n"," [0.001 0.002 0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.001 0.001 0.001 0.001]\n"," [0.001 0.002 0.002 0.002]\n"," [0.    0.001 0.    0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.005 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.002]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 5000, 0.725, 0.000\n","Optimal Action Value Estimates:\n","[[0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.005]\n"," [0.001 0.002 0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.001 0.001 0.001 0.001]\n"," [0.001 0.001 0.001 0.001]\n"," [0.    0.001 0.    0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.005 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.002]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 6000, 0.670, 0.000\n","Optimal Action Value Estimates:\n","[[0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.005]\n"," [0.001 0.002 0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.001 0.001 0.001 0.001]\n"," [0.001 0.001 0.001 0.001]\n"," [0.    0.001 0.    0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.005 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.002]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 7000, 0.615, 0.000\n","Optimal Action Value Estimates:\n","[[0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.005]\n"," [0.001 0.001 0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.001 0.001 0.001 0.001]\n"," [0.001 0.001 0.001 0.001]\n"," [0.    0.001 0.    0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.005 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.001]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 8000, 0.560, 0.000\n","Optimal Action Value Estimates:\n","[[0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.005]\n"," [0.001 0.001 0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.001 0.001 0.001 0.001]\n"," [0.001 0.001 0.001 0.001]\n"," [0.    0.001 0.    0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.005 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.001]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 9000, 0.505, 0.000\n","Optimal Action Value Estimates:\n","[[0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.005]\n"," [0.    0.001 0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.001 0.001 0.001 0.001]\n"," [0.001 0.001 0.001 0.001]\n"," [0.    0.001 0.    0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.005 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.001 0.001]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 10000, 0.450, 0.000\n","Optimal Action Value Estimates:\n","[[0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.005]\n"," [0.    0.001 0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.001 0.001 0.001 0.001]\n"," [0.001 0.001 0.001 0.001]\n"," [0.    0.001 0.    0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.005 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.001 0.001]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 11000, 0.395, 0.000\n","Optimal Action Value Estimates:\n","[[0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.005]\n"," [0.    0.001 0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.001 0.001 0.001 0.001]\n"," [0.001 0.001 0.001 0.001]\n"," [0.    0.001 0.    0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.005 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.001 0.001]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 12000, 0.340, 0.000\n","Optimal Action Value Estimates:\n","[[0.    0.    0.    0.001 0.001 0.002 0.002 0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.005 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 13000, 0.285, 0.000\n","Optimal Action Value Estimates:\n","[[0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.004]\n"," [0.    0.    0.    0.001 0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.    0.    0.    0.001]\n"," [0.    0.    0.    0.001]\n"," [0.    0.001 0.    0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.001]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 14000, 0.230, 0.000\n","Optimal Action Value Estimates:\n","[[0.001 0.001 0.001 0.002 0.001 0.002 0.002 0.004]\n"," [0.    0.001 0.001 0.001 0.001 0.001 0.001 0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.001 0.001 0.001 0.001]\n"," [0.001 0.001 0.001 0.001]\n"," [0.    0.001 0.    0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.001]\n"," [0.001 0.    0.    0.   ]\n"," [0.    0.    0.    0.001]\n"," [0.    0.    0.    0.001]\n"," [0.    0.    0.001 0.   ]\n"," [0.001 0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 15000, 0.175, 0.000\n","Optimal Action Value Estimates:\n","[[0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.004]\n"," [0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.003]\n"," [0.001 0.001 0.001 0.    0.    0.    0.001 0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.009]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.    0.001 0.    0.   ]\n"," [0.001 0.001 0.001 0.001]\n"," [0.    0.001 0.    0.001]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.004 0.003 0.004 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.001 0.    0.    0.   ]\n"," [0.    0.    0.    0.001]\n"," [0.    0.    0.    0.001]\n"," [0.    0.    0.001 0.   ]\n"," [0.001 0.    0.    0.   ]\n"," [0.003 0.    0.    0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.    0.001]\n"," [0.001 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.001]\n"," [0.    0.003 0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.009 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 16000, 0.120, 0.000\n","Optimal Action Value Estimates:\n","[[0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.003]\n"," [0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.002]\n"," [0.001 0.001 0.001 0.    0.001 0.001 0.002 0.002]\n"," [0.001 0.001 0.001 0.    0.    0.    0.    0.003]\n"," [0.    0.    0.    0.    0.    0.    0.    0.004]\n"," [0.    0.    0.    0.    0.    0.    0.    0.008]\n"," [0.    0.    0.    0.    0.    0.    0.    0.005]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[0.001 0.001 0.001 0.   ]\n"," [0.001 0.001 0.    0.001]\n"," [0.    0.001 0.001 0.001]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.002 0.   ]\n"," [0.    0.    0.    0.002]\n"," [0.003 0.003 0.003 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.    0.001]\n"," [0.001 0.    0.    0.   ]\n"," [0.    0.    0.    0.001]\n"," [0.    0.    0.    0.001]\n"," [0.    0.    0.002 0.   ]\n"," [0.002 0.    0.    0.   ]\n"," [0.002 0.    0.    0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.    0.001]\n"," [0.001 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.    0.001]\n"," [0.    0.    0.    0.002]\n"," [0.    0.002 0.    0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.001 0.   ]\n"," [0.    0.    0.    0.001]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.003 0.003 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.    0.003]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.004 0.008 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.005 0.    0.004 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 17000, 0.065, 0.001\n","Optimal Action Value Estimates:\n","[[0.003 0.004 0.002 0.002 0.002 0.002 0.001 0.001]\n"," [0.002 0.002 0.001 0.001 0.001 0.001 0.002 0.001]\n"," [0.001 0.001 0.001 0.    0.002 0.002 0.002 0.003]\n"," [0.001 0.001 0.001 0.001 0.002 0.    0.001 0.003]\n"," [0.001 0.001 0.    0.    0.    0.    0.004 0.023]\n"," [0.001 0.    0.    0.    0.    0.    0.    0.013]\n"," [0.    0.    0.    0.    0.    0.    0.    0.01 ]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.     0.001  0.003  0.   ]\n"," [ 0.001  0.     0.001  0.004]\n"," [ 0.001  0.001  0.001  0.002]\n"," [ 0.     0.     0.002  0.   ]\n"," [ 0.     0.     0.002  0.   ]\n"," [ 0.002  0.    -0.002  0.   ]\n"," [ 0.     0.     0.001 -0.002]\n"," [ 0.001  0.001 -0.002  0.   ]\n"," [ 0.     0.     0.002  0.   ]\n"," [ 0.002  0.     0.     0.   ]\n"," [-0.     0.     0.001  0.   ]\n"," [ 0.     0.     0.     0.001]\n"," [ 0.     0.     0.     0.001]\n"," [ 0.     0.    -0.     0.001]\n"," [ 0.002  0.     0.     0.   ]\n"," [ 0.001  0.     0.     0.   ]\n"," [ 0.     0.     0.001  0.   ]\n"," [ 0.     0.     0.     0.001]\n"," [ 0.     0.     0.     0.001]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.002  0.   ]\n"," [ 0.     0.     0.     0.002]\n"," [ 0.     0.     0.     0.002]\n"," [ 0.    -0.002  0.003  0.   ]\n"," [ 0.     0.     0.001  0.   ]\n"," [ 0.     0.     0.001  0.   ]\n"," [ 0.     0.     0.     0.001]\n"," [ 0.     0.     0.     0.001]\n"," [ 0.002  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.001  0.     0.     0.   ]\n"," [ 0.     0.003  0.001  0.   ]\n"," [ 0.     0.     0.001  0.   ]\n"," [ 0.001  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.004  0.     0.   ]\n"," [ 0.     0.023  0.     0.003]\n"," [ 0.001  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.004  0.013  0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.01   0.     0.004  0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 18000, 0.010, 0.002\n","Optimal Action Value Estimates:\n","[[0.002 0.002 0.004 0.003 0.004 0.002 0.004 0.   ]\n"," [0.002 0.002 0.002 0.002 0.002 0.003 0.    0.   ]\n"," [0.002 0.002 0.002 0.    0.002 0.001 0.    0.001]\n"," [0.002 0.002 0.002 0.002 0.002 0.    0.002 0.007]\n"," [0.001 0.001 0.    0.    0.002 0.    0.011 0.021]\n"," [0.003 0.    0.    0.    0.    0.    0.    0.024]\n"," [0.005 0.    0.    0.    0.    0.    0.    0.024]\n"," [0.004 0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.     0.002  0.001  0.   ]\n"," [-0.002  0.002 -0.    -0.001]\n"," [ 0.001  0.001  0.004  0.001]\n"," [ 0.     0.003 -0.006  0.   ]\n"," [ 0.004  0.    -0.009 -0.004]\n"," [ 0.     0.002 -0.002  0.   ]\n"," [ 0.004  0.    -0.014 -0.002]\n"," [ 0.    -0.007 -0.002  0.   ]\n"," [-0.001  0.001  0.002  0.001]\n"," [ 0.002  0.     0.     0.   ]\n"," [-0.     0.     0.002  0.   ]\n"," [ 0.     0.     0.002 -0.003]\n"," [ 0.002  0.     0.    -0.   ]\n"," [ 0.     0.    -0.     0.003]\n"," [-0.002  0.    -0.     0.   ]\n"," [-0.001  0.     0.     0.   ]\n"," [ 0.     0.     0.002  0.   ]\n"," [ 0.     0.     0.     0.002]\n"," [ 0.     0.     0.     0.002]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.002  0.   ]\n"," [ 0.     0.     0.     0.001]\n"," [ 0.     0.     0.    -0.002]\n"," [ 0.    -0.002  0.001  0.   ]\n"," [ 0.     0.     0.002  0.   ]\n"," [ 0.     0.     0.002  0.   ]\n"," [ 0.     0.     0.     0.002]\n"," [ 0.     0.     0.     0.002]\n"," [ 0.002  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.002  0.     0.     0.   ]\n"," [ 0.     0.007  0.001  0.   ]\n"," [ 0.     0.     0.001  0.   ]\n"," [ 0.001  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.002  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.011  0.     0.   ]\n"," [ 0.     0.021  0.     0.003]\n"," [ 0.003  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.004  0.024  0.   ]\n"," [ 0.     0.     0.005  0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.024  0.     0.004  0.   ]\n"," [ 0.004  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 19000, 0.010, 0.001\n","Optimal Action Value Estimates:\n","[[0.    0.    0.    0.    0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.    0.    0.002 0.004 0.005]\n"," [0.    0.    0.    0.    0.    0.    0.003 0.012]\n"," [0.    0.    0.    0.    0.001 0.    0.012 0.025]\n"," [0.    0.    0.    0.    0.    0.    0.    0.028]\n"," [0.    0.    0.    0.    0.    0.    0.    0.029]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[ 0.    -0.003 -0.    -0.001]\n"," [-0.002  0.    -0.    -0.001]\n"," [ 0.     0.    -0.001 -0.   ]\n"," [ 0.     0.    -0.006  0.   ]\n"," [ 0.     0.    -0.009 -0.004]\n"," [ 0.     0.    -0.002  0.   ]\n"," [-0.     0.    -0.014 -0.002]\n"," [ 0.    -0.007 -0.002  0.   ]\n"," [-0.001 -0.    -0.001  0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [-0.     0.     0.     0.   ]\n"," [ 0.     0.     0.    -0.003]\n"," [ 0.     0.     0.    -0.   ]\n"," [ 0.     0.    -0.     0.   ]\n"," [-0.002  0.    -0.     0.   ]\n"," [-0.001  0.     0.     0.   ]\n"," [ 0.     0.    -0.002  0.   ]\n"," [ 0.     0.     0.    -0.001]\n"," [ 0.     0.     0.    -0.001]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.    -0.001  0.   ]\n"," [ 0.     0.     0.     0.002]\n"," [ 0.     0.004  0.    -0.002]\n"," [ 0.    -0.002  0.005  0.   ]\n"," [ 0.     0.    -0.002  0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [-0.001  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.003  0.     0.     0.   ]\n"," [ 0.     0.012  0.001  0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.001  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.012  0.     0.   ]\n"," [ 0.     0.025  0.     0.003]\n"," [-0.002  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.004  0.028  0.   ]\n"," [ 0.     0.    -0.008  0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.029  0.     0.004  0.   ]\n"," [-0.007  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]]\n","\n","Current Episode, Epsilon, Trailing Success %: 20000, 0.010, 0.001\n","Optimal Action Value Estimates:\n","[[0.    0.    0.    0.001 0.001 0.002 0.003 0.003]\n"," [0.    0.    0.    0.001 0.001 0.001 0.002 0.003]\n"," [0.    0.    0.    0.    0.001 0.002 0.003 0.003]\n"," [0.    0.    0.001 0.001 0.001 0.    0.004 0.001]\n"," [0.    0.001 0.    0.    0.001 0.001 0.014 0.033]\n"," [0.    0.    0.    0.    0.    0.    0.    0.042]\n"," [0.    0.    0.    0.    0.    0.    0.    0.033]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.001 -0.003  0.    -0.001]\n"," [-0.002  0.    -0.    -0.001]\n"," [ 0.     0.    -0.001 -0.   ]\n"," [ 0.     0.001 -0.006  0.   ]\n"," [-0.001  0.001 -0.009 -0.004]\n"," [ 0.     0.002 -0.002  0.   ]\n"," [-0.     0.003 -0.014 -0.002]\n"," [-0.    -0.007 -0.002  0.003]\n"," [-0.001  0.    -0.001 -0.001]\n"," [ 0.    -0.003  0.    -0.001]\n"," [-0.     0.     0.     0.   ]\n"," [ 0.     0.     0.001 -0.003]\n"," [ 0.001  0.    -0.    -0.   ]\n"," [ 0.001 -0.    -0.     0.   ]\n"," [-0.002  0.002 -0.     0.   ]\n"," [-0.001  0.     0.003 -0.   ]\n"," [ 0.     0.    -0.002 -0.001]\n"," [-0.001  0.     0.    -0.001]\n"," [ 0.     0.     0.    -0.001]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.001  0.    -0.001  0.   ]\n"," [ 0.     0.     0.     0.002]\n"," [ 0.003 -0.003  0.    -0.002]\n"," [ 0.    -0.002 -0.     0.003]\n"," [ 0.     0.    -0.002  0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.001]\n"," [ 0.     0.     0.     0.001]\n"," [-0.001  0.     0.     0.001]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.004  0.     0.     0.   ]\n"," [ 0.     0.001  0.001  0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.001  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.001  0.     0.     0.   ]\n"," [ 0.     0.001  0.     0.   ]\n"," [ 0.     0.014  0.     0.   ]\n"," [ 0.     0.033  0.     0.003]\n"," [-0.002  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.004  0.042  0.   ]\n"," [ 0.     0.    -0.008  0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.033  0.     0.004  0.   ]\n"," [-0.007  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"IAqLLdIjZs6y","colab_type":"code","outputId":"970837de-ea99-4080-f641-91c5356dd8c1","executionInfo":{"status":"ok","timestamp":1555718157812,"user_tz":240,"elapsed":313699,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":1354}},"cell_type":"code","source":["print(\"Current Episode, Epsilon, Trailing Success %: {}, {:.3f}, {:.3f}\".format(current_episode, epsilon,\n","                                                                                np.mean(stats_success[-1000:])))\n","optimal_action_estimates = np.max(action_value_array,axis=1)\n","print(\"Optimal Action Value Estimates:\")\n","print(np.round(optimal_action_estimates.reshape(8,8),3))\n","print(\"estimate of the optimal state value at each state\")\n","print(\"\")\n","print(\"All Action Value Estimates:\")\n","print(np.round(action_value_array.reshape((64,4)),3))\n","print(\"\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Current Episode, Epsilon, Trailing Success %: 20000, 0.010, 0.001\n","Optimal Action Value Estimates:\n","[[0.    0.    0.    0.001 0.001 0.002 0.003 0.003]\n"," [0.    0.    0.    0.001 0.001 0.001 0.002 0.003]\n"," [0.    0.    0.    0.    0.001 0.002 0.003 0.003]\n"," [0.    0.    0.001 0.001 0.001 0.    0.004 0.001]\n"," [0.    0.001 0.    0.    0.001 0.001 0.014 0.033]\n"," [0.    0.    0.    0.    0.    0.    0.    0.042]\n"," [0.    0.    0.    0.    0.    0.    0.    0.033]\n"," [0.    0.    0.    0.    0.    0.    0.    0.   ]]\n","estimate of the optimal state value at each state\n","\n","All Action Value Estimates:\n","[[-0.001 -0.003  0.    -0.001]\n"," [-0.002  0.    -0.    -0.001]\n"," [ 0.     0.    -0.001 -0.   ]\n"," [ 0.     0.001 -0.006  0.   ]\n"," [-0.001  0.001 -0.009 -0.004]\n"," [ 0.     0.002 -0.002  0.   ]\n"," [-0.     0.003 -0.014 -0.002]\n"," [-0.    -0.007 -0.002  0.003]\n"," [-0.001  0.    -0.001 -0.001]\n"," [ 0.    -0.003  0.    -0.001]\n"," [-0.     0.     0.     0.   ]\n"," [ 0.     0.     0.001 -0.003]\n"," [ 0.001  0.    -0.    -0.   ]\n"," [ 0.001 -0.    -0.     0.   ]\n"," [-0.002  0.002 -0.     0.   ]\n"," [-0.001  0.     0.003 -0.   ]\n"," [ 0.     0.    -0.002 -0.001]\n"," [-0.001  0.     0.    -0.001]\n"," [ 0.     0.     0.    -0.001]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.001  0.    -0.001  0.   ]\n"," [ 0.     0.     0.     0.002]\n"," [ 0.003 -0.003  0.    -0.002]\n"," [ 0.    -0.002 -0.     0.003]\n"," [ 0.     0.    -0.002  0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.001]\n"," [ 0.     0.     0.     0.001]\n"," [-0.001  0.     0.     0.001]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.004  0.     0.     0.   ]\n"," [ 0.     0.001  0.001  0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.001  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.001  0.     0.     0.   ]\n"," [ 0.     0.001  0.     0.   ]\n"," [ 0.     0.014  0.     0.   ]\n"," [ 0.     0.033  0.     0.003]\n"," [-0.002  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.004  0.042  0.   ]\n"," [ 0.     0.    -0.008  0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.033  0.     0.004  0.   ]\n"," [-0.007  0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]\n"," [ 0.     0.     0.     0.   ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"ML0Jw8C0hVEH","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}