{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reinforcement Learning with TensorFlow & TRFL -- TD(λ).ipynb","version":"0.3.2","provenance":[{"file_id":"1BmiEkGoqe_CAaZmggviivF7Ygbd1iPe4","timestamp":1552945822274},{"file_id":"110YMHk2yHqxguCj1yaJaXVmkRUadX9GV","timestamp":1552791755615},{"file_id":"1cMFan2NCLOZ8w_xKSEyXTF09paJ0F930","timestamp":1552618439794},{"file_id":"1SToTDuBpTdV2UVRN9bapskLUesVhBH1J","timestamp":1551831097681},{"file_id":"1ssliB1HogX4KFHRyNKLU2aeiAV865kLv","timestamp":1551578593280},{"file_id":"1N74NgQBdDCDRER81p_VdJA_lsZOR2Hjo","timestamp":1550336742833}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"npzJ574a6A94","colab_type":"text"},"cell_type":"markdown","source":["**Reinforcement Learning with TensorFlow & TRFL: TD(λ)**\n","\n","Outline:\n","1. TD(λ)\n","1. TD(λ) with trfl.td_lambda() toy example\n","1. FrozenLake\n","* TD(λ) with trfl.td_lambda() on FrozenLake\n"]},{"metadata":{"id":"RyxlWytnVqJI","colab_type":"code","outputId":"3130fb85-c575-4174-dfdc-c120d687fcf1","colab":{"base_uri":"https://localhost:8080/","height":328}},"cell_type":"code","source":["#TRFL works with TensorFlow 1.12\n","#installs TensorFlow version 1.12 then restarts the runtime\n","!pip install tensorflow==1.12\n","\n","import os\n","os.kill(os.getpid(), 9)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==1.12 in /usr/local/lib/python3.6/dist-packages (1.12.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.15.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.7)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.11.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.33.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.9)\n","Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.12.2)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (3.7.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.7.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.16.2)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.2.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.1)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (0.15.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (40.9.0)\n"],"name":"stdout"}]},{"metadata":{"id":"XRS56AQDVybG","colab_type":"code","outputId":"8b99f7fa-97c1-4fc7-afaa-f95f04d98262","executionInfo":{"status":"ok","timestamp":1555640837938,"user_tz":240,"elapsed":36491,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":174}},"cell_type":"code","source":["#install tensorflow-probability 0.5.0 that works with TensorFlow 1.12\n","!pip install tensorflow-probability==0.5.0\n","\n","#install TRFL\n","!pip install trfl==1.0"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-probability==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.11.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.5.0) (1.16.2)\n","Requirement already satisfied: trfl==1.0 in /usr/local/lib/python3.6/dist-packages (1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.11.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.16.2)\n","Requirement already satisfied: dm-sonnet in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (1.23)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trfl==1.0) (0.7.1)\n"],"name":"stdout"}]},{"metadata":{"id":"SGop2a_BZCBl","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import trfl\n","import tensorflow_probability as tfp"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FI6L75U7UWtj","colab_type":"text"},"cell_type":"markdown","source":["** TD(λ) **\n","\n","TD(λ) is a way of weighting n-step returns and relating Monte Carlo and TD methods. TD(0), ie λ = 0, is the one step return. TD(1), ie λ = 1, is the Monte Carlo return. In the toy example and FrozenLake examples in this notebook, you can modify the λ values to see the changes in state estimation.\n","\n","\n","** TD(λ) Toy Example **\n","\n","Below is a toy example to illustrate the affects of different values of λ, discount, and learning_rate. Imagine a number line where the agent starts at 0. The agent can only move right and the episode ends when the agent reaches 10. The agent receives a reward of 1 at state 10. The toy example lets you change various hyperparameters to see how the TD(λ) updates change."]},{"metadata":{"id":"gCi2qKhny9Mv","colab_type":"code","outputId":"6042da37-e89c-489b-bd44-d67a263edc80","executionInfo":{"status":"ok","timestamp":1555640914559,"user_tz":240,"elapsed":110571,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":188}},"cell_type":"code","source":["#hyperparamters\n","learning_rate = 0.1 #update state values with this size learning rate\n","lambda_value = 0.9 #lambda value. range: [0,1]. weight lambda returns by this amount\n","discount = 0.9 #discount factor (gamma). range: [0,1]. Decay rewards by this amount each step\n","n_episode = 10 #number of episodes to run toy example\n","done_state = 10 #episode ends after reaching this state\n","\n","#set up env, tensors and TRFL\n","state_value_array = np.zeros((done_state+1,1)) #estimated values of states\n","\n","tf.reset_default_graph()\n","\n","state_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"state_value\")\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","bootstrap_ = tf.placeholder(dtype=tf.float32, shape=[1], name=\"bootstrap\")\n","\n","#TRFL usage: TD Lambda\n","td_lambda_return_ = trfl.td_lambda(state_, reward_, discount_, bootstrap_, lambda_=lambda_value)\n","\n","with tf.Session() as sess:\n","  for i in range(n_episode):\n","    current_state = 0\n","    state_list, reward_list, state_int_list = [], [], []\n","    done = 0\n","    \n","    while not done:\n","      state_list.append(state_value_array[current_state])\n","      state_int_list.append(current_state)\n","\n","      current_state += 1\n","\n","      if current_state < done_state+1:\n","        reward = 0\n","        done = 0\n","        bootstrap_v = state_value_array[current_state]\n","      else:\n","        reward = 1\n","        done = 1\n","        bootstrap_v = 0.\n","      reward_list.append(reward)   \n","      \n","      td_lambda_output = sess.run(td_lambda_return_, feed_dict={\n","          state_:np.array(state_list).reshape(-1,1),\n","          reward_:np.array(reward_list).reshape(-1,1),\n","          discount_:np.array([discount]*len(state_list)).reshape(-1,1),\n","          bootstrap_:np.array(bootstrap_v).reshape((1,))\n","        })\n","\n","      state_value_array[state_int_list] += learning_rate*td_lambda_output.extra.temporal_differences\n","\n","    print(\"Finished episode {}: state values are {}\".format(i,np.round(np.squeeze(state_value_array),3)))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Finished episode 0: state values are [0.012 0.015 0.019 0.023 0.028 0.035 0.043 0.053 0.066 0.081 0.1  ]\n","Finished episode 1: state values are [0.03  0.036 0.043 0.051 0.061 0.073 0.088 0.106 0.128 0.156 0.19 ]\n","Finished episode 2: state values are [0.051 0.059 0.069 0.081 0.095 0.112 0.132 0.157 0.187 0.224 0.271]\n","Finished episode 3: state values are [0.072 0.084 0.096 0.111 0.129 0.15  0.175 0.205 0.242 0.287 0.344]\n","Finished episode 4: state values are [0.094 0.108 0.123 0.141 0.162 0.187 0.216 0.251 0.293 0.344 0.41 ]\n","Finished episode 5: state values are [0.115 0.131 0.149 0.17  0.194 0.222 0.255 0.293 0.34  0.397 0.469]\n","Finished episode 6: state values are [0.135 0.153 0.174 0.197 0.224 0.255 0.291 0.333 0.383 0.445 0.522]\n","Finished episode 7: state values are [0.154 0.174 0.196 0.222 0.251 0.285 0.324 0.369 0.423 0.488 0.57 ]\n","Finished episode 8: state values are [0.172 0.193 0.218 0.245 0.277 0.313 0.355 0.403 0.46  0.528 0.613]\n","Finished episode 9: state values are [0.188 0.211 0.237 0.267 0.301 0.339 0.383 0.434 0.493 0.563 0.651]\n"],"name":"stdout"}]},{"metadata":{"id":"IlXBfRia0b1C","colab_type":"text"},"cell_type":"markdown","source":["** TRFL Usage **\n","\n","The lambda_ in trfl.td_lambda() defaults to 1.0. The optional argument can be a constant between 0 and 1 or a tensor of values. The shape of the inputs [sequence_length, batch_size] where sequence_length is the time dimension where index 0 is the start of the sequence. The exception is bootstrap_ input which is shape batch_size and is the bootstrap value for each sequence.\n","\n","** FrozenLake **\n","\n","FrozenLake is a GridWorld like environment (env) where the agent tries to navigate a two-dimensional world from a starting point a goal. The agent has to avoid falling into holes while navigating a slippery surface that sometimes causes the agent to move unexpectedly."]},{"metadata":{"id":"4G9w9t6O0a0Z","colab_type":"code","outputId":"771ebc83-382e-432c-b239-f6997adfb801","executionInfo":{"status":"ok","timestamp":1555640914706,"user_tz":240,"elapsed":109609,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":960}},"cell_type":"code","source":["env = gym.make('FrozenLake-v0')\n","env.reset()\n","env.reset()\n","env.render()\n","\n","for i in range(10):\n","  action = env.action_space.sample()\n","  obs, reward, done, info = env.step(action)\n","  env.render()\n","  if done:\n","    env.reset()\n","    \n","env.close()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","  (Down)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","  (Left)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","  (Up)\n","SFFF\n","F\u001b[41mH\u001b[0mFH\n","FFFH\n","HFFG\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","  (Down)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n"],"name":"stdout"}]},{"metadata":{"id":"KMY4yn1q3sfu","colab_type":"text"},"cell_type":"markdown","source":["** TD(λ) with trfl.td_lambda() **\n","\n","We'll do two examples. In both cases the agent will move under a random policy. In the first example, the agent will act deterministically. In the second, the agent will act non-deterministically and sometimes move in an unexpected way."]},{"metadata":{"id":"atNhOPAJlWtb","colab_type":"code","colab":{}},"cell_type":"code","source":["#deterministic env\n","from gym.envs.registration import register\n","register(\n","    id='FrozenLakeNotSlippery-v0',\n","    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name' : '4x4', 'is_slippery': False}\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"s6LkhpKyLH7X","colab_type":"code","colab":{}},"cell_type":"code","source":["#env = gym.make('FrozenLake-v0')\n","env = gym.make('FrozenLakeNotSlippery-v0')\n","\n","#hyperparameters\n","episodes = 10000\n","learning_rate = 0.01\n","discount = 1.\n","lambda_val = 0.5\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","state_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"state_value\")\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","bootstrap_ = tf.placeholder(dtype=tf.float32, shape=[1], name=\"bootstrap\")\n","#lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\") #optionally can do lambda placeholder\n","td_lambda_return_ = trfl.td_lambda(state_, reward_, discount_, bootstrap_, lambda_=lambda_val)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GFfd4RTI30M7","colab_type":"text"},"cell_type":"markdown","source":["** TRFL Usage **\n","\n","The lambda_ in trfl.td_lambda() defaults to 1.0. The optional argument can be a constant between 0 and 1 or a tensor of values. The shape of the inputs [sequence_length, batch_size] where sequence_length is the time dimension where index 0 is the start of the sequence. The exception is bootstrap_ input which is shape batch_size and is the bootstrap value for each sequence."]},{"metadata":{"id":"69dhkdh5nMNL","colab_type":"code","outputId":"7a77aa9f-849c-4f53-b72c-40806b7d4dad","executionInfo":{"status":"ok","timestamp":1555641322880,"user_tz":240,"elapsed":514715,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":1217}},"cell_type":"code","source":["with tf.Session() as sess:\n","  \n","  #initialize the estimated state values to zero\n","  state_value_array = np.zeros((16,1))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  state_list, reward_list, state_int_list = [], [], []\n","\n","  while current_episode < episodes:\n","    #take a random action\n","    random_action = env.action_space.sample()\n","    next_state, rew, done, info = env.step(random_action)\n","\n","    state_list.append(state_value_array[current_state])\n","    state_int_list.append(current_state)\n","    reward_list.append(rew)\n","    bootstrap_v = state_value_array[next_state]\n","    \n","    current_state = next_state\n","    \n","    #run td lambda in the session to get lambda returns\n","    td_lambda_output = sess.run(td_lambda_return_, feed_dict={\n","        state_:np.array(state_list).reshape(-1,1),\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(state_list)).reshape(-1,1),\n","        bootstrap_:np.array(bootstrap_v).reshape((1,))\n","      })\n","    #use the lambda returns to update the tabular state value esimates\n","    state_value_array[state_int_list] += learning_rate*td_lambda_output.extra.temporal_differences\n","    \n","    if done:\n","      state_list, reward_list, state_int_list = [], [], []\n","      current_state = env.reset()\n","      current_episode += 1\n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode: {}\".format(current_episode))\n","        print(\"Reshaped State Value Estimates:\")\n","        print(np.round(state_value_array.reshape(4,4),3))\n","        print(\"\")  \n","          "],"execution_count":7,"outputs":[{"output_type":"stream","text":["Current Episode: 1000\n","Reshaped State Value Estimates:\n","[[0.008 0.007 0.009 0.005]\n"," [0.008 0.    0.013 0.   ]\n"," [0.013 0.022 0.031 0.   ]\n"," [0.    0.03  0.102 0.   ]]\n","\n","Current Episode: 2000\n","Reshaped State Value Estimates:\n","[[0.017 0.016 0.019 0.017]\n"," [0.019 0.    0.025 0.   ]\n"," [0.023 0.037 0.048 0.   ]\n"," [0.    0.054 0.132 0.   ]]\n","\n","Current Episode: 3000\n","Reshaped State Value Estimates:\n","[[0.027 0.027 0.036 0.024]\n"," [0.027 0.    0.053 0.   ]\n"," [0.035 0.056 0.094 0.   ]\n"," [0.    0.076 0.19  0.   ]]\n","\n","Current Episode: 4000\n","Reshaped State Value Estimates:\n","[[0.039 0.039 0.05  0.035]\n"," [0.037 0.    0.069 0.   ]\n"," [0.045 0.069 0.112 0.   ]\n"," [0.    0.11  0.213 0.   ]]\n","\n","Current Episode: 5000\n","Reshaped State Value Estimates:\n","[[0.038 0.037 0.044 0.04 ]\n"," [0.039 0.    0.055 0.   ]\n"," [0.053 0.077 0.111 0.   ]\n"," [0.    0.12  0.222 0.   ]]\n","\n","Current Episode: 6000\n","Reshaped State Value Estimates:\n","[[0.045 0.044 0.057 0.047]\n"," [0.046 0.    0.078 0.   ]\n"," [0.06  0.095 0.132 0.   ]\n"," [0.    0.14  0.285 0.   ]]\n","\n","Current Episode: 7000\n","Reshaped State Value Estimates:\n","[[0.059 0.053 0.057 0.047]\n"," [0.066 0.    0.074 0.   ]\n"," [0.089 0.129 0.143 0.   ]\n"," [0.    0.182 0.241 0.   ]]\n","\n","Current Episode: 8000\n","Reshaped State Value Estimates:\n","[[0.057 0.052 0.055 0.05 ]\n"," [0.065 0.    0.064 0.   ]\n"," [0.103 0.146 0.118 0.   ]\n"," [0.    0.184 0.219 0.   ]]\n","\n","Current Episode: 9000\n","Reshaped State Value Estimates:\n","[[0.055 0.05  0.055 0.041]\n"," [0.057 0.    0.075 0.   ]\n"," [0.077 0.12  0.144 0.   ]\n"," [0.    0.167 0.284 0.   ]]\n","\n","Current Episode: 10000\n","Reshaped State Value Estimates:\n","[[0.065 0.064 0.069 0.061]\n"," [0.068 0.    0.095 0.   ]\n"," [0.093 0.13  0.16  0.   ]\n"," [0.    0.168 0.275 0.   ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"Ja90iYZ0iL9-","colab_type":"code","colab":{}},"cell_type":"code","source":["#non deterministic env\n","env = gym.make('FrozenLake-v0')\n","#env = gym.make('FrozenLakeNotSlippery-v0')\n","\n","#hyperparameters\n","episodes = 10000\n","learning_rate = 0.01\n","discount = 0.99\n","lambda_val = 0.5\n","stats_every = 1000\n","\n","tf.reset_default_graph()\n","\n","state_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"state_value\")\n","reward_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"reward\")\n","discount_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"discount\")\n","bootstrap_ = tf.placeholder(dtype=tf.float32, shape=[1], name=\"bootstrap\")\n","#lambda_ = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"lambda\") #optionally can do lambda placeholder\n","td_lambda_return_ = trfl.td_lambda(state_, reward_, discount_, bootstrap_, lambda_=lambda_val)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cPeY43Y6ib1q","colab_type":"code","outputId":"bcb8cd78-dd71-40b7-edac-e60619ef58a5","executionInfo":{"status":"ok","timestamp":1555641626086,"user_tz":240,"elapsed":814238,"user":{"displayName":"Jim DiLorenzo","photoUrl":"","userId":"09166577195279766198"}},"colab":{"base_uri":"https://localhost:8080/","height":1217}},"cell_type":"code","source":["with tf.Session() as sess:\n","  \n","  #initialize the estimated state values to zero\n","  state_value_array = np.zeros((16,1))\n","  #reset the env\n","  current_state = env.reset()\n","\n","  current_episode = 1\n","  state_list, reward_list, state_int_list = [], [], []\n","\n","  while current_episode < episodes:\n","    #take a random action\n","    random_action = env.action_space.sample()\n","    next_state, rew, done, info = env.step(random_action)\n","\n","    state_list.append(state_value_array[current_state])\n","    state_int_list.append(current_state)\n","    reward_list.append(rew)\n","    bootstrap_v = state_value_array[next_state]\n","    \n","    current_state = next_state\n","    #run td lambda in the session to get lambda returns\n","    td_lambda_output = sess.run(td_lambda_return_, feed_dict={\n","        state_:np.array(state_list).reshape(-1,1),\n","        reward_:np.array(reward_list).reshape(-1,1),\n","        discount_:np.array([discount]*len(state_list)).reshape(-1,1),\n","        bootstrap_:np.array(bootstrap_v).reshape((1,))\n","      })\n","    #use the lambda returns to update the tabular state value esimates\n","    state_value_array[state_int_list] += learning_rate*td_lambda_output.extra.temporal_differences\n","    \n","    if done:\n","      state_list, reward_list, state_int_list = [], [], []\n","      current_state = env.reset()\n","      current_episode += 1\n","      if current_episode % stats_every == 0:\n","        print(\"Current Episode: {}\".format(current_episode))\n","        print(\"Reshaped State Value Estimates:\")\n","        print(np.round(state_value_array.reshape(4,4),3))\n","        print(\"\")  "],"execution_count":9,"outputs":[{"output_type":"stream","text":["Current Episode: 1000\n","Reshaped State Value Estimates:\n","[[0.006 0.006 0.007 0.004]\n"," [0.006 0.    0.011 0.   ]\n"," [0.008 0.015 0.025 0.   ]\n"," [0.    0.015 0.067 0.   ]]\n","\n","Current Episode: 2000\n","Reshaped State Value Estimates:\n","[[0.015 0.012 0.013 0.009]\n"," [0.016 0.    0.02  0.   ]\n"," [0.024 0.038 0.053 0.   ]\n"," [0.    0.04  0.122 0.   ]]\n","\n","Current Episode: 3000\n","Reshaped State Value Estimates:\n","[[0.02  0.018 0.021 0.016]\n"," [0.023 0.    0.026 0.   ]\n"," [0.029 0.042 0.048 0.   ]\n"," [0.    0.062 0.129 0.   ]]\n","\n","Current Episode: 4000\n","Reshaped State Value Estimates:\n","[[0.037 0.031 0.033 0.026]\n"," [0.043 0.    0.047 0.   ]\n"," [0.055 0.075 0.097 0.   ]\n"," [0.    0.098 0.181 0.   ]]\n","\n","Current Episode: 5000\n","Reshaped State Value Estimates:\n","[[0.033 0.03  0.034 0.026]\n"," [0.036 0.    0.051 0.   ]\n"," [0.054 0.09  0.1   0.   ]\n"," [0.    0.153 0.238 0.   ]]\n","\n","Current Episode: 6000\n","Reshaped State Value Estimates:\n","[[0.034 0.033 0.038 0.035]\n"," [0.036 0.    0.048 0.   ]\n"," [0.046 0.076 0.105 0.   ]\n"," [0.    0.131 0.219 0.   ]]\n","\n","Current Episode: 7000\n","Reshaped State Value Estimates:\n","[[0.03  0.029 0.032 0.029]\n"," [0.033 0.    0.038 0.   ]\n"," [0.051 0.086 0.092 0.   ]\n"," [0.    0.126 0.208 0.   ]]\n","\n","Current Episode: 8000\n","Reshaped State Value Estimates:\n","[[0.034 0.034 0.039 0.035]\n"," [0.036 0.    0.048 0.   ]\n"," [0.05  0.087 0.098 0.   ]\n"," [0.    0.119 0.181 0.   ]]\n","\n","Current Episode: 9000\n","Reshaped State Value Estimates:\n","[[0.033 0.03  0.037 0.032]\n"," [0.036 0.    0.056 0.   ]\n"," [0.045 0.069 0.101 0.   ]\n"," [0.    0.106 0.194 0.   ]]\n","\n","Current Episode: 10000\n","Reshaped State Value Estimates:\n","[[0.036 0.033 0.04  0.032]\n"," [0.038 0.    0.062 0.   ]\n"," [0.048 0.076 0.095 0.   ]\n"," [0.    0.104 0.16  0.   ]]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"mvr0pGmRiqIU","colab_type":"text"},"cell_type":"markdown","source":["The agent is not very successful when acting randomly. When we do Q(λ) later this section, we'll see the agent perform better with a learned policy and see how changing values like discount, lambda, and learning rate can change performance."]},{"metadata":{"id":"8E3nSFKPaw1q","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}